{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Eager Tensor Poet (Tensorflow 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM8iyPzWuc8G"
   },
   "source": [
    "### Only execute next block, if you want to test with different TF runtime.\n",
    "\n",
    "Remember to restart runtime after installing new software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7hgtQU725ap"
   },
   "outputs": [],
   "source": [
    "# %pip install -U tensorflow-gpu tensorflow-addons tensorflow-federated tensorboard # tf-nightly-gpu  # Currently not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laU7LRAguts6"
   },
   "source": [
    "### Select TF version (in colab)\n",
    "\n",
    "This should be the default starting point for working with the standard environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [],
   "source": [
    "## Import TensorFlow\n",
    "## from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "    ## %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    ## Non colab people need to make sure that tf 2 is installed.\n",
    "    pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "**THIS IS WORK IN PROGRESS**\n",
    "\n",
    "A tensorflow deep LSTM model for text generation\n",
    "\n",
    "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
    "\n",
    "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "rum30R2JzeEl"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Check system\n",
    "\n",
    "### Tensorflow api version check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow api v2 active: 2.6.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if 'api.v2' in tf.version.__name__:\n",
    "        print(f\"Tensorflow api v2 active: {tf.__version__}\")\n",
    "    else:\n",
    "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
    "except:\n",
    "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P08FdKKnEAP6"
   },
   "source": [
    "### GPU/TPU check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJOrQMUxO5WE"
   },
   "source": [
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU/TPU is available, it will be used for training.\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "txvC_JXHO5WF"
   },
   "outputs": [],
   "source": [
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path\n",
    "\n",
    "is_colab_notebook = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select hardware:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT HARDWARE TYPE HERE:\n",
    "## EDIT THIS:\n",
    "\n",
    "use_tpu = False     \n",
    "use_gpu = True    \n",
    "use_eager = True  # TPU: set to False, Apple: set to True, other: True (default) or False.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "gjWwUUfuEAP7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPU available\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:13:51.931400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:51.970218: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:51.971490: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU -> [LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "GPU -> [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "TPU -> []\n",
      "GPUs: ['/device:GPU:0']\n",
      "[('/device:CPU:0', ''), ('/device:GPU:0', 'device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:13:51.980850: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-10-13 14:13:51.984323: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:51.985395: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:51.986400: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.784791: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.785287: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.785702: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.786099: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10437 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "2021-10-13 14:13:52.786941: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n",
      "2021-10-13 14:13:52.788324: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.788900: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.789306: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.789742: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.790147: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.790527: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 10437 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "2021-10-13 14:13:52.790668: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.791078: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.791479: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.791949: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.792355: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:13:52.792826: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /device:GPU:0 with 10437 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "try:\n",
    "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "    use_tpu = True\n",
    "    tpu_is_init = False\n",
    "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
    "except:\n",
    "    print(\"No TPU available\")\n",
    "\n",
    "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
    "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
    "    print(\"{} -> {}\".format(hw,hwlist))\n",
    "\n",
    "\n",
    "if use_tpu is False:\n",
    "    def get_available_devs_of_type(type):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [x.name for x in local_device_protos if type in x.name]\n",
    "\n",
    "    def get_dev_desc():\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
    "\n",
    "    def get_available_gpus():\n",
    "        return get_available_devs_of_type('GPU')\n",
    "\n",
    "    dl = get_available_gpus()\n",
    "    if len(dl)==0:\n",
    "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
    "        if is_colab_notebook is True:\n",
    "            print(\"         Hint: In Colab Runtime / Set runtime type, set runtime type to GPU or TPU.\")\n",
    "        print(get_available_devs_of_type(''))\n",
    "    else:\n",
    "        use_gpu = True\n",
    "        print(f\"GPUs: {dl}\")\n",
    "        print(get_dev_desc())\n",
    "else:\n",
    "    use_eager = False  # Eager mode cannot be used with TPUs.\n",
    "    print(\"DISABLING eager execution because TPUs do not support dynamic execution.\")\n",
    "\n",
    "if use_eager is False:\n",
    "   tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "I1nN1tbCO5WN"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "-Pz4xVgaQtfy"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        self.total_size=0\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=self.get_cache_name(author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
    "                    self.data += dat\n",
    "                    self.total_size += len(dat)\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    self.total_size += len(dat)\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def get_cache_name(self, author, title):\n",
    "        if self.cache_dir is None:\n",
    "            return None\n",
    "        cname=f\"{author} - {title}.txt\"\n",
    "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
    "        return cache_filepath\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            # smpX.append(Xi)\n",
    "            # smpy.append(yi)\n",
    "            if i==0:\n",
    "                smpX=np.array(Xi, dtype=np.float32)\n",
    "                smpy=np.array(yi, dtype=np.float32)\n",
    "            else:\n",
    "                smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "                smpy = np.vstack((smpy, np.array(yi, dtype=np.float32)))\n",
    "                # smpy = np.append(smpy, np.array(yi, dtype=np.float32), axis=0)\n",
    "        return np.array(smpX), np.array(smpy)\n",
    "    \n",
    "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
    "        X, y = self.get_random_sample_batch(batch_size, length)\n",
    "        # xoh = one_hot(X,len(self.i2c))\n",
    "        xoh = tf.keras.backend.one_hot(X, len(self.i2c))\n",
    "        ykc = tf.keras.backend.constant(y)\n",
    "        return xoh, ykc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucJsgqNgEAQC"
   },
   "source": [
    "### Data sources\n",
    "\n",
    "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
    "\n",
    "The name given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the lib array contains of:\n",
    "\n",
    "1. a local filename or https(s) link,\n",
    "2. an Author's name\n",
    "3. a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "tZDo-bIPEAQD"
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768-0.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144-0.txt', 'Virginia Wolf', 'Voyage out'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158-0.txt', 'Jane Austen', 'Emma')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "mfjcZdU9O5Wi"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "    else:\n",
    "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "            json.dump(libdesc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "rUK3G2KhO5Wl"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Women-Writers/Data/Jane Austen - Pride and Prejudice.txt from cache\n",
      "Reading ./Women-Writers/Data/Emily Brontë - Wuthering Heights.txt from cache\n",
      "Downloading ./Women-Writers/Data/Virginia Wolf - Voyage out.txt\n",
      "Caching ./Women-Writers/Data/Virginia Wolf - Voyage out.txt\n",
      "Reading ./Women-Writers/Data/Jane Austen - Emma.txt from cache\n",
      "Total size of texts: 2531401\n"
     ]
    }
   ],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)\n",
    "print(f\"Total size of texts: {textlib.total_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 64\n",
    "iNumBatches = 0\n",
    "if use_tpu is True:\n",
    "    BATCH_SIZE=256\n",
    "    use_tpu_model_for_tpu=True\n",
    "    STATEFUL=False\n",
    "    LSTM_UNITS = 512\n",
    "    LSTM_LAYERS = 4\n",
    "\n",
    "else:\n",
    "    BATCH_SIZE = 128\n",
    "    STATEFUL = True\n",
    "    LSTM_UNITS = 256\n",
    "    LSTM_LAYERS = 2\n",
    "\n",
    "if iNumBatches==0:\n",
    "    NUM_BATCHES=BATCH_SIZE  # int(textlib.total_size/BATCH_SIZE/SEQUENCE_LEN)\n",
    "else:\n",
    "    NUM_BATCHES=iNumBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "EeB7jugCV4lI"
   },
   "outputs": [],
   "source": [
    "dx=[]\n",
    "dy=[]\n",
    "for i in range(NUM_BATCHES):\n",
    "    x,y=textlib.get_random_onehot_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
    "    dx.append(x)\n",
    "    dy.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "3whYiFuwS8q4"
   },
   "outputs": [],
   "source": [
    "data_xy=(dx,dy) # tf.keras.backend.constant(np.array([dx,dy]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "DCy7WmQyS9T-"
   },
   "outputs": [],
   "source": [
    "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "boow8wR7sLwi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 64, 128), (128, 64)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_buffer=10000\n",
    "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "BLtDLPjGEAQi"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True, kernel_regularizer=0.0, recurrent_regularizer=0.0):\n",
    "    model = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "        #                          batch_input_shape=[batch_size, None]),\n",
    "        # tf.keras.layers.Flatten(),\n",
    "        *[tf.keras.layers.LSTM(lstm_units,\n",
    "                            # input_shape=(timesteps, data_dim)\n",
    "                            batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                            return_sequences=True,\n",
    "                            stateful=stateful,\n",
    "                            kernel_regularizer=tf.keras.regularizers.l1(kernel_regularizer),\n",
    "                            recurrent_regularizer=tf.keras.regularizers.l1(recurrent_regularizer),\n",
    "                            recurrent_initializer='glorot_uniform')  for _ in range(lstm_layers)],\n",
    "        # *[tf.keras.layers.LSTM(lstm_units,\n",
    "        #                     return_sequences=True,\n",
    "        #                     stateful=stateful,\n",
    "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def build_tpu_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    # print(\"NOT ADAPTED!\")\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
    "    #     embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, None, SEQUENCE_LEN])\n",
    "    with tpu_strategy.scope():\n",
    "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
    "                        batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                        return_sequences=True,\n",
    "                        stateful=stateful,\n",
    "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
    "#     tf.keras.layers.LSTM(lstm_units,\n",
    "#                          return_sequences=True,\n",
    "#                          stateful=stateful,\n",
    "#                          # recurrent_initializer='glorot_uniform',\n",
    "#                         unroll=True)\n",
    "    dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # embedded,\n",
    "        *lstm,\n",
    "        dense\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "oQIxWHMGNKHk"
   },
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    print(TPU_ADDRESS)\n",
    "    os.environ['COLAB_TPU_ADDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "NMsaykJjEAQl"
   },
   "outputs": [],
   "source": [
    "if use_tpu is True and not tpu_is_init:\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
    "    # tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
    "    tpu_is_init=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "nf-NHZ326NqJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-tpu mode\n"
     ]
    }
   ],
   "source": [
    "if use_tpu is True:\n",
    "    if use_tpu_model_for_tpu is True:\n",
    "        print(\"tpu, simple model\")\n",
    "        # with tpu_strategy.scope():\n",
    "        model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          # embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          stateful=STATEFUL)\n",
    "    else:\n",
    "        print(\"tpu, default model\")\n",
    "        with tpu_strategy.scope():\n",
    "            model = build_model(\n",
    "              vocab_size = len(textlib.i2c),\n",
    "              steps=SEQUENCE_LEN,\n",
    "              # embedding_dim=EMBEDDING_DIM,\n",
    "              lstm_units=LSTM_UNITS,\n",
    "              lstm_layers=LSTM_LAYERS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              stateful=STATEFUL)        \n",
    "else:\n",
    "    print(\"non-tpu mode\")\n",
    "    model = build_model(\n",
    "        vocab_size = len(textlib.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        stateful=STATEFUL,\n",
    "        kernel_regularizer=1e-7,\n",
    "        recurrent_regularizer=1e-7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ7inpxLveap"
   },
   "source": [
    "### Some sanity checks of the (untrained!) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "-Fb_Q2TYO5XI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 64, 128), (128, 64)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "GhhF2vqmEAQo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:15:50.396597: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2021-10-13 14:15:51.360453: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 64, 128) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:  # no sanity for TPU, since eager not supported:\n",
    "    for input_example_batch, target_example_batch in dataset.take(1):\n",
    "        model.reset_states()\n",
    "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
    "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "8vxZF0wOEAQr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm (LSTM)                  (128, 64, 256)            394240    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (128, 64, 256)            525312    \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (128, 64, 128)            32896     \n",
      "=================================================================\n",
      "Total params: 952,448\n",
      "Trainable params: 952,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "LYZs8Ss2947k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 64, 128), (128, 64)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "fKo54K1lEAQt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[107  66  70 110  94  57  15  54  87  72 110  27  43 109   7  97  68  53\n",
      "  11  43  33   8 123 107 101   0  86   3  99  34  84  78 104  34 111  47\n",
      "   0 112  35 108  95  62  20  87  57  62  91  33  26  60  74 125 126  43\n",
      " 124 123  92  68 112  83  56 104  29  34]\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "ic4RuDZLEAQy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ρ'6όλHE*’qόM5υjδWIu5.cῶράT— ιY$zὲYέ#Tῦ-ώὰ(d’H(‘.\n",
      "N;᾽ἴ5ὑῶπWῦ@FὲsY\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:\n",
    "    print(textlib.decode(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soB-Q8YXvndE"
   },
   "source": [
    "### Loss function, optimizer, tensorboard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "6i-0Y2uYEAQ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 64, 128)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.854022\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "if use_eager is True:\n",
    "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "py9WnmosEAQ3"
   },
   "outputs": [],
   "source": [
    "opti = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.3)\n",
    "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
    "\n",
    "def scalar_loss(labels, logits):\n",
    "    bl=loss(labels, logits)\n",
    "    return tf.reduce_mean(bl)\n",
    "\n",
    "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "5SKvObcsEAQ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:15:56.626521: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-13 14:15:56.626543: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n",
      "2021-10-13 14:15:56.628189: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1614] Profiler found 1 GPUs\n",
      "2021-10-13 14:15:56.955541: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-13 14:15:56.955774: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch') # , histogram_freq=1) # update_freq='epoch', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "o0Ew6pgWzeFj"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-515c29288eb2b3c3\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-515c29288eb2b3c3\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDFbZcN0vxOB"
   },
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "kh2yUKBoEAQ8"
   },
   "outputs": [],
   "source": [
    "EPOCHS=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "RLbsTmtnEAQ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  1/128 [..............................] - ETA: 4:15 - loss: 4.8574 - scalar_loss: 4.8542"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:16:16.137826: I tensorflow/core/profiler/lib/profiler_session.cc:131] Profiler session initializing.\n",
      "2021-10-13 14:16:16.137848: I tensorflow/core/profiler/lib/profiler_session.cc:146] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  5/128 [>.............................] - ETA: 15s - loss: 4.6185 - scalar_loss: 4.6153"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:16:16.412658: I tensorflow/core/profiler/lib/profiler_session.cc:66] Profiler session collecting data.\n",
      "2021-10-13 14:16:16.413300: I tensorflow/core/profiler/internal/gpu/cupti_tracer.cc:1748] CUPTI activity buffer flushed\n",
      "2021-10-13 14:16:16.442631: I tensorflow/core/profiler/internal/gpu/cupti_collector.cc:673]  GpuTracer has collected 983 callback api events and 990 activity events. \n",
      "2021-10-13 14:16:16.452195: I tensorflow/core/profiler/lib/profiler_session.cc:164] Profiler session tear down.\n",
      "2021-10-13 14:16:16.464307: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16\n",
      "\n",
      "2021-10-13 14:16:16.474119: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.trace.json.gz\n",
      "2021-10-13 14:16:16.488866: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16\n",
      "\n",
      "2021-10-13 14:16:16.490967: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.memory_profile.json.gz\n",
      "2021-10-13 14:16:16.491463: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16\n",
      "Dumped tool data for xplane.pb to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.xplane.pb\n",
      "Dumped tool data for overview_page.pb to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to logs/20211013-141556/train/plugins/profile/2021_10_13_14_16_16/rocinante.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 5s 27ms/step - loss: 3.2358 - scalar_loss: 3.2327\n",
      "Epoch 2/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.8473 - scalar_loss: 2.8441\n",
      "Epoch 3/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.5085 - scalar_loss: 2.5051\n",
      "Epoch 4/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.3681 - scalar_loss: 2.3646\n",
      "Epoch 5/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.2713 - scalar_loss: 2.2677\n",
      "Epoch 6/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.1990 - scalar_loss: 2.1954\n",
      "Epoch 7/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.1371 - scalar_loss: 2.1334\n",
      "Epoch 8/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 2.0859 - scalar_loss: 2.0822\n",
      "Epoch 9/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.0428 - scalar_loss: 2.0391\n",
      "Epoch 10/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 2.0062 - scalar_loss: 2.0024\n",
      "Epoch 11/100\n",
      "128/128 [==============================] - 3s 22ms/step - loss: 1.9721 - scalar_loss: 1.9683\n",
      "Epoch 12/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.9426 - scalar_loss: 1.9388\n",
      "Epoch 13/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.9169 - scalar_loss: 1.9130\n",
      "Epoch 14/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.8907 - scalar_loss: 1.8868\n",
      "Epoch 15/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.8672 - scalar_loss: 1.8633\n",
      "Epoch 16/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.8450 - scalar_loss: 1.8411\n",
      "Epoch 17/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.8248 - scalar_loss: 1.8208\n",
      "Epoch 18/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.8061 - scalar_loss: 1.8021\n",
      "Epoch 19/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.7878 - scalar_loss: 1.7837\n",
      "Epoch 20/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.7689 - scalar_loss: 1.7648:\n",
      "Epoch 21/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.7540 - scalar_loss: 1.7498\n",
      "Epoch 22/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.7376 - scalar_loss: 1.7334\n",
      "Epoch 23/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.7231 - scalar_loss: 1.7189\n",
      "Epoch 24/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.7092 - scalar_loss: 1.7050\n",
      "Epoch 25/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.6956 - scalar_loss: 1.6913\n",
      "Epoch 26/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.6829 - scalar_loss: 1.6785\n",
      "Epoch 27/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.6688 - scalar_loss: 1.6644\n",
      "Epoch 28/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.6575 - scalar_loss: 1.6530\n",
      "Epoch 29/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.6452 - scalar_loss: 1.6407\n",
      "Epoch 30/100\n",
      "128/128 [==============================] - 3s 22ms/step - loss: 1.6350 - scalar_loss: 1.6304\n",
      "Epoch 31/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.6238 - scalar_loss: 1.6192\n",
      "Epoch 32/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.6146 - scalar_loss: 1.6099\n",
      "Epoch 33/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.6042 - scalar_loss: 1.5994\n",
      "Epoch 34/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5957 - scalar_loss: 1.5910\n",
      "Epoch 35/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5868 - scalar_loss: 1.5819: 1s - loss: 1.5871\n",
      "Epoch 36/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.5774 - scalar_loss: 1.5726\n",
      "Epoch 37/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.5689 - scalar_loss: 1.5640\n",
      "Epoch 38/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.5611 - scalar_loss: 1.5562\n",
      "Epoch 39/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5531 - scalar_loss: 1.5481\n",
      "Epoch 40/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.5450 - scalar_loss: 1.5399\n",
      "Epoch 41/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.5377 - scalar_loss: 1.5325\n",
      "Epoch 42/100\n",
      "128/128 [==============================] - 3s 22ms/step - loss: 1.5310 - scalar_loss: 1.5258\n",
      "Epoch 43/100\n",
      "128/128 [==============================] - 3s 22ms/step - loss: 1.5249 - scalar_loss: 1.5197\n",
      "Epoch 44/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5170 - scalar_loss: 1.5117\n",
      "Epoch 45/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5100 - scalar_loss: 1.5046\n",
      "Epoch 46/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.5050 - scalar_loss: 1.4996\n",
      "Epoch 47/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4983 - scalar_loss: 1.4928\n",
      "Epoch 48/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4924 - scalar_loss: 1.4869\n",
      "Epoch 49/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4865 - scalar_loss: 1.4809\n",
      "Epoch 50/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4818 - scalar_loss: 1.4762\n",
      "Epoch 51/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4759 - scalar_loss: 1.4702\n",
      "Epoch 52/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4709 - scalar_loss: 1.4651\n",
      "Epoch 53/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4652 - scalar_loss: 1.4594\n",
      "Epoch 54/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4607 - scalar_loss: 1.4548\n",
      "Epoch 55/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4572 - scalar_loss: 1.4513\n",
      "Epoch 56/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4504 - scalar_loss: 1.4444\n",
      "Epoch 57/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4464 - scalar_loss: 1.4404\n",
      "Epoch 58/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4423 - scalar_loss: 1.4361\n",
      "Epoch 59/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4377 - scalar_loss: 1.4316\n",
      "Epoch 60/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4339 - scalar_loss: 1.4276\n",
      "Epoch 61/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4293 - scalar_loss: 1.4230\n",
      "Epoch 62/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4248 - scalar_loss: 1.4185\n",
      "Epoch 63/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4219 - scalar_loss: 1.4156\n",
      "Epoch 64/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4173 - scalar_loss: 1.4109\n",
      "Epoch 65/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4136 - scalar_loss: 1.4071\n",
      "Epoch 66/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4104 - scalar_loss: 1.4039\n",
      "Epoch 67/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.4085 - scalar_loss: 1.4019\n",
      "Epoch 68/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4044 - scalar_loss: 1.3978\n",
      "Epoch 69/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.4008 - scalar_loss: 1.3942\n",
      "Epoch 70/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3964 - scalar_loss: 1.3897\n",
      "Epoch 71/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3954 - scalar_loss: 1.3887\n",
      "Epoch 72/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.3906 - scalar_loss: 1.3838\n",
      "Epoch 73/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3871 - scalar_loss: 1.3803\n",
      "Epoch 74/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3840 - scalar_loss: 1.3771\n",
      "Epoch 75/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3814 - scalar_loss: 1.3744\n",
      "Epoch 76/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3771 - scalar_loss: 1.3701\n",
      "Epoch 77/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3744 - scalar_loss: 1.3673\n",
      "Epoch 78/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3743 - scalar_loss: 1.3672\n",
      "Epoch 79/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3697 - scalar_loss: 1.3626\n",
      "Epoch 80/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.3664 - scalar_loss: 1.3592\n",
      "Epoch 81/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3634 - scalar_loss: 1.3562\n",
      "Epoch 82/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3608 - scalar_loss: 1.3535\n",
      "Epoch 83/100\n",
      "128/128 [==============================] - 3s 23ms/step - loss: 1.3583 - scalar_loss: 1.3509\n",
      "Epoch 84/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3557 - scalar_loss: 1.3483\n",
      "Epoch 85/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3530 - scalar_loss: 1.3455\n",
      "Epoch 86/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3505 - scalar_loss: 1.3430\n",
      "Epoch 87/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3477 - scalar_loss: 1.3402\n",
      "Epoch 88/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3456 - scalar_loss: 1.3381\n",
      "Epoch 89/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3424 - scalar_loss: 1.3348\n",
      "Epoch 90/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3402 - scalar_loss: 1.3325\n",
      "Epoch 91/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3374 - scalar_loss: 1.3297\n",
      "Epoch 92/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3358 - scalar_loss: 1.3280\n",
      "Epoch 93/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3331 - scalar_loss: 1.3253\n",
      "Epoch 94/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3306 - scalar_loss: 1.3228\n",
      "Epoch 95/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3279 - scalar_loss: 1.3200\n",
      "Epoch 96/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3258 - scalar_loss: 1.3179\n",
      "Epoch 97/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3240 - scalar_loss: 1.3161\n",
      "Epoch 98/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3205 - scalar_loss: 1.3125\n",
      "Epoch 99/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3192 - scalar_loss: 1.3112\n",
      "Epoch 100/100\n",
      "128/128 [==============================] - 3s 24ms/step - loss: 1.3174 - scalar_loss: 1.3093\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvupep98EARA"
   },
   "outputs": [],
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "-pvmuA-8aTx3"
   },
   "outputs": [],
   "source": [
    "use_tpu_for_generation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "qA7qRl5EEARE"
   },
   "outputs": [],
   "source": [
    "if not use_tpu_for_generation:\n",
    "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=1)\n",
    "else:\n",
    "    gen_model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          #embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=1,\n",
    "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "yFRyOB8we2YA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_100'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "KCunzXAQKBI6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f9cd082f7c0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "YJbQqPFEevop"
   },
   "outputs": [],
   "source": [
    "gen_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "nQ6JhIGxEARF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (1, 64, 256)              394240    \n",
      "_________________________________________________________________\n",
      "lstm_3 (LSTM)                (1, 64, 256)              525312    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (1, 64, 128)              32896     \n",
      "=================================================================\n",
      "Total params: 952,448\n",
      "Trainable params: 952,448\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "geLmNIvzEARH"
   },
   "outputs": [],
   "source": [
    "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
    "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if not use_tpu:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=predicted_tensor.eval()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "      print(\"out:\"+''.join(text_generated))\n",
    "\n",
    "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
    "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "      input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  return (start_string + ''.join(text_generated), ids)\n",
    "\n",
    "def generate_text(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[0:SEQUENCE_LEN] # \n",
    "  input_eval_0 = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval_1 = tf.expand_dims(input_eval_0, 0)\n",
    "\n",
    "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = cutstr # []\n",
    "  ids=input_eval_0 # []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model.predict(input_eval, steps=1, batch_size=1)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if use_eager is True:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=tf.keras.backend.eval(predicted_tensor)\n",
    "          print(predicted_id)\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      text_generated +=textlib.i2c[predicted_id]\n",
    "      text_generated = text_generated[-SEQUENCE_LEN:]\n",
    "      print(text_generated)\n",
    "\n",
    "      # input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval_1 = tf.expand_dims(ids[-SEQUENCE_LEN:], 0)\n",
    "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
    "  return (''.join(text_generated), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "x2Fl4hJzO5YM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
    "len(start_string[0:SEQUENCE_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "H7750ibzEARJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2419/3842145049.py:11: The name tf.keras.backend.get_session is deprecated. Please use tf.compat.v1.keras.backend.get_session instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-10-13 14:22:48.300214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:22:48.301030: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:22:48.301460: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:22:48.301895: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:22:48.302358: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:937] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2021-10-13 14:22:48.302665: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10437 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:05:00.0, compute capability: 6.1\n",
      "2021-10-13 14:22:48.302715: I tensorflow/core/common_runtime/process_util.cc:146] Creating new thread pool with default inter op setting: 2. Tune using inter_op_parallelism_threads for best performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ith the clarity of thought of an artificial life form, the disco\n",
      "th the clarity of thought of an artificial life form, the discov\n",
      "h the clarity of thought of an artificial life form, the discove\n",
      " the clarity of thought of an artificial life form, the discover\n",
      "the clarity of thought of an artificial life form, the discovery\n",
      "he clarity of thought of an artificial life form, the discovery \n",
      "e clarity of thought of an artificial life form, the discovery o\n",
      " clarity of thought of an artificial life form, the discovery of\n",
      "clarity of thought of an artificial life form, the discovery of \n",
      "larity of thought of an artificial life form, the discovery of t\n",
      "arity of thought of an artificial life form, the discovery of th\n",
      "rity of thought of an artificial life form, the discovery of the\n",
      "ity of thought of an artificial life form, the discovery of the \n",
      "ty of thought of an artificial life form, the discovery of the a\n",
      "y of thought of an artificial life form, the discovery of the au\n",
      " of thought of an artificial life form, the discovery of the aut\n",
      "of thought of an artificial life form, the discovery of the auth\n",
      "f thought of an artificial life form, the discovery of the autho\n",
      " thought of an artificial life form, the discovery of the authom\n",
      "thought of an artificial life form, the discovery of the authom \n",
      "hought of an artificial life form, the discovery of the authom o\n",
      "ought of an artificial life form, the discovery of the authom of\n",
      "ught of an artificial life form, the discovery of the authom of\n",
      "\n",
      "ght of an artificial life form, the discovery of the authom of\n",
      "d\n",
      "ht of an artificial life form, the discovery of the authom of\n",
      "do\n",
      "t of an artificial life form, the discovery of the authom of\n",
      "don\n",
      " of an artificial life form, the discovery of the authom of\n",
      "done\n",
      "of an artificial life form, the discovery of the authom of\n",
      "done \n",
      "f an artificial life form, the discovery of the authom of\n",
      "done w\n",
      " an artificial life form, the discovery of the authom of\n",
      "done wo\n",
      "an artificial life form, the discovery of the authom of\n",
      "done wor\n",
      "n artificial life form, the discovery of the authom of\n",
      "done wore\n",
      " artificial life form, the discovery of the authom of\n",
      "done wore \n",
      "artificial life form, the discovery of the authom of\n",
      "done wore a\n",
      "rtificial life form, the discovery of the authom of\n",
      "done wore an\n",
      "tificial life form, the discovery of the authom of\n",
      "done wore and\n",
      "ificial life form, the discovery of the authom of\n",
      "done wore and \n",
      "ficial life form, the discovery of the authom of\n",
      "done wore and e\n",
      "icial life form, the discovery of the authom of\n",
      "done wore and en\n",
      "cial life form, the discovery of the authom of\n",
      "done wore and ent\n",
      "ial life form, the discovery of the authom of\n",
      "done wore and enti\n",
      "al life form, the discovery of the authom of\n",
      "done wore and entir\n",
      "l life form, the discovery of the authom of\n",
      "done wore and entire\n",
      " life form, the discovery of the authom of\n",
      "done wore and entirel\n",
      "life form, the discovery of the authom of\n",
      "done wore and entirely\n",
      "ife form, the discovery of the authom of\n",
      "done wore and entirely.\n",
      "fe form, the discovery of the authom of\n",
      "done wore and entirely. \n",
      "e form, the discovery of the authom of\n",
      "done wore and entirely. M\n",
      " form, the discovery of the authom of\n",
      "done wore and entirely. Mr\n",
      "form, the discovery of the authom of\n",
      "done wore and entirely. Mr.\n",
      "orm, the discovery of the authom of\n",
      "done wore and entirely. Mr. \n",
      "rm, the discovery of the authom of\n",
      "done wore and entirely. Mr. H\n",
      "m, the discovery of the authom of\n",
      "done wore and entirely. Mr. He\n",
      ", the discovery of the authom of\n",
      "done wore and entirely. Mr. Hea\n",
      " the discovery of the authom of\n",
      "done wore and entirely. Mr. Heat\n",
      "the discovery of the authom of\n",
      "done wore and entirely. Mr. Heath\n",
      "he discovery of the authom of\n",
      "done wore and entirely. Mr. Heathc\n",
      "e discovery of the authom of\n",
      "done wore and entirely. Mr. Heathcl\n",
      " discovery of the authom of\n",
      "done wore and entirely. Mr. Heathcli\n",
      "discovery of the authom of\n",
      "done wore and entirely. Mr. Heathclif\n",
      "iscovery of the authom of\n",
      "done wore and entirely. Mr. Heathcliff\n",
      "scovery of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’\n",
      "covery of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s\n",
      "overy of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s \n",
      "very of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s f\n",
      "ery of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s fe\n",
      "ry of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s fee\n",
      "y of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      " of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "\n",
      "of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "o\n",
      "f the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      " the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "\n",
      "the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "t\n",
      "he authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "th\n",
      "e authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the\n",
      " authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the \n",
      "authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the p\n",
      "uthom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the po\n",
      "thom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poo\n",
      "hom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor\n",
      "om of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor \n",
      "m of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor l\n",
      " of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor li\n",
      "of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor lit\n",
      "f\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor litt\n",
      "\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor littl\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little\n",
      "one wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little \n",
      "ne wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little s\n",
      "e wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little se\n",
      " wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little ser\n",
      "wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little seri\n",
      "ore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serio\n",
      "re and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little seriou\n",
      "e and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious\n",
      " and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious \n",
      "and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious t\n",
      "nd entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious ta\n",
      "d entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious tal\n",
      " entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk\n",
      "entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk \n",
      "ntirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk i\n",
      "tirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk il\n",
      "irely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill\n",
      "rely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill \n",
      "ely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill i\n",
      "ly. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in\n",
      "y. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in \n",
      ". Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in f\n",
      " Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in fr\n",
      "Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in fro\n",
      "r. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from\n",
      ". Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from \n",
      " Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from t\n",
      "Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from th\n",
      "eathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the\n",
      "athcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the \n",
      "thcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the h\n",
      "hcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the he\n",
      "cliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the hea\n",
      "liff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal\n",
      "iff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal \n",
      "ff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal c\n",
      "f’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal ch\n",
      "’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal cha\n",
      "s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal char\n",
      " feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal charm\n",
      "feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal charm,\n",
      "feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal charm,\n"
     ]
    }
   ],
   "source": [
    "if use_tpu_for_generation:\n",
    "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
    "    with sess.as_default():\n",
    "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "else:\n",
    "    if use_eager:\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        if not tf.executing_eagerly():\n",
    "            print(\"Eager engine stall.\")\n",
    "        else:\n",
    "            sess=tf.compat.v1.keras.backend.get_session()\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
    "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "b-mbC9l8EARL"
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(tx, minQuoteLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "Tql-eLGvEARO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With the clarity of thought of an artificial life form, the discovery of the authom of\n",
      "done wore and entirely. Mr. Heathcliff’s feet\n",
      "of\n",
      "the poor little serious talk ill in from the heal charm,\n"
     ]
    }
   ],
   "source": [
    "txt=textlib.decode(id)\n",
    "txti=txt.split('\\r\\n')\n",
    "for t in txti:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "ZBil008pEARS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "feet<br>of<span style=\"background-color:#e2d7d5;\"><br>the poor </span><sup>[4]</sup>littl<span style=\"background-color:#eadbd8;\">e serious ta</span><sup>[3]</sup>lk ill<span style=\"background-color:#e2d7d5;\"> in from the </span><sup>[4]</sup>heal charm,"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detectPlagiarism(tx, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXYmlO_IEARU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWE_ZZMKEARV"
   },
   "source": [
    "## References:\n",
    "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
    "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## 6. A dialog with the trained model [not ported yet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "\n",
    "def doDialog():\n",
    "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'reset' to reset the conversation context,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        doini = True\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt == 'reset':\n",
    "                doini = True\n",
    "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                print(\"(conversation context marked for reset)\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t > 0.05 and t < 1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(1):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    if doini:\n",
    "                        doini = False\n",
    "                        g_state = sess.run(\n",
    "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                          model.temperature: temperature})\n",
    "            ans = 0\n",
    "            numEndPrompts = 0\n",
    "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                      model.temperature: temperature})\n",
    "                inds = list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
    "            textlib.source_highlight(xso, 13)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "0JEPK2WIQtgI"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'Session'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2419/2699383149.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Talk to the net!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdoDialog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_2419/2647647399.py\u001b[0m in \u001b[0;36mdoDialog\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mminAnswerSize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m64\u001b[0m  \u001b[0;31m# Minimum length of the answer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSession\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Please enter some dialog.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"The net will answer according to your input.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'Session'"
     ]
    }
   ],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of eager_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

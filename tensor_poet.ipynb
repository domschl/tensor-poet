{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Eager Tensor Poet (Tensorflow 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wM8iyPzWuc8G"
   },
   "source": [
    "### Only execute next block, if you want to test with different TF runtime.\n",
    "\n",
    "Remember to restart runtime after installing new software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b7hgtQU725ap"
   },
   "outputs": [],
   "source": [
    "# %pip install -U tensorflow-gpu tensorflow-addons tensorflow-federated tensorboard # tf-nightly-gpu  # Currently not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "laU7LRAguts6"
   },
   "source": [
    "### Select TF version (in colab)\n",
    "\n",
    "This should be the default starting point for working with the standard environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [],
   "source": [
    "## Import TensorFlow\n",
    "## from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "    ## %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    ## Non colab people need to make sure that tf 2 is installed.\n",
    "    pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "**THIS IS WORK IN PROGRESS**\n",
    "\n",
    "A tensorflow deep LSTM model for text generation\n",
    "\n",
    "This code can use either CPU, GPU, TPU or Apple MLCompute (in testing) when running on Google Colab.\n",
    "\n",
    "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rum30R2JzeEl"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Check system\n",
    "\n",
    "### Tensorflow api version check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'api.v2' in tf.version.__name__:\n",
    "        print(f\"Tensorflow api v2 active: {tf.__version__}\")\n",
    "    else:\n",
    "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
    "except:\n",
    "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P08FdKKnEAP6"
   },
   "source": [
    "### GPU/TPU check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lJOrQMUxO5WE"
   },
   "source": [
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU/TPU is available, it will be used for training.\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "txvC_JXHO5WF"
   },
   "outputs": [],
   "source": [
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path\n",
    "\n",
    "is_colab_notebook = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## SELECT HARDWARE TYPE HERE:\n",
    "## EDIT THIS:\n",
    "\n",
    "use_tpu = False     \n",
    "use_gpu = False    \n",
    "use_eager = False  # Do not use with TPU or MLCOMPUTE\n",
    "use_mlcompute = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjWwUUfuEAP7"
   },
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "if use_mlcompute is True:\n",
    "    from tensorflow.python.compiler.mlcompute import mlcompute\n",
    "\n",
    "    use_eager = False  # Eager mode cannot be used with TPUs.\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    print(\"DISABLING eager execution because using MLCOMPUTE.\")\n",
    "\n",
    "    mlcompute.set_mlc_device(device_name='any') # Available options are cpu, gpu, and any.\n",
    "    hwlist=tf.config.experimental.list_logical_devices()\n",
    "    print(f\"hwlist: {hwlist}\")\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    print([(x.name, x.physical_device_desc) for x in local_device_protos])\n",
    "else:\n",
    "    try:\n",
    "        TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "        use_tpu = True\n",
    "        tpu_is_init = False\n",
    "        tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "        print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
    "    except:\n",
    "        print(\"No TPU available\")\n",
    "\n",
    "    for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
    "        hwlist=tf.config.experimental.list_logical_devices(hw)\n",
    "        print(\"{} -> {}\".format(hw,hwlist))\n",
    "\n",
    "\n",
    "    if use_tpu is False:\n",
    "        def get_available_devs_of_type(type):\n",
    "            local_device_protos = device_lib.list_local_devices()\n",
    "            return [x.name for x in local_device_protos if type in x.name]\n",
    "\n",
    "        def get_dev_desc():\n",
    "            local_device_protos = device_lib.list_local_devices()\n",
    "            return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
    "\n",
    "        def get_available_gpus():\n",
    "            return get_available_devs_of_type('GPU')\n",
    "\n",
    "        dl = get_available_gpus()\n",
    "        if len(dl)==0:\n",
    "            print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
    "            if is_colab_notebook is True:\n",
    "                print(\"         Hint: In Colab Runtime / Set runtime type, set runtime type to GPU or TPU.\")\n",
    "            print(get_available_devs_of_type(''))\n",
    "        else:\n",
    "            use_gpu = True\n",
    "            print(f\"GPUs: {dl}\")\n",
    "            print(get_dev_desc())\n",
    "    else:\n",
    "        use_eager = False  # Eager mode cannot be used with TPUs.\n",
    "        print(\"DISABLING eager execution because TPUs do not support dynamic execution.\")\n",
    "\n",
    "if use_eager is False:\n",
    "   tf.compat.v1.disable_eager_execution()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I1nN1tbCO5WN"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Pz4xVgaQtfy"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        self.total_size=0\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=self.get_cache_name(author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
    "                    self.data += dat\n",
    "                    self.total_size += len(dat)\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    self.total_size += len(dat)\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def get_cache_name(self, author, title):\n",
    "        if self.cache_dir is None:\n",
    "            return None\n",
    "        cname=f\"{author} - {title}.txt\"\n",
    "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
    "        return cache_filepath\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            # smpX.append(Xi)\n",
    "            # smpy.append(yi)\n",
    "            if i==0:\n",
    "                smpX=np.array(Xi, dtype=np.float32)\n",
    "                smpy=np.array(yi, dtype=np.float32)\n",
    "            else:\n",
    "                smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "                smpy = np.vstack((smpy, np.array(yi, dtype=np.float32)))\n",
    "                # smpy = np.append(smpy, np.array(yi, dtype=np.float32), axis=0)\n",
    "        return np.array(smpX), np.array(smpy)\n",
    "    \n",
    "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
    "        X, y = self.get_random_sample_batch(batch_size, length)\n",
    "        # xoh = one_hot(X,len(self.i2c))\n",
    "        xoh = tf.keras.backend.one_hot(X, len(self.i2c))\n",
    "        ykc = tf.keras.backend.constant(y)\n",
    "        return xoh, ykc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ucJsgqNgEAQC"
   },
   "source": [
    "### Data sources\n",
    "\n",
    "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
    "\n",
    "The name given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the lib array contains of:\n",
    "\n",
    "1. a local filename or https(s) link,\n",
    "2. an Author's name\n",
    "3. a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZDo-bIPEAQD"
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768-0.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Wolf', 'Voyage out'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mfjcZdU9O5Wi"
   },
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "    else:\n",
    "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "            json.dump(libdesc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rUK3G2KhO5Wl"
   },
   "outputs": [],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)\n",
    "print(f\"Total size of texts: {textlib.total_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 60\n",
    "if use_tpu is True:\n",
    "    BATCH_SIZE=256\n",
    "    use_tpu_model_for_tpu=True\n",
    "    STATEFUL=False\n",
    "    LSTM_UNITS = 512\n",
    "    LSTM_LAYERS = 2\n",
    "\n",
    "else:\n",
    "    if use_mlcompute is True:\n",
    "        BATCH_SIZE = 64\n",
    "        STATEFUL = True\n",
    "        LSTM_UNITS = 128\n",
    "        LSTM_LAYERS = 1\n",
    "    else:\n",
    "        BATCH_SIZE = 256\n",
    "        STATEFUL = True\n",
    "        LSTM_UNITS = 512\n",
    "        LSTM_LAYERS = 2\n",
    "\n",
    "NUM_BATCHES=256  # int(textlib.total_size/BATCH_SIZE/SEQUENCE_LEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EeB7jugCV4lI"
   },
   "outputs": [],
   "source": [
    "dx=[]\n",
    "dy=[]\n",
    "for i in range(NUM_BATCHES):\n",
    "    x,y=textlib.get_random_onehot_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
    "    dx.append(x)\n",
    "    dy.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3whYiFuwS8q4"
   },
   "outputs": [],
   "source": [
    "data_xy=(dx,dy) # tf.keras.backend.constant(np.array([dx,dy]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DCy7WmQyS9T-"
   },
   "outputs": [],
   "source": [
    "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "boow8wR7sLwi"
   },
   "outputs": [],
   "source": [
    "shuffle_buffer=10000\n",
    "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BLtDLPjGEAQi"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    model = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "        #                          batch_input_shape=[batch_size, None]),\n",
    "        # tf.keras.layers.Flatten(),\n",
    "        *[tf.keras.layers.LSTM(lstm_units,\n",
    "                            # input_shape=(timesteps, data_dim)\n",
    "                            batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                            return_sequences=True,\n",
    "                            stateful=stateful,\n",
    "                            recurrent_initializer='glorot_uniform')  for _ in range(lstm_layers)],\n",
    "        # *[tf.keras.layers.LSTM(lstm_units,\n",
    "        #                     return_sequences=True,\n",
    "        #                     stateful=stateful,\n",
    "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def build_tpu_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    # print(\"NOT ADAPTED!\")\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
    "    #     embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, None, SEQUENCE_LEN])\n",
    "    with tpu_strategy.scope():\n",
    "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
    "                        batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                        return_sequences=True,\n",
    "                        stateful=stateful,\n",
    "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
    "#     tf.keras.layers.LSTM(lstm_units,\n",
    "#                          return_sequences=True,\n",
    "#                          stateful=stateful,\n",
    "#                          # recurrent_initializer='glorot_uniform',\n",
    "#                         unroll=True)\n",
    "    dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # embedded,\n",
    "        *lstm,\n",
    "        dense\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQIxWHMGNKHk"
   },
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    print(TPU_ADDRESS)\n",
    "    os.environ['COLAB_TPU_ADDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NMsaykJjEAQl"
   },
   "outputs": [],
   "source": [
    "if use_tpu is True and not tpu_is_init:\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
    "    # tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
    "    tpu_is_init=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nf-NHZ326NqJ"
   },
   "outputs": [],
   "source": [
    "if use_tpu is True:\n",
    "    if use_tpu_model_for_tpu is True:\n",
    "        print(\"tpu, simple model\")\n",
    "        # with tpu_strategy.scope():\n",
    "        model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          # embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          stateful=STATEFUL)\n",
    "    else:\n",
    "        print(\"tpu, default model\")\n",
    "        with tpu_strategy.scope():\n",
    "            model = build_model(\n",
    "              vocab_size = len(textlib.i2c),\n",
    "              steps=SEQUENCE_LEN,\n",
    "              # embedding_dim=EMBEDDING_DIM,\n",
    "              lstm_units=LSTM_UNITS,\n",
    "              lstm_layers=LSTM_LAYERS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              stateful=STATEFUL)        \n",
    "else:\n",
    "    print(\"non-tpu mode\")\n",
    "    model = build_model(\n",
    "        vocab_size = len(textlib.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        stateful=STATEFUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ7inpxLveap"
   },
   "source": [
    "### Some sanity checks of the (untrained!) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-Fb_Q2TYO5XI"
   },
   "outputs": [],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhhF2vqmEAQo"
   },
   "outputs": [],
   "source": [
    "if use_eager is True:  # no sanity for TPU, since eager not supported:\n",
    "    for input_example_batch, target_example_batch in dataset.take(1):\n",
    "        model.reset_states()\n",
    "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
    "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8vxZF0wOEAQr"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LYZs8Ss2947k"
   },
   "outputs": [],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fKo54K1lEAQt"
   },
   "outputs": [],
   "source": [
    "if use_eager is True:\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ic4RuDZLEAQy"
   },
   "outputs": [],
   "source": [
    "if use_eager is True:\n",
    "    print(textlib.decode(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soB-Q8YXvndE"
   },
   "source": [
    "### Loss function, optimizer, tensorboard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6i-0Y2uYEAQ0"
   },
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "if use_eager is True:\n",
    "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "py9WnmosEAQ3"
   },
   "outputs": [],
   "source": [
    "opti = tf.keras.optimizers.Adam(lr=0.001, clipvalue=1.0)\n",
    "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
    "\n",
    "def scalar_loss(labels, logits):\n",
    "    bl=loss(labels, logits)\n",
    "    return tf.reduce_mean(bl)\n",
    "\n",
    "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5SKvObcsEAQ5"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch') # , histogram_freq=1) # update_freq='epoch', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o0Ew6pgWzeFj"
   },
   "outputs": [],
   "source": [
    "if use_mlcompute is False:  # Tensorboard currently borked with mlcompute\n",
    "    # !kill -9 1082 # doesnt work either..\n",
    "    %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDFbZcN0vxOB"
   },
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh2yUKBoEAQ8"
   },
   "outputs": [],
   "source": [
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RLbsTmtnEAQ-"
   },
   "outputs": [],
   "source": [
    "if use_mlcompute is True:\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])\n",
    "else:\n",
    "    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvupep98EARA"
   },
   "outputs": [],
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pvmuA-8aTx3"
   },
   "outputs": [],
   "source": [
    "use_tpu_for_generation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA7qRl5EEARE"
   },
   "outputs": [],
   "source": [
    "if not use_tpu_for_generation:\n",
    "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=1)\n",
    "else:\n",
    "    gen_model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          #embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=1,\n",
    "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFRyOB8we2YA"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCunzXAQKBI6"
   },
   "outputs": [],
   "source": [
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJbQqPFEevop"
   },
   "outputs": [],
   "source": [
    "gen_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ6JhIGxEARF"
   },
   "outputs": [],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geLmNIvzEARH"
   },
   "outputs": [],
   "source": [
    "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
    "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if not use_tpu:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=predicted_tensor.eval()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "      print(\"out:\"+''.join(text_generated))\n",
    "\n",
    "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
    "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "      input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  return (start_string + ''.join(text_generated), ids)\n",
    "\n",
    "def generate_text(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[0:SEQUENCE_LEN] # \n",
    "  input_eval_0 = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval_1 = tf.expand_dims(input_eval_0, 0)\n",
    "\n",
    "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = cutstr # []\n",
    "  ids=input_eval_0 # []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model.predict(input_eval, steps=1, batch_size=1)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if use_eager is True:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=tf.keras.backend.eval(predicted_tensor)\n",
    "          print(predicted_id)\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      text_generated +=textlib.i2c[predicted_id]\n",
    "      text_generated = text_generated[-SEQUENCE_LEN:]\n",
    "      print(text_generated)\n",
    "\n",
    "      # input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval_1 = tf.expand_dims(ids[-SEQUENCE_LEN:], 0)\n",
    "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
    "  return (''.join(text_generated), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2Fl4hJzO5YM"
   },
   "outputs": [],
   "source": [
    "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
    "len(start_string[0:SEQUENCE_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7750ibzEARJ"
   },
   "outputs": [],
   "source": [
    "if use_tpu_for_generation:\n",
    "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
    "    with sess.as_default():\n",
    "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "else:\n",
    "    if use_eager:\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        if not tf.executing_eagerly():\n",
    "            print(\"Eager engine stall.\")\n",
    "        else:\n",
    "            sess=tf.compat.v1.keras.backend.get_session()\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
    "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-mbC9l8EARL"
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(tx, minQuoteLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tql-eLGvEARO"
   },
   "outputs": [],
   "source": [
    "txt=textlib.decode(id)\n",
    "txti=txt.split('\\r\\n')\n",
    "for t in txti:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBil008pEARS"
   },
   "outputs": [],
   "source": [
    "detectPlagiarism(tx, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXYmlO_IEARU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWE_ZZMKEARV"
   },
   "source": [
    "## References:\n",
    "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
    "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## 6. A dialog with the trained model [not ported yet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "\n",
    "def doDialog():\n",
    "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'reset' to reset the conversation context,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        doini = True\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt == 'reset':\n",
    "                doini = True\n",
    "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                print(\"(conversation context marked for reset)\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t > 0.05 and t < 1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(1):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    if doini:\n",
    "                        doini = False\n",
    "                        g_state = sess.run(\n",
    "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                          model.temperature: temperature})\n",
    "            ans = 0\n",
    "            numEndPrompts = 0\n",
    "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                      model.temperature: temperature})\n",
    "                inds = list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
    "            textlib.source_highlight(xso, 13)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JEPK2WIQtgI"
   },
   "outputs": [],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of eager_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

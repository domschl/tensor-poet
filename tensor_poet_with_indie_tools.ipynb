{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Eager Tensor Poet (Tensorflow 2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "jtpy59Yq-Qfz"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Requirement already satisfied: ml-indie-tools in /opt/homebrew/lib/python3.9/site-packages (0.0.24)\n",
      "Collecting ml-indie-tools\n",
      "  Using cached ml_indie_tools-0.0.25-py3-none-any.whl (25 kB)\n",
      "Installing collected packages: ml-indie-tools\n",
      "  Attempting uninstall: ml-indie-tools\n",
      "    Found existing installation: ml-indie-tools 0.0.24\n",
      "    Uninstalling ml-indie-tools-0.0.24:\n",
      "      Successfully uninstalled ml-indie-tools-0.0.24\n",
      "\u001b[33m  DEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "\u001b[33mDEPRECATION: Configuring installation scheme with distutils config files is deprecated and will no longer work in the near future. If you are using a Homebrew or Linuxbrew Python, please see discussion at https://github.com/Homebrew/homebrew-core/issues/76621\u001b[0m\n",
      "Successfully installed ml-indie-tools-0.0.25\n"
     ]
    }
   ],
   "source": [
    "!pip install -U ml-indie-tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_indie_tools.env_tools import MLEnv\n",
    "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
    "from ml_indie_tools.Text_Dataset import Text_Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "A tensorflow deep LSTM model for text generation\n",
    "\n",
    "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
    "\n",
    "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "llPw84PkEAP2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'OS: Darwin, Python: 3.9.9 (Conda), Jupyter Notebook Tensorflow: 2.7.0, GPU: METAL'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
    "ml_env.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='women_writers'\n",
    "model_name='lstm_v1'\n",
    "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
    "encoding, batch generation, and formatted source display. It read some \n",
    "books from Project Gutenberg and supports creation of training batches. \n",
    "The output functions support highlighting to allow to compare generated \n",
    "texts with the actual sources to help to identify identical (memorized) \n",
    "parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "HjkelBcNO5WV"
   },
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO)\n",
    "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
    "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:GutenbergLib:Index not loaded, trying to load...\n",
      "INFO:GutenbergLib:Gutenberg index read from local cache: ./data/gutenberg_cache/gutenberg_index\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/64457.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/63022.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/54066.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/54012.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/54011.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/54010.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/53747.txt\n",
      "WARNING:GutenbergLib:End-text is taking more than half of the book!\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/42671.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/42078.txt\n",
      "WARNING:GutenbergLib:End-text is taking more than half of the book!\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/31100.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/9182.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/5670.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/1342.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/1245.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/1212.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/946.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/771.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/768.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/767.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/161.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25 matching books found with search {'author': ['brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/158.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/144.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/141.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/121.txt\n",
      "INFO:GutenbergLib:Book read from cache at ./data/gutenberg_cache/105.txt\n"
     ]
    }
   ],
   "source": [
    "# sample searches\n",
    "search_spec= {\"author\": [\"brontë\",\"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
    "\n",
    "book_list=gd.search(search_spec)\n",
    "book_cnt = len(book_list)\n",
    "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
    "if book_cnt<40:\n",
    "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
    "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
    "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
    "else:\n",
    "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:Datasets:Loaded 25 texts\n"
     ]
    }
   ],
   "source": [
    "td = Text_Dataset(book_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_sample_batch(td, batch_size, length):\n",
    "    for i in range(batch_size):\n",
    "        Xi, yi = td.get_random_char_tokenized_sample_pair(length)\n",
    "        if i==0:\n",
    "            smpX=np.array(Xi, dtype=np.float32)\n",
    "            smpy=np.array(yi, dtype=np.float32)\n",
    "        else:\n",
    "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
    "            smpy = np.vstack((smpy, np.array(yi, dtype=np.float32)))\n",
    "    return np.array(smpX), np.array(smpy)\n",
    "\n",
    "def get_random_onehot_sample_batch(td, batch_size, length):\n",
    "    X, y = get_random_sample_batch(td, batch_size, length)\n",
    "    xoh = tf.keras.backend.one_hot(X, len(td.i2c))\n",
    "    yk = tf.keras.backend.constant(y)\n",
    "    return xoh, yk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 96\n",
    "iNumBatches = 0\n",
    "if ml_env.is_tpu is True:\n",
    "    BATCH_SIZE=256\n",
    "    use_tpu_model_for_tpu=True\n",
    "    STATEFUL=False\n",
    "    LSTM_UNITS = 512\n",
    "    LSTM_LAYERS = 4\n",
    "\n",
    "else:\n",
    "    BATCH_SIZE = 128\n",
    "    STATEFUL = True\n",
    "    LSTM_UNITS = 512\n",
    "    LSTM_LAYERS = 4\n",
    "\n",
    "if iNumBatches==0:\n",
    "    NUM_BATCHES=BATCH_SIZE  # int(textlib.total_size/BATCH_SIZE/SEQUENCE_LEN)\n",
    "else:\n",
    "    NUM_BATCHES=iNumBatches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "EeB7jugCV4lI"
   },
   "outputs": [],
   "source": [
    "dx=[]\n",
    "dy=[]\n",
    "for i in range(NUM_BATCHES):\n",
    "    x,y=get_random_onehot_sample_batch(td, BATCH_SIZE,SEQUENCE_LEN)\n",
    "    dx.append(x)\n",
    "    dy.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "3whYiFuwS8q4"
   },
   "outputs": [],
   "source": [
    "data_xy=(dx,dy) # tf.keras.backend.constant(np.array([dx,dy]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "DCy7WmQyS9T-"
   },
   "outputs": [],
   "source": [
    "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "id": "boow8wR7sLwi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 96, 188), (128, 96)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_buffer=10000\n",
    "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "BLtDLPjGEAQi"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    model = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "        #                          batch_input_shape=[batch_size, None]),\n",
    "        # tf.keras.layers.Flatten(),\n",
    "        *[tf.keras.layers.LSTM(lstm_units,\n",
    "                            # input_shape=(timesteps, data_dim)\n",
    "                            batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                            return_sequences=True,\n",
    "                            stateful=stateful,\n",
    "                            recurrent_initializer='glorot_uniform')  for _ in range(lstm_layers)],\n",
    "        # *[tf.keras.layers.LSTM(lstm_units,\n",
    "        #                     return_sequences=True,\n",
    "        #                     stateful=stateful,\n",
    "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def build_tpu_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    # print(\"NOT ADAPTED!\")\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
    "    #     embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, None, SEQUENCE_LEN])\n",
    "    with tpu_strategy.scope():\n",
    "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
    "                        batch_input_shape=[batch_size, steps, vocab_size],\n",
    "                        return_sequences=True,\n",
    "                        stateful=stateful,\n",
    "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
    "#     tf.keras.layers.LSTM(lstm_units,\n",
    "#                          return_sequences=True,\n",
    "#                          stateful=stateful,\n",
    "#                          # recurrent_initializer='glorot_uniform',\n",
    "#                         unroll=True)\n",
    "    dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        # embedded,\n",
    "        *lstm,\n",
    "        dense\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "id": "oQIxWHMGNKHk"
   },
   "outputs": [],
   "source": [
    "if ml_env.is_tpu:\n",
    "    print(TPU_ADDRESS)\n",
    "    os.environ['COLAB_TPU_ADDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "NMsaykJjEAQl"
   },
   "outputs": [],
   "source": [
    "if ml_env.is_tpu is True and not tpu_is_init:\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
    "    # tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
    "    tpu_is_init=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "nf-NHZ326NqJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-tpu mode\n"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is True:\n",
    "    if use_tpu_model_for_tpu is True:\n",
    "        print(\"tpu, simple model\")\n",
    "        # with tpu_strategy.scope():\n",
    "        model = build_tpu_model(\n",
    "          vocab_size = len(td.i2c),\n",
    "          # embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          stateful=STATEFUL)\n",
    "    else:\n",
    "        print(\"tpu, default model\")\n",
    "        with tpu_strategy.scope():\n",
    "            model = build_model(\n",
    "              vocab_size = len(td.i2c),\n",
    "              steps=SEQUENCE_LEN,\n",
    "              # embedding_dim=EMBEDDING_DIM,\n",
    "              lstm_units=LSTM_UNITS,\n",
    "              lstm_layers=LSTM_LAYERS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              stateful=STATEFUL)        \n",
    "else:\n",
    "    print(\"non-tpu mode\")\n",
    "    model = build_model(\n",
    "        vocab_size = len(td.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        stateful=STATEFUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQ7inpxLveap"
   },
   "source": [
    "### Some sanity checks of the (untrained!) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "-Fb_Q2TYO5XI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 96, 188), (128, 96)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "GhhF2vqmEAQo"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:40:17.528395: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:40:17.675733: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:40:17.900605: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:40:18.319305: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:40:18.502319: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 96, 188) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is False:  # no sanity for TPU, since eager not supported:\n",
    "    for input_example_batch, target_example_batch in dataset.take(1):\n",
    "        model.reset_states()\n",
    "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
    "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "8vxZF0wOEAQr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_4 (LSTM)                (128, 96, 512)            1435648   \n",
      "_________________________________________________________________\n",
      "lstm_5 (LSTM)                (128, 96, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "lstm_6 (LSTM)                (128, 96, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "lstm_7 (LSTM)                (128, 96, 512)            2099200   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (128, 96, 188)            96444     \n",
      "=================================================================\n",
      "Total params: 7,829,692\n",
      "Trainable params: 7,829,692\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "LYZs8Ss2947k"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((128, 96, 188), (128, 96)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "fKo54K1lEAQt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 84  39   5 108 136  46  33 137  44  83 127 148 138  48  19 185  24  93\n",
      "  28   6  80 181 123  93 131 164  21   2  32 154  84 162 139  44  48   9\n",
      "  81  96   3  50  83 151  36 130  42  20  30 101 172 149 120 182  51  58\n",
      "  33 170  66 109  65 106  45 175  18 167  79 172  72  26 173  77  98  74\n",
      " 152  45 169 106 107 178 105 122  36  44 147  56 155  87 145 123  43   5\n",
      " 164 136  50  96  12  73]\n"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is False:\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "ic4RuDZLEAQy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"-Kνuἐ[αι?e&wπχὑὢiμlTῶάiᾀ’F<eos>é^\"çëιπ18φ<sos>n?jRàhmό7äἔὗόἂδ[Ê,έvDἈËk“4äoV#γεθ/ἈöD_ώ2QRικX}9;άἀK’unφ'.\n"
     ]
    }
   ],
   "source": [
    "if ml_env.is_tpu is False:\n",
    "    print(td.decode(sampled_indices, tokenizer='char'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "soB-Q8YXvndE"
   },
   "source": [
    "### Loss function, optimizer, tensorboard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "6i-0Y2uYEAQ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (128, 96, 188)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       5.236963\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "if ml_env.is_tpu is False:\n",
    "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "py9WnmosEAQ3"
   },
   "outputs": [],
   "source": [
    "opti = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.3)\n",
    "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
    "\n",
    "def scalar_loss(labels, logits):\n",
    "    bl=loss(labels, logits)\n",
    "    return tf.reduce_mean(bl)\n",
    "\n",
    "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "5SKvObcsEAQ5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:42:32.753617: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-01-01 17:42:32.753643: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n",
      "2022-01-01 17:42:32.754126: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n"
     ]
    }
   ],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = os.path.join(model_path, 'training_checkpoints')\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch') # , histogram_freq=1) # update_freq='epoch', "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "o0Ew6pgWzeFj"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kDFbZcN0vxOB"
   },
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "kh2yUKBoEAQ8"
   },
   "outputs": [],
   "source": [
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "RLbsTmtnEAQ-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:42:46.845285: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:47.188861: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:47.408739: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:47.776498: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:47.991612: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:48.912379: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:50.271436: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:50.593615: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n",
      "2022-01-01 17:42:50.980080: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1/128 [..............................] - ETA: 11:47 - loss: 5.2370 - scalar_loss: 5.2370"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:42:51.436017: I tensorflow/core/profiler/lib/profiler_session.cc:110] Profiler session initializing.\n",
      "2022-01-01 17:42:51.436032: I tensorflow/core/profiler/lib/profiler_session.cc:125] Profiler session started.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  2/128 [..............................] - ETA: 1:57 - loss: 5.2282 - scalar_loss: 5.2282 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-01-01 17:42:52.298814: I tensorflow/core/profiler/lib/profiler_session.cc:67] Profiler session collecting data.\n",
      "2022-01-01 17:42:52.302534: I tensorflow/core/profiler/lib/profiler_session.cc:143] Profiler session tear down.\n",
      "2022-01-01 17:42:52.313331: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52\n",
      "\n",
      "2022-01-01 17:42:52.314437: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for trace.json.gz to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.trace.json.gz\n",
      "2022-01-01 17:42:52.318166: I tensorflow/core/profiler/rpc/client/save_profile.cc:136] Creating directory: ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52\n",
      "\n",
      "2022-01-01 17:42:52.318345: I tensorflow/core/profiler/rpc/client/save_profile.cc:142] Dumped gzipped tool data for memory_profile.json.gz to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.memory_profile.json.gz\n",
      "2022-01-01 17:42:52.318739: I tensorflow/core/profiler/rpc/client/capture_profile.cc:251] Creating directory: ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52\n",
      "Dumped tool data for xplane.pb to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.xplane.pb\n",
      "Dumped tool data for overview_page.pb to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.overview_page.pb\n",
      "Dumped tool data for input_pipeline.pb to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.input_pipeline.pb\n",
      "Dumped tool data for tensorflow_stats.pb to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.tensorflow_stats.pb\n",
      "Dumped tool data for kernel_stats.pb to ./logs/20220101-174232/train/plugins/profile/2022_01_01_17_42_52/m1air.fritz.box.kernel_stats.pb\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128/128 [==============================] - 112s 839ms/step - loss: 3.2137 - scalar_loss: 3.2137\n",
      "Epoch 2/20\n",
      "128/128 [==============================] - 108s 843ms/step - loss: 3.1207 - scalar_loss: 3.1207\n",
      "Epoch 3/20\n",
      "128/128 [==============================] - 106s 827ms/step - loss: 3.1212 - scalar_loss: 3.1212\n",
      "Epoch 4/20\n",
      "128/128 [==============================] - 106s 826ms/step - loss: 3.1208 - scalar_loss: 3.1208\n",
      "Epoch 5/20\n",
      "128/128 [==============================] - 106s 831ms/step - loss: 3.1206 - scalar_loss: 3.1206\n",
      "Epoch 6/20\n",
      "128/128 [==============================] - 107s 837ms/step - loss: 3.1205 - scalar_loss: 3.1205\n",
      "Epoch 7/20\n",
      "128/128 [==============================] - 107s 837ms/step - loss: 3.1202 - scalar_loss: 3.1202\n",
      "Epoch 8/20\n",
      "128/128 [==============================] - 107s 835ms/step - loss: 3.1204 - scalar_loss: 3.1204\n",
      "Epoch 9/20\n",
      "128/128 [==============================] - 106s 828ms/step - loss: 3.1200 - scalar_loss: 3.1200\n",
      "Epoch 10/20\n",
      " 79/128 [=================>............] - ETA: 41s - loss: 3.1226 - scalar_loss: 3.1226"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9j/c42slkls67q1v67xnn3n9bx80000gn/T/ipykernel_10106/3758039885.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1182\u001b[0m                 _r=1):\n\u001b[1;32m   1183\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1184\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1185\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    908\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    909\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 910\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    940\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   3128\u001b[0m       (graph_function,\n\u001b[1;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 3130\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   3131\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   3132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1957\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1958\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1959\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1960\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    599\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/lib/python3.9/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     56\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     59\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     60\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rvupep98EARA"
   },
   "outputs": [],
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-pvmuA-8aTx3"
   },
   "outputs": [],
   "source": [
    "use_tpu_for_generation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qA7qRl5EEARE"
   },
   "outputs": [],
   "source": [
    "if not use_tpu_for_generation:\n",
    "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
    "        # embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=1)\n",
    "else:\n",
    "    gen_model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          #embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=1,\n",
    "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yFRyOB8we2YA"
   },
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KCunzXAQKBI6"
   },
   "outputs": [],
   "source": [
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YJbQqPFEevop"
   },
   "outputs": [],
   "source": [
    "gen_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQ6JhIGxEARF"
   },
   "outputs": [],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geLmNIvzEARH"
   },
   "outputs": [],
   "source": [
    "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
    "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if not use_tpu:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=predicted_tensor.eval()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "      print(\"out:\"+''.join(text_generated))\n",
    "\n",
    "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
    "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "      input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  return (start_string + ''.join(text_generated), ids)\n",
    "\n",
    "def generate_text(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[0:SEQUENCE_LEN] # \n",
    "  input_eval_0 = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval_1 = tf.expand_dims(input_eval_0, 0)\n",
    "\n",
    "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = cutstr # []\n",
    "  ids=input_eval_0 # []\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model.predict(input_eval, steps=1, batch_size=1)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if use_eager is True:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=tf.keras.backend.eval(predicted_tensor)\n",
    "          print(predicted_id)\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      text_generated +=textlib.i2c[predicted_id]\n",
    "      text_generated = text_generated[-SEQUENCE_LEN:]\n",
    "      print(text_generated)\n",
    "\n",
    "      # input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval_1 = tf.expand_dims(ids[-SEQUENCE_LEN:], 0)\n",
    "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
    "  return (''.join(text_generated), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x2Fl4hJzO5YM"
   },
   "outputs": [],
   "source": [
    "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
    "len(start_string[0:SEQUENCE_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7750ibzEARJ"
   },
   "outputs": [],
   "source": [
    "if use_tpu_for_generation:\n",
    "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
    "    with sess.as_default():\n",
    "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "else:\n",
    "    if use_eager:\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        if not tf.executing_eagerly():\n",
    "            print(\"Eager engine stall.\")\n",
    "        else:\n",
    "            sess=tf.compat.v1.keras.backend.get_session()\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
    "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b-mbC9l8EARL"
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(tx, minQuoteLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tql-eLGvEARO"
   },
   "outputs": [],
   "source": [
    "txt=textlib.decode(id)\n",
    "txti=txt.split('\\r\\n')\n",
    "for t in txti:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZBil008pEARS"
   },
   "outputs": [],
   "source": [
    "detectPlagiarism(tx, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lXYmlO_IEARU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yWE_ZZMKEARV"
   },
   "source": [
    "## References:\n",
    "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
    "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## 6. A dialog with the trained model [not ported yet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "\n",
    "def doDialog():\n",
    "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'reset' to reset the conversation context,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        doini = True\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt == 'reset':\n",
    "                doini = True\n",
    "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                print(\"(conversation context marked for reset)\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t > 0.05 and t < 1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(1):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    if doini:\n",
    "                        doini = False\n",
    "                        g_state = sess.run(\n",
    "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                          model.temperature: temperature})\n",
    "            ans = 0\n",
    "            numEndPrompts = 0\n",
    "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                      model.temperature: temperature})\n",
    "                inds = list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
    "            textlib.source_highlight(xso, 13)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0JEPK2WIQtgI"
   },
   "outputs": [],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "machine_shape": "hm",
   "name": "Copy of eager_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet_with_indie_tools.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Eager Tensor Poet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "jtpy59Yq-Qfz",
        "outputId": "8fdf7758-72b7-4620-8f23-36bffc8d1d76",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ml-indie-tools\n",
            "  Downloading ml_indie_tools-0.0.25-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ml-indie-tools\n",
            "Successfully installed ml-indie-tools-0.0.25\n"
          ]
        }
      ],
      "source": [
        "!pip install -U ml-indie-tools"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "EgLLjG4yQtft"
      },
      "outputs": [],
      "source": [
        "import logging\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "U5T4m6earb1e"
      },
      "outputs": [],
      "source": [
        "from ml_indie_tools.env_tools import MLEnv\n",
        "from ml_indie_tools.Gutenberg_Dataset import Gutenberg_Dataset\n",
        "from ml_indie_tools.Text_Dataset import Text_Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "A tensorflow deep LSTM model for text generation\n",
        "\n",
        "This code can use either CPU, GPU, TPU when running on Google Colab.\n",
        "\n",
        "Select the corresponding runtime (menu: **`Runtime / Change runtime type`**)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "llPw84PkEAP2",
        "outputId": "a4b1ffd7-89c8-4130-e325-8d77b08b3ba5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 696
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.75.163.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.75.163.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'OS: Linux, Python: 3.7.12, Colab Jupyter Notebook Tensorflow: 2.7.0, TPU: TPU, 8 nodes v2 (8GB)'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "ml_env = MLEnv(platform='tf', accelerator='fastest')\n",
        "ml_env.describe()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if ml_env.is_tpu is True:   # DUPLICATE with LIB!\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # tpu=TPU_ADDRESS)\n",
        "    # tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
        "    tpu_is_init=True\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    use_eager=False\n",
        "else:\n",
        "    use_eager=True"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZ6t9b6ZwSxi",
        "outputId": "bd4c4037-4068-496e-b7d1-364802373929"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Deallocate tpu buffers before initializing tpu system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.75.163.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.75.163.138:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.75.163.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.75.163.138:8470\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "t-TP3Pnsrb1f"
      },
      "outputs": [],
      "source": [
        "project_name='women_writers'\n",
        "model_name='lstm_v1'\n",
        "root_path, project_path, model_path, data_path, log_path = ml_env.init_paths(project_name=project_name, model_name=model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`Text_Dataset` and `Gutenberg_Dataset` classes: libraries for training, \n",
        "encoding, batch generation, and formatted source display. It read some \n",
        "books from Project Gutenberg and supports creation of training batches. \n",
        "The output functions support highlighting to allow to compare generated \n",
        "texts with the actual sources to help to identify identical (memorized) \n",
        "parts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HjkelBcNO5WV"
      },
      "outputs": [],
      "source": [
        "use_dark_mode=False  # Set to false for white background"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BF8eyWnCrb1h"
      },
      "outputs": [],
      "source": [
        "logging.basicConfig(level=logging.INFO)\n",
        "cache_dir = os.path.join(data_path, 'gutenberg_cache')\n",
        "gd = Gutenberg_Dataset(cache_dir=cache_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "C66X7ynnrb1h",
        "outputId": "78e98a4a-3d32-47a3-f769-62b99eaff795",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:GutenbergLib:Index not loaded, trying to load...\n",
            "WARNING:GutenbergLib:End-text is taking more than half of the book!\n",
            "WARNING:GutenbergLib:End-text is taking more than half of the book!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "25 matching books found with search {'author': ['brontë', 'Jane Austen', 'Virginia Woolf'], 'language': ['english']}.\n"
          ]
        }
      ],
      "source": [
        "# sample searches\n",
        "search_spec= {\"author\": [\"brontë\",\"Jane Austen\", \"Virginia Woolf\"], \"language\": [\"english\"]}\n",
        "\n",
        "book_list=gd.search(search_spec)\n",
        "book_cnt = len(book_list)\n",
        "print(f\"{book_cnt} matching books found with search {search_spec}.\")\n",
        "if book_cnt<40:\n",
        "    # Note: please verify that book_cnt is 'reasonable'. If you plan to use a large number of texts, \n",
        "    # consider [mirroring Gutenberg](https://github.com/domschl/ml-indie-tools#working-with-a-local-mirror-of-project-gutenberg)\n",
        "    book_list = gd.insert_book_texts(book_list, download_count_limit=book_cnt)  \n",
        "else:\n",
        "    logging.error(\"Please verify your book_list, a large number of books is scheduled for download. ABORTED.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "2jBH3Z15rb1h"
      },
      "outputs": [],
      "source": [
        "td = Text_Dataset(book_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "f7_tc2Lirb1i"
      },
      "outputs": [],
      "source": [
        "def get_random_sample_batch(td, batch_size, length):\n",
        "    for i in range(batch_size):\n",
        "        Xi, yi = td.get_random_char_tokenized_sample_pair(length)\n",
        "        if i==0:\n",
        "            smpX=np.array(Xi, dtype=np.float32)\n",
        "            smpy=np.array(yi, dtype=np.float32)\n",
        "        else:\n",
        "            smpX = np.vstack((smpX, np.array(Xi, dtype=np.float32)))\n",
        "            smpy = np.vstack((smpy, np.array(yi, dtype=np.float32)))\n",
        "    return np.array(smpX), np.array(smpy)\n",
        "\n",
        "def get_random_onehot_sample_batch(td, batch_size, length):\n",
        "    X, y = get_random_sample_batch(td, batch_size, length)\n",
        "    xoh = tf.keras.backend.one_hot(X, len(td.i2c))\n",
        "    yk = tf.keras.backend.constant(y)\n",
        "    return xoh, yk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "jY3hUuhQYzdT"
      },
      "outputs": [],
      "source": [
        "SEQUENCE_LEN = 80\n",
        "iNumBatches = 64\n",
        "if ml_env.is_tpu is True:\n",
        "    BATCH_SIZE=768\n",
        "    use_tpu_model_for_tpu=True\n",
        "    STATEFUL=False\n",
        "    LSTM_UNITS = 512\n",
        "    LSTM_LAYERS = 4\n",
        "else:\n",
        "    BATCH_SIZE = 128\n",
        "    STATEFUL = True\n",
        "    LSTM_UNITS = 512\n",
        "    LSTM_LAYERS = 4\n",
        "\n",
        "if iNumBatches==0:   # XXX WRONG NUMBER for tests!\n",
        "    NUM_BATCHES=BATCH_SIZE  # int(textlib.total_size/BATCH_SIZE/SEQUENCE_LEN)\n",
        "else:\n",
        "    NUM_BATCHES=iNumBatches"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "EeB7jugCV4lI"
      },
      "outputs": [],
      "source": [
        "dx=[]\n",
        "dy=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "    x,y=get_random_onehot_sample_batch(td, BATCH_SIZE,SEQUENCE_LEN)\n",
        "    dx.append(x)\n",
        "    dy.append(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "3whYiFuwS8q4"
      },
      "outputs": [],
      "source": [
        "data_xy=(dx,dy) # tf.keras.backend.constant(np.array([dx,dy]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DCy7WmQyS9T-"
      },
      "outputs": [],
      "source": [
        "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "boow8wR7sLwi",
        "outputId": "25f3110f-2c74-4338-d68c-9bbc26a1a0e4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((768, 80, 188), (768, 80)), types: (tf.float32, tf.float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "shuffle_buffer=10000\n",
        "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "BLtDLPjGEAQi"
      },
      "outputs": [],
      "source": [
        "def build_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
        "    model = tf.keras.Sequential([\n",
        "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "        #                          batch_input_shape=[batch_size, None]),\n",
        "        # tf.keras.layers.Flatten(),\n",
        "        *[tf.keras.layers.LSTM(lstm_units,\n",
        "                            # input_shape=(timesteps, data_dim)\n",
        "                            batch_input_shape=[batch_size, steps, vocab_size],\n",
        "                            return_sequences=True,\n",
        "                            stateful=stateful,\n",
        "                            recurrent_initializer='glorot_uniform')  for _ in range(lstm_layers)],\n",
        "        # *[tf.keras.layers.LSTM(lstm_units,\n",
        "        #                     return_sequences=True,\n",
        "        #                     stateful=stateful,\n",
        "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "        ])\n",
        "    return model\n",
        "\n",
        "def build_tpu_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
        "    # print(\"NOT ADAPTED!\")\n",
        "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
        "    #     embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, None, SEQUENCE_LEN])\n",
        "    with tpu_strategy.scope():\n",
        "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
        "                        batch_input_shape=[batch_size, steps, vocab_size],\n",
        "                        return_sequences=True,\n",
        "                        stateful=stateful,\n",
        "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
        "#     tf.keras.layers.LSTM(lstm_units,\n",
        "#                          return_sequences=True,\n",
        "#                          stateful=stateful,\n",
        "#                          # recurrent_initializer='glorot_uniform',\n",
        "#                         unroll=True)\n",
        "    dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        # embedded,\n",
        "        *lstm,\n",
        "        dense\n",
        "        ])\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NMsaykJjEAQl"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "nf-NHZ326NqJ",
        "outputId": "5241cad7-7beb-4a31-8654-dd8c1de92b40",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tpu, simple model\n"
          ]
        }
      ],
      "source": [
        "if ml_env.is_tpu is True:\n",
        "    if use_tpu_model_for_tpu is True:\n",
        "        print(\"tpu, simple model\")\n",
        "        # with tpu_strategy.scope():\n",
        "        model = build_tpu_model(\n",
        "          vocab_size = len(td.i2c),\n",
        "          # embedding_dim=EMBEDDING_DIM,\n",
        "          steps=SEQUENCE_LEN,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          lstm_layers=LSTM_LAYERS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          stateful=STATEFUL)\n",
        "    else:\n",
        "        print(\"tpu, default model\")\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_model(\n",
        "              vocab_size = len(td.i2c),\n",
        "              steps=SEQUENCE_LEN,\n",
        "              # embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              stateful=STATEFUL)        \n",
        "else:\n",
        "    print(\"non-tpu mode\")\n",
        "    model = build_model(\n",
        "        vocab_size = len(td.i2c),\n",
        "        # embedding_dim=EMBEDDING_DIM,\n",
        "        steps=SEQUENCE_LEN,\n",
        "        lstm_units=LSTM_UNITS,\n",
        "        lstm_layers=LSTM_LAYERS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        stateful=STATEFUL)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ7inpxLveap"
      },
      "source": [
        "### Some sanity checks of the (untrained!) model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "-Fb_Q2TYO5XI"
      },
      "outputs": [],
      "source": [
        "if use_eager is True:\n",
        "    dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "GhhF2vqmEAQo"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:  # no sanity for TPU, since eager not supported:\n",
        "    for input_example_batch, target_example_batch in dataset.take(1):\n",
        "        model.reset_states()\n",
        "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
        "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "outputId": "3ccc67e5-3437-4f8f-cf80-897ad5e3134c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " lstm (LSTM)                 (768, 80, 512)            1435648   \n",
            "                                                                 \n",
            " lstm_1 (LSTM)               (768, 80, 512)            2099200   \n",
            "                                                                 \n",
            " lstm_2 (LSTM)               (768, 80, 512)            2099200   \n",
            "                                                                 \n",
            " lstm_3 (LSTM)               (768, 80, 512)            2099200   \n",
            "                                                                 \n",
            " dense (Dense)               (768, 80, 188)            96444     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7,829,692\n",
            "Trainable params: 7,829,692\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "LYZs8Ss2947k"
      },
      "outputs": [],
      "source": [
        "if use_eager is True:\n",
        "    dataset.take(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "fKo54K1lEAQt"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "    print(sampled_indices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ic4RuDZLEAQy"
      },
      "outputs": [],
      "source": [
        "if ml_env.is_tpu is False:\n",
        "    print(td.decode(sampled_indices, tokenizer='char'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "6i-0Y2uYEAQ0"
      },
      "outputs": [],
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "if ml_env.is_tpu is False:\n",
        "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "py9WnmosEAQ3"
      },
      "outputs": [],
      "source": [
        "opti = tf.keras.optimizers.Adam(learning_rate=0.001, clipvalue=0.3)\n",
        "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
        "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
        "\n",
        "def scalar_loss(labels, logits):\n",
        "    bl=loss(labels, logits)\n",
        "    return tf.reduce_mean(bl)\n",
        "\n",
        "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "5SKvObcsEAQ5"
      },
      "outputs": [],
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = os.path.join(model_path, 'training_checkpoints')\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(log_path, datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch') # , histogram_freq=1) # update_freq='epoch', "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "o0Ew6pgWzeFj",
        "outputId": "f3089369-7394-427f-ab20-3b0f30cc3c39",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 838
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 876), started 0:06:52 ago. (Use '!kill 876' to kill it.)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "        (async () => {\n",
              "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
              "            url.searchParams.set('tensorboardColab', 'true');\n",
              "            const iframe = document.createElement('iframe');\n",
              "            iframe.src = url;\n",
              "            iframe.setAttribute('width', '100%');\n",
              "            iframe.setAttribute('height', '800');\n",
              "            iframe.setAttribute('frameborder', 0);\n",
              "            document.body.appendChild(iframe);\n",
              "        })();\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "%tensorboard --logdir logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "kh2yUKBoEAQ8"
      },
      "outputs": [],
      "source": [
        "EPOCHS=20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "outputId": "6f56e870-d6f1-4d65-b5b4-33753d8fbf33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train on 64 steps\n",
            "Epoch 1/20\n",
            "15/64 [======>.......................] - ETA: 6:47 - batch: 7.0000 - size: 1.0000 - loss: 3.7271 - scalar_loss: 3.7271"
          ]
        }
      ],
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rvupep98EARA"
      },
      "outputs": [],
      "source": [
        "# Generate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-pvmuA-8aTx3"
      },
      "outputs": [],
      "source": [
        "use_tpu_for_generation=False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qA7qRl5EEARE"
      },
      "outputs": [],
      "source": [
        "if not use_tpu_for_generation:\n",
        "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
        "        # embedding_dim=EMBEDDING_DIM,\n",
        "        steps=SEQUENCE_LEN,\n",
        "        lstm_units=LSTM_UNITS,\n",
        "        lstm_layers=LSTM_LAYERS,\n",
        "        batch_size=1)\n",
        "else:\n",
        "    gen_model = build_tpu_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          #embedding_dim=EMBEDDING_DIM,\n",
        "          steps=SEQUENCE_LEN,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          lstm_layers=LSTM_LAYERS,\n",
        "          batch_size=1,\n",
        "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFRyOB8we2YA"
      },
      "outputs": [],
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KCunzXAQKBI6"
      },
      "outputs": [],
      "source": [
        "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJbQqPFEevop"
      },
      "outputs": [],
      "source": [
        "gen_model.build(tf.TensorShape([1, None]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nQ6JhIGxEARF"
      },
      "outputs": [],
      "source": [
        "gen_model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "geLmNIvzEARH"
      },
      "outputs": [],
      "source": [
        "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 128\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
        "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
        "      if not use_tpu:\n",
        "          predicted_id=predicted_tensor.numpy()\n",
        "      else:\n",
        "          predicted_id=predicted_tensor.eval()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "      print(\"out:\"+''.join(text_generated))\n",
        "\n",
        "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
        "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)\n",
        "\n",
        "def generate_text(model, start_string, temp=0.6):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 128\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  cutstr=start_string[0:SEQUENCE_LEN] # \n",
        "  input_eval_0 = [textlib.c2i[s] for s in cutstr]\n",
        "  input_eval_1 = tf.expand_dims(input_eval_0, 0)\n",
        "\n",
        "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = cutstr # []\n",
        "  ids=input_eval_0 # []\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model.predict(input_eval, steps=1, batch_size=1)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
        "      if use_eager is True:\n",
        "          predicted_id=predicted_tensor.numpy()\n",
        "      else:\n",
        "          predicted_id=tf.keras.backend.eval(predicted_tensor)\n",
        "          print(predicted_id)\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      text_generated +=textlib.i2c[predicted_id]\n",
        "      text_generated = text_generated[-SEQUENCE_LEN:]\n",
        "      print(text_generated)\n",
        "\n",
        "      # input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval_1 = tf.expand_dims(ids[-SEQUENCE_LEN:], 0)\n",
        "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
        "  return (''.join(text_generated), ids)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x2Fl4hJzO5YM"
      },
      "outputs": [],
      "source": [
        "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
        "len(start_string[0:SEQUENCE_LEN])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H7750ibzEARJ"
      },
      "outputs": [],
      "source": [
        "if use_tpu_for_generation:\n",
        "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
        "    with sess.as_default():\n",
        "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
        "else:\n",
        "    if use_eager:\n",
        "        tf.compat.v1.enable_eager_execution()\n",
        "        if not tf.executing_eagerly():\n",
        "            print(\"Eager engine stall.\")\n",
        "        else:\n",
        "            sess=tf.compat.v1.keras.backend.get_session()\n",
        "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
        "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
        "    print(tx)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b-mbC9l8EARL"
      },
      "outputs": [],
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tql-eLGvEARO"
      },
      "outputs": [],
      "source": [
        "txt=textlib.decode(id)\n",
        "txti=txt.split('\\r\\n')\n",
        "for t in txti:\n",
        "    print(t)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBil008pEARS"
      },
      "outputs": [],
      "source": [
        "detectPlagiarism(tx, textlib)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lXYmlO_IEARU"
      },
      "outputs": [],
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model [not ported yet]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uxDNYZiEQtgF"
      },
      "outputs": [],
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JEPK2WIQtgI"
      },
      "outputs": [],
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "50m_8R9qrb1n"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "TPU",
    "colab": {
      "collapsed_sections": [],
      "machine_shape": "hm",
      "name": "eager_poet_with_indie_tools.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "[View in Colaboratory](https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow-gpu==2.0.0-alpha0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "# [WIP] Eager Tensor Poet (tf 2.0)\n",
    "\n",
    "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
    "\n",
    "A tensorflow deep LSTM model for text generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "from urllib.request import urlopen  # Py3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e4yYLii8Qtfw"
   },
   "source": [
    "### Content\n",
    "This notebook contains the following sections:\n",
    "1. TextLibrary: utilities to work with text files\n",
    "  * loading of a list of files (local or URLs)\n",
    "  * encoding for training\n",
    "  * formatted output with quote-highlighting\n",
    "2. Transform text data to tf.data\n",
    "\n",
    "\n",
    "...\n",
    "\n",
    "\n",
    "x. Definition of the tensorflow model\n",
    "x. Model and training parameters\n",
    "x. The actual training on the data (required 1. - 3.)\n",
    "  * Training can be restarted, since the model is saved periodically.\n",
    "x. Generation of text from the trained model (requires 1. - 4.)\n",
    "x. In dialog with with the model (requires 1. - 4.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Check system\n",
    "\n",
    "### Tensorflow api version check\n",
    "\n",
    "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.13.x at the time of this writing. the version check below is preliminary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if 'api.v2' in tf.version.__name__:\n",
    "        print(\"Tensorflow api v2 active.\")\n",
    "    else:\n",
    "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
    "except:\n",
    "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "has_gpu=False\n",
    "\n",
    "def get_available_devs_of_type(type):\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [x.name for x in local_device_protos if type in x.name]\n",
    "\n",
    "def get_dev_desc():\n",
    "    local_device_protos = device_lib.list_local_devices()\n",
    "    return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
    "\n",
    "def get_available_gpus():\n",
    "    return get_available_devs_of_type('GPU')\n",
    "\n",
    "dl = get_available_gpus()\n",
    "if len(dl)==0:\n",
    "    print(\"WARNING: You do not have a GPU, this is going to be very slow\")\n",
    "    print(get_available_devs_of_type(''))\n",
    "else:\n",
    "    print(f\"GPUs: {dl}\")\n",
    "    print(get_dev_desc())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Pz4xVgaQtfy"
   },
   "outputs": [],
   "source": [
    "# TextLibrary class: text library for training, encoding, batch generation,\n",
    "# and formatted source display\n",
    "\n",
    "\n",
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        index = 1\n",
    "        for descriptor in descriptors:\n",
    "            fd = {}\n",
    "            if descriptor[:4] == 'http':\n",
    "                try:\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    self.data += dat\n",
    "                    fd[\"name\"] = descriptor\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"name\"] = os.path.splitext(os.path.basename(descriptor))[0]\n",
    "                try:\n",
    "                    f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def display_colored_html(self, textlist, pre='', post=''):\n",
    "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f[\"name\"]\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"TinyShakespeare\",\n",
    "    \"description\": \"Small Shakespeare 'standard' corpus\",\n",
    "    \"lib\": [\n",
    "        # 'data/tiny-shakespeare.txt',\n",
    "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
    "        'http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt',\n",
    "    ]\n",
    "}\n",
    "\n",
    "textlib = TextLibrary(libdesc[\"lib\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = textlib.get_encoded_slice_array(len(textlib.data))\n",
    "textlib_dataset = tf.data.Dataset.from_tensor_slices(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "n=np.array([])\n",
    "for i in textlib_dataset.take(90):\n",
    "    n=np.append(n,i.numpy())\n",
    "print(n)    \n",
    "print(textlib.decode(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 80\n",
    "BATCH_SIZE = 256\n",
    "LSTM_UNITS = 1024\n",
    "EMBEDDING_DIM = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size=len(data)//SEQUENCE_LEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences=textlib_dataset.batch(SEQUENCE_LEN+1,drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "for arr in sequences.take(3):\n",
    "    n=arr.numpy()\n",
    "    print(arr)\n",
    "    print(\">\"+textlib.decode(n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_input_target(chunk):\n",
    "    input_text = chunk[:-1]\n",
    "    target_text = chunk[1:]\n",
    "    return input_text, target_text\n",
    "\n",
    "dataset = sequences.map(split_input_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick test\n",
    "for input_text, output_text in dataset.take(2):\n",
    "    print(\"I:\"+textlib.decode(input_text.numpy()))\n",
    "    print(\"O:\"+textlib.decode(output_text.numpy()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buffer size to shuffle the dataset\n",
    "# (TF data is designed to work with possibly infinite sequences,\n",
    "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
    "# it maintains a buffer in which it shuffles elements).\n",
    "BUFFER_SIZE = 100000\n",
    "\n",
    "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, lstm_units, batch_size):\n",
    "  model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "                              batch_input_shape=[batch_size, None]),\n",
    "    tf.keras.layers.LSTM(lstm_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.LSTM(lstm_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.LSTM(lstm_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=True,\n",
    "                        recurrent_initializer='glorot_uniform'),\n",
    "    tf.keras.layers.Dense(vocab_size)\n",
    "  ])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(\n",
    "  vocab_size = len(textlib.i2c),\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  lstm_units=LSTM_UNITS,\n",
    "  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_example_batch, target_example_batch in dataset.take(1):\n",
    "  example_batch_predictions = model(input_example_batch)\n",
    "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlib.decode(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss=loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_model(vocab_size = len(textlib.i2c),\n",
    "  embedding_dim=EMBEDDING_DIM,\n",
    "  lstm_units=LSTM_UNITS,\n",
    "  batch_size=1)\n",
    "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
    "\n",
    "model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, start_string):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 1000\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  input_eval = [textlib.c2i[s] for s in start_string]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = .40\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "\n",
    "  return (start_string + ''.join(text_generated), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tx,id=generate_text(model, start_string=\"ROMEO: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(tx, minQuoteLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textlib.decode(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detectPlagiarism(tx, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**below this point not yet ported**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5oK6Ej8OQtf1"
   },
   "source": [
    "## 2. Definition of the Tensorflow model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GP4sk47FQtf1"
   },
   "outputs": [],
   "source": [
    "# The tensorflow model for text generation\n",
    "class TensorPoetModel:\n",
    "    def __init__(self, params):\n",
    "        self.model_name = params[\"model_name\"]\n",
    "        self.vocab_size = params[\"vocab_size\"]\n",
    "        self.neurons = params[\"neurons\"]\n",
    "        self.layers = params[\"layers\"]\n",
    "        self.learning_rate = params[\"learning_rate\"]\n",
    "        self.steps = params[\"steps\"]\n",
    "        self.logdir = params[\"logdir\"]\n",
    "        self.checkpoint = params[\"checkpoint\"]\n",
    "        # self.clip = -1.0 * params[\"clip\"]\n",
    "\n",
    "        tf.reset_default_graph()\n",
    "\n",
    "        # Training & Generating:\n",
    "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
    "\n",
    "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
    "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
    "\n",
    "        stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(\n",
    "            self.neurons, name='basic_lstm_cell') for _ in range(self.layers)])\n",
    "\n",
    "        batch_size = tf.shape(self.X)[0]\n",
    "\n",
    "        self.init_state_0 = stacked_cell.zero_state(batch_size, tf.float32)\n",
    "        self.init_state = self.init_state_0\n",
    "\n",
    "        with tf.variable_scope('rnn') as scope:\n",
    "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X,\n",
    "                                                    initial_state=self.init_state,\n",
    "                                                    dtype=tf.float32)\n",
    "            self.init_state = states\n",
    "\n",
    "        self.final_state = self.init_state\n",
    "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
    "\n",
    "        softmax_w = tf.Variable(tf.random_normal(\n",
    "            [self.neurons, self.vocab_size]), dtype=tf.float32, name='sm_w')\n",
    "        softmax_b = tf.Variable(\n",
    "            [self.vocab_size], dtype=tf.float32, name='sm_b')\n",
    "\n",
    "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
    "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
    "\n",
    "        output_softmax = tf.nn.softmax(logits)\n",
    "\n",
    "        self.temperature = tf.placeholder(tf.float32)\n",
    "        self.output_softmax_temp = tf.nn.softmax(\n",
    "            tf.div(logits, self.temperature))\n",
    "\n",
    "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "            labels=onehot_y, logits=logits)\n",
    "\n",
    "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
    "\n",
    "        # Clipping isn't necessary, even for really deep networks:\n",
    "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
    "        # minclip = -1.0 * self.clip\n",
    "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) \n",
    "        #     for grad, var in grads]\n",
    "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
    "\n",
    "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
    "        correct_prediction = tf.equal(self.y, self.prediction)\n",
    "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "        error = 1.0 - self.accuracy\n",
    "\n",
    "        # Tensorboard\n",
    "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
    "        tf.summary.scalar(\"error\", error)\n",
    "        self.summary_merged = tf.summary.merge_all()\n",
    "\n",
    "        # Init\n",
    "        self.init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "48X8-lFFQtf3"
   },
   "source": [
    "## 3. Parameters for model and training\n",
    "\n",
    "The library description `libdesc` contains a list in `lib` with local filenames of text-files or http, https URLs pointing to text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FhMGIIPEQtf3"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Model parameter:\n",
    "modelParamsShakespeare = {\n",
    "    \"model_name\": \"shakespeare\",\n",
    "    \"logdir\": \"tensorlog/shakespeare\",\n",
    "    \"checkpoint\": \"shakespeare.ckpt\",\n",
    "    \"vocab_size\": len(textlib.i2c),\n",
    "    \"neurons\": 512,\n",
    "    \"layers\": 4,\n",
    "    \"learning_rate\": 4.e-4,\n",
    "    \"steps\": 128,\n",
    "}\n",
    "\n",
    "# Look for optional json description of a library:\n",
    "if os.path.exists('bk/lib-phil-deen.json'):\n",
    "    with open('bk/lib-phil-deen.json') as data_file:    \n",
    "        libdescphil = json.load(data_file)\n",
    "        textlib = TextLibrary(libdescphil[\"lib\"])\n",
    "        modelParamsPhil = {\n",
    "            \"model_name\": \"phil\",\n",
    "            \"logdir\": \"tensorlog/phil\",\n",
    "            \"checkpoint\": \"phil.ckpt\",\n",
    "            \"vocab_size\": len(textlib.i2c),\n",
    "            \"neurons\": 256,\n",
    "            \"layers\": 8,\n",
    "            \"learning_rate\": 1.e-3,\n",
    "            \"steps\": 128,\n",
    "        }\n",
    "        model = TensorPoetModel(modelParamsPhil)\n",
    "else:\n",
    "    model = TensorPoetModel(modelParamsShakespeare)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5ciDOIUiQtf6"
   },
   "outputs": [],
   "source": [
    "# Training Parameter:\n",
    "\n",
    "trainParams = {\n",
    "    \"max_iter\": 1000000,\n",
    "    \"restoreCheckpoints\": False,\n",
    "    \"generateDuringTraining\": True,\n",
    "    \"generated_text_size\": 200,\n",
    "    \"verbose\": True,\n",
    "    \"statusEveryNIter\": 500,\n",
    "    \"saveEveryNIter\": 500,\n",
    "    \"batch_size\": 128,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOlVTPtFQtf7"
   },
   "source": [
    "## 4. The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3x0U5-0dQtf9"
   },
   "outputs": [],
   "source": [
    "# Run training:\n",
    "with tf.Session() as sess:\n",
    "    batch_size = trainParams[\"batch_size\"]\n",
    "    epl = len(textlib.data) / (batch_size * model.steps)\n",
    "    model.init.run()\n",
    "    tflogdir = model.logdir\n",
    "    tflogdir = os.path.realpath(tflogdir)\n",
    "    if not os.path.exists(tflogdir):\n",
    "        os.makedirs(tflogdir)\n",
    "    print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
    "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
    "    train_writer.add_graph(sess.graph)\n",
    "    # vl=tf.trainable_variables()\n",
    "    # print(vl)\n",
    "    saver = tf.train.Saver()\n",
    "    checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "    # FFR: tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None,\n",
    "    # saver_def=None, collection_list=None, as_text=False, graph=None, export_scope=None,\n",
    "    # clear_devices=False, **kwargs)\n",
    "    start_iter = 0\n",
    "    if trainParams[\"restoreCheckpoints\"]:\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir,\n",
    "                                              latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "    av_batch_time=0.0\n",
    "    for iteration in range(start_iter, trainParams[\"max_iter\"]):\n",
    "        # Train with batches from the text library:\n",
    "        t1=time.time()\n",
    "        X_batch, y_batch = textlib.get_random_sample_batch(\n",
    "            batch_size, model.steps)\n",
    "        i_state = sess.run([model.init_state_0], feed_dict={model.X: X_batch})\n",
    "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
    "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
    "                                         model.init_state: i_state})\n",
    "        t2=time.time()\n",
    "        if av_batch_time==0.0:\n",
    "            av_batch_time=(t2-t1)*1000.0\n",
    "        else:\n",
    "            av_batch_time=(av_batch_time*5.0+(t2-t1)*1000.0)/6.0\n",
    "        \n",
    "        # Output training statistics every 100 iterations:\n",
    "        if iteration % 200 == 0:\n",
    "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
    "                                                          model.accuracy, model.prediction,\n",
    "                                                          model.summary_merged],\n",
    "                                                         feed_dict={model.X: X_batch, model.y: y_batch})\n",
    "            train_writer.add_summary(summary, iteration)\n",
    "            ep = iteration / epl\n",
    "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f} time per batch: {4:.5f}ms\".format(\n",
    "                ep, iteration, ce, accuracy, av_batch_time))\n",
    "            if trainParams[\"verbose\"]:\n",
    "                for ind in range(1):  # model.batch_size):\n",
    "                    ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
    "                    yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
    "                    print(\"   y:\", ys)\n",
    "                    print(\"  yp:\", yps)\n",
    "\n",
    "        # Generate sample texts for different temperature every ..NIter iterations:\n",
    "        if (iteration+1) % trainParams[\"statusEveryNIter\"] == 0:\n",
    "\n",
    "            # Save training data\n",
    "            # print(\"S>\")\n",
    "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
    "            # print(\"S<\")\n",
    "\n",
    "            if trainParams[\"generateDuringTraining\"]:\n",
    "                # Generate sample\n",
    "                for t in range(4, 11, 3):\n",
    "                    temp = float(t) / 10.0\n",
    "                    xs = ' ' * model.steps\n",
    "                    xso = ''\n",
    "                    doini = True\n",
    "                    for i in range(trainParams[\"generated_text_size\"]):\n",
    "                        X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                        if doini:\n",
    "                            doini = False\n",
    "                            g_state = sess.run(\n",
    "                                [model.init_state_0], feed_dict={model.X: X_new})\n",
    "\n",
    "                        g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                                   feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                              model.temperature: temp})\n",
    "                        inds = list(range(model.vocab_size))\n",
    "                        ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                        nc = textlib.i2c[ind]\n",
    "                        xso += nc\n",
    "                        xs = xs[1:]+nc\n",
    "\n",
    "                    print(\"----------------- temperature =\",\n",
    "                          temp, \"----------------------\")\n",
    "                    # print(xso)\n",
    "                    # 20: minimum quote size detected.\n",
    "                    textlib.source_highlight(xso, 20)\n",
    "                print(\"---------------------------------------\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sScszXSqQtf_"
   },
   "source": [
    "## 5. Generation of text from the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1P6TrXpkQtf_"
   },
   "outputs": [],
   "source": [
    "# Generating text using the model data generated during training.\n",
    "def ghostWriter(textsize, temperature=1.0):\n",
    "    xso = None\n",
    "    with tf.Session() as sess:\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
    "            return None\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return None\n",
    "\n",
    "        xs = ' ' * model.steps\n",
    "        xso = ''\n",
    "        doini = True\n",
    "        for i in range(textsize):\n",
    "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "            if doini:\n",
    "                doini = False\n",
    "                g_state = sess.run([model.init_state_0],\n",
    "                                   feed_dict={model.X: X_new})\n",
    "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                       feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                  model.temperature: temperature})\n",
    "            inds = list(range(model.vocab_size))\n",
    "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "            nc = textlib.i2c[ind]\n",
    "            xso += nc\n",
    "            xs = xs[1:]+nc\n",
    "    return(xso)\n",
    "\n",
    "\n",
    "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o5S9gO2iQtgD"
   },
   "outputs": [],
   "source": [
    "tgen=ghostWriter(500)\n",
    "detectPlagiarism(tgen, textlib)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## 6. A dialog with the trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "\n",
    "def doDialog():\n",
    "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'reset' to reset the conversation context,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        doini = True\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt == 'reset':\n",
    "                doini = True\n",
    "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                print(\"(conversation context marked for reset)\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t > 0.05 and t < 1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(1):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    if doini:\n",
    "                        doini = False\n",
    "                        g_state = sess.run(\n",
    "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                          model.temperature: temperature})\n",
    "            ans = 0\n",
    "            numEndPrompts = 0\n",
    "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                      model.temperature: temperature})\n",
    "                inds = list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
    "            textlib.source_highlight(xso, 13)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JEPK2WIQtgI"
   },
   "outputs": [],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "include_colab_link": true,
   "name": "tensor-poet.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

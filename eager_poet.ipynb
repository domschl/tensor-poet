{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_poet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiOXyvbEAPo",
        "colab_type": "code",
        "outputId": "896cfa9e-6f68-4e64-c67e-45f260fa54d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 366
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-gpu-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190731)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: tb-nightly<1.16.0a0,>=1.15.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0a20190731)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.7)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (2.3.2)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0.dev2019073000)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.4)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (3.1.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu-2.0-preview) (2.8.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "# [WIP] Eager Tensor Poet (tf 2.0)\n",
        "\n",
        "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation\n",
        "\n",
        "This code can use either CPU, GPU or TPU when running on Google Colab.\n",
        "\n",
        "Select the corresponding runtime (menu: Runtime / Change runtime type)\n",
        "\n",
        "Note: TPU support is not yet working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "from urllib.request import urlopen  # Py3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1",
        "colab_type": "text"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n",
        "\n",
        "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.14.x at the time of this writing. the version check below is preliminary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab_type": "code",
        "outputId": "d9726679-181a-45f4-e454-ee89f8ea085c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(\"Tensorflow api v2 active.\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P08FdKKnEAP6",
        "colab_type": "text"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWwUUfuEAP7",
        "colab_type": "code",
        "outputId": "ae419d5d-237a-44a3-e399-2b716545aba9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 116
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        print(\"         Hint: If using Google Colab, set runtime type to TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')]\n",
            "GPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU')]\n",
            "TPU -> []\n",
            "GPUs: ['/device:XLA_GPU:0', '/device:GPU:0']\n",
            "[('/device:CPU:0', ''), ('/device:XLA_GPU:0', 'device: XLA_GPU device'), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:GPU:0', 'device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "# TextLibrary class: text library for training, encoding, batch generation,\n",
        "# and formatted source display\n",
        "\n",
        "\n",
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJsgqNgEAQC",
        "colab_type": "text"
      },
      "source": [
        "### Read text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDo-bIPEAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 80\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=256\n",
        "    use_simple_model_for_tpu=True\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "LSTM_UNITS = 768\n",
        "EMBEDDING_DIM = 120\n",
        "LSTM_LAYERS = 6\n",
        "NUM_BATCHES=30"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeB7jugCV4lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dx=[]\n",
        "dy=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "    x,y=textlib.get_random_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
        "    dx.append(x)\n",
        "    dy.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3whYiFuwS8q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_xy=(dx,dy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boow8wR7sLwi",
        "colab_type": "code",
        "outputId": "0b7a815b-2350-41d2-d148-fbcfda26f1e7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "shuffle_buffer=10000\n",
        "dataset=textlib_dataset.shuffle(shuffle_buffer)\n",
        "dataset.take(1)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 80), (256, 80)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtDLPjGEAQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    *[tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform') for _ in range(lstm_layers)],\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "@tf.function\n",
        "def build_simple_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, SEQUENCE_LEN]),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform',\n",
        "                        unroll=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIxWHMGNKHk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# dev_strings=[]\n",
        "# for log_dev in tf.config.experimental.list_logical_devices('TPU'):\n",
        "#     dev_strings.append(log_dev.name)\n",
        "# print(dev_strings)\n",
        "\n",
        "# for i in range(8):\n",
        "#     dev_strings.append('/TPU:{}'.format(i))\n",
        "# print(dev_strings)\n",
        "\n",
        "if use_tpu:\n",
        "    print(TPU_ADDRESS)\n",
        "    os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaykJjEAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_tpu is True:\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.config.experimental_connect_to_host(cluster_resolver.master())\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)  # <-- this currently fails with colab/TPU\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
        "    \n",
        "    if use_simple_model_for_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_simple_model(\n",
        "              vocab_size = len(textlib.i2c),\n",
        "              embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_model(\n",
        "              vocab_size = len(textlib.i2c),\n",
        "              embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE)        \n",
        "else:\n",
        "    model = build_model(\n",
        "      vocab_size = len(textlib.i2c),\n",
        "      embedding_dim=EMBEDDING_DIM,\n",
        "      lstm_units=LSTM_UNITS,\n",
        "      lstm_layers=LSTM_LAYERS,\n",
        "      batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhF2vqmEAQo",
        "colab_type": "code",
        "outputId": "d2dbb40d-71ca-4cf4-df44-dc51ab161903",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 80, 89) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab_type": "code",
        "outputId": "469dc873-49b4-4609-93ae-710a271e161d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (256, None, 120)          10680     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (256, None, 768)          2731008   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 89)           68441     \n",
            "=================================================================\n",
            "Total params: 26,418,449\n",
            "Trainable params: 26,418,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYZs8Ss2947k",
        "colab_type": "code",
        "outputId": "421902c8-8df1-477b-e67b-435347732ce6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 80), (256, 80)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo54K1lEAQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOQdlNQEAQv",
        "colab_type": "code",
        "outputId": "b5a0959e-0bd9-4314-d052-0271146ecdf9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 100
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([87, 62,  9, 14, 71, 60, 37, 58, 35, 44, 48, 17,  0, 39, 61, 29, 75,\n",
              "       85, 39, 74, 41, 21, 61, 43, 46, 20, 40, 20,  9, 63, 78, 46, 70, 49,\n",
              "       33, 19, 64, 44, 88, 42, 18, 85,  8, 36, 80, 47, 19, 40, 68, 62, 20,\n",
              "        0, 50, 74,  1, 27, 58, 15, 46, 10, 32, 65, 65, 40, 62, 53, 51, 87,\n",
              "        2, 54, 51, 49, 26, 62, 18, 80, 13, 84, 57, 38])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4RuDZLEAQy",
        "colab_type": "code",
        "outputId": "64dcfc9a-f1a5-45c8-8572-74bb36e4905d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        }
      },
      "source": [
        "textlib.decode(sampled_indices)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'}/tg6ULHY5#kTANK;$AxDaN10dRdt(X093vi)5`Sf$c-?[iRV/dT7xh\\nHE0Gm__R/C4}eI43\\r/f?b@F:'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-0Y2uYEAQ0",
        "colab_type": "code",
        "outputId": "44e8f5cb-b534-4713-908d-f383a1e81e10",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 80, 89)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4883966\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py9WnmosEAQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# adam_clipped = tf.keras.optimizers.Adam(lr=0.003, clipvalue=1.0)\n",
        "adam_clipped = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
        "\n",
        "model.compile(optimizer=adam_clipped, loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKvObcsEAQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "colab_type": "code",
        "outputId": "10c4a48f-d211-41f3-c8c9-959870d90234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 15:08:14.861467 140320682334080 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:457: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "30/30 [==============================] - 62s 2s/step - loss: 3.4082\n",
            "Epoch 2/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1841\n",
            "Epoch 3/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1822\n",
            "Epoch 4/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1824\n",
            "Epoch 5/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1823\n",
            "Epoch 6/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1823\n",
            "Epoch 7/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 3.1824\n",
            "Epoch 8/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1825\n",
            "Epoch 9/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 3.1824\n",
            "Epoch 10/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1823\n",
            "Epoch 11/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1821\n",
            "Epoch 12/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1824\n",
            "Epoch 13/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1823\n",
            "Epoch 14/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1822\n",
            "Epoch 15/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 3.1822\n",
            "Epoch 16/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1822\n",
            "Epoch 17/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1821\n",
            "Epoch 18/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1808\n",
            "Epoch 19/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1756\n",
            "Epoch 20/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.1688\n",
            "Epoch 21/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 3.0789\n",
            "Epoch 22/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.8912\n",
            "Epoch 23/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.7295\n",
            "Epoch 24/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.5532\n",
            "Epoch 25/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.4106\n",
            "Epoch 26/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.2971\n",
            "Epoch 27/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.2111\n",
            "Epoch 28/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.1334\n",
            "Epoch 29/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.0727\n",
            "Epoch 30/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 2.0474\n",
            "Epoch 31/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.9660\n",
            "Epoch 32/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.9150\n",
            "Epoch 33/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.8708\n",
            "Epoch 34/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.8356\n",
            "Epoch 35/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.7936\n",
            "Epoch 36/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.8355\n",
            "Epoch 37/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.7306\n",
            "Epoch 38/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.6937\n",
            "Epoch 39/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.6662\n",
            "Epoch 40/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.6374\n",
            "Epoch 41/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.6081\n",
            "Epoch 42/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.5854\n",
            "Epoch 43/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.5607\n",
            "Epoch 44/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.5384\n",
            "Epoch 45/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.5131\n",
            "Epoch 46/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4933\n",
            "Epoch 47/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4762\n",
            "Epoch 48/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4576\n",
            "Epoch 49/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4382\n",
            "Epoch 50/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4197\n",
            "Epoch 51/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.4043\n",
            "Epoch 52/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3890\n",
            "Epoch 53/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3770\n",
            "Epoch 54/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3596\n",
            "Epoch 55/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3486\n",
            "Epoch 56/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3356\n",
            "Epoch 57/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3180\n",
            "Epoch 58/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.3094\n",
            "Epoch 59/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2946\n",
            "Epoch 60/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2833\n",
            "Epoch 61/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2679\n",
            "Epoch 62/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2528\n",
            "Epoch 63/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2394\n",
            "Epoch 64/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2310\n",
            "Epoch 65/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.2133\n",
            "Epoch 66/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.2055\n",
            "Epoch 67/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.1892\n",
            "Epoch 68/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.1710\n",
            "Epoch 69/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.1607\n",
            "Epoch 70/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.1485\n",
            "Epoch 71/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.1341\n",
            "Epoch 72/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.1150\n",
            "Epoch 73/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.1084\n",
            "Epoch 74/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.0866\n",
            "Epoch 75/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.0838\n",
            "Epoch 76/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 1.0613\n",
            "Epoch 77/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.0451\n",
            "Epoch 78/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.0237\n",
            "Epoch 79/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 1.0128\n",
            "Epoch 80/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.9918\n",
            "Epoch 81/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.9783\n",
            "Epoch 82/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.9585\n",
            "Epoch 83/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.9418\n",
            "Epoch 84/100\n",
            "30/30 [==============================] - 57s 2s/step - loss: 0.9201\n",
            "Epoch 85/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.9101\n",
            "Epoch 86/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8890\n",
            "Epoch 87/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8859\n",
            "Epoch 88/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8606\n",
            "Epoch 89/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8415\n",
            "Epoch 90/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8235\n",
            "Epoch 91/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8119\n",
            "Epoch 92/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.8034\n",
            "Epoch 93/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.7885\n",
            "Epoch 94/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.7739\n",
            "Epoch 95/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.7594\n",
            "Epoch 96/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.7322\n",
            "Epoch 97/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.7113\n",
            "Epoch 98/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.6895\n",
            "Epoch 99/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.6791\n",
            "Epoch 100/100\n",
            "30/30 [==============================] - 58s 2s/step - loss: 0.6719\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvupep98EARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMrNH0eEARC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "934821c7-f6c3-4f2c-a80d-50da49a5d784"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_100'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7qRl5EEARE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gen_model = build_model(vocab_size = len(textlib.i2c),\n",
        "  embedding_dim=EMBEDDING_DIM,\n",
        "  lstm_units=LSTM_UNITS,\n",
        "  lstm_layers=LSTM_LAYERS,\n",
        "  batch_size=1)\n",
        "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "gen_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6JhIGxEARF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        },
        "outputId": "0054e90f-f0dc-4b4d-fb8e-1b5f116d64b0"
      },
      "source": [
        "gen_model.summary()"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 120)            10680     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, None, 768)            2731008   \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 89)             68441     \n",
            "=================================================================\n",
            "Total params: 26,418,449\n",
            "Trainable params: 26,418,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLmNIvzEARH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string, temp=0.6):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [textlib.c2i[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7750ibzEARJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mbC9l8EARL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tql-eLGvEARO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "bdb07d25-f174-4e9c-8889-64a3b330d4f9"
      },
      "source": [
        "txt=textlib.decode(id)\n",
        "txti=txt.split('\\r\\n')\n",
        "for t in txti:\n",
        "    print(t)"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Ellen who had seen them\n",
            "in the conservatory to a pretty terrible place than she had taken up the study\n",
            "of botany since her daughter married, and it was to be talked of beauty of her resemblence\n",
            "in her kyes, and to the love of the place,\n",
            "the streets, the people who said the long time time to me.  It had no sooner beneath here before them.\n",
            "\n",
            "\"Who writes the best Latin verse in your college, Mr. Woodhouse's spirits,\n",
            "which terred to the time, it was not very likely, from Richmond Catherine letter too. In the stone on the same spot now. It was a laughtouble protector, which he may be\n",
            "recognised to love each other. The poke floom was too polite not to continue the conversation of the stairs of the subject.--The\n",
            "bad no more formerly inviting a chind to eat\n",
            "attempt to write like a man. Every other woman does not attempt to the place and half a stone compliment\n",
            "in her knew to be settled and perfectlious still, only to a some people sitting by the\n",
            "tentation of a statesman in Section \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBil008pEARS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        },
        "outputId": "997e6bce-7bae-473c-e5b4-4f5e9592f5ff"
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "W<span style=\"background-color:#e2d7d5;\">ith the cl</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">arity of th</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">ought of an</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"> artificial li</span><sup>[3]</sup>fe for<span style=\"background-color:#eadbd8;\">m, the dis</span><sup>[3]</sup>cussi<span style=\"background-color:#e2d7d5;\">on went on</span><sup>[4]</sup>: Elle<span style=\"background-color:#eadbd8;\">n who had see</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">n them\r<br>in</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"> the conservatory</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\"> to a prett</span><sup>[2]</sup>y<span style=\"background-color:#eadbd8;\"> terrible p</span><sup>[3]</sup>lac<span style=\"background-color:#eadbd8;\">e than she had </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">taken up the study\r<br>of botany since her daughter married, and it was </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">to be talked o</span><sup>[4]</sup>f<span style=\"background-color:#e2d7d5;\"> beauty of her</span><sup>[4]</sup> resemblen<span style=\"background-color:#e2d7d5;\">ce\r<br>in her </span><sup>[4]</sup>k<span style=\"background-color:#eadbd8;\">yes, and to </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">the love of </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">the place,\r<br>the streets, the people</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\"> who said </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">the long t</span><sup>[3]</sup>i<span style=\"background-color:#eadbd8;\">me time to </span><sup>[3]</sup>me<span style=\"background-color:#ebdef0;\">.  It had </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">no sooner </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">beneath her</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">e before them</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">.\r<br>\r<br>\"Who writes the best Latin verse in your college, </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">Mr. Woodhouse's spirits,\r<br>which</span><sup>[4]</sup> ter<span style=\"background-color:#eadbd8;\">red to the t</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">ime, it was </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">not very likely, </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">from Richmond </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">Catherine le</span><sup>[2]</sup>tter<span style=\"background-color:#eadbd8;\"> too. In th</span><sup>[3]</sup>e<span style=\"background-color:#eadbd8;\"> stone on the </span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">same spot now</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">. It was a l</span><sup>[4]</sup>aughtouble<span style=\"background-color:#ebdef0;\"> protector, </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">which he may </span><sup>[2]</sup>be\r<br><span style=\"background-color:#eadbd8;\">recognised t</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">o love each other</span><sup>[3]</sup>. The poke floom<span style=\"background-color:#eadbd8;\"> was too polite not to continue the conversation</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\"> of the stairs</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\"> of the subject</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">.--The\r<br>ba</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">d no more for</span><sup>[4]</sup>merly<span style=\"background-color:#eadbd8;\"> inviting </span><sup>[3]</sup>a chin<span style=\"background-color:#e2d7d5;\">d to eat\r<br></span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">attempt to write like a man. Every other woman does</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\"> not attempt to th</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">e place and </span><sup>[4]</sup>half a sto<span style=\"background-color:#e2d7d5;\">ne compliment</span><sup>[4]</sup>\r<br>in her<span style=\"background-color:#eadbd8;\"> knew to be </span><sup>[3]</sup>settled<span style=\"background-color:#e2d7d5;\"> and perfectl</span><sup>[4]</sup>ious still<span style=\"background-color:#e2d7d5;\">, only to </span><sup>[4]</sup>a<span style=\"background-color:#e2d7d5;\"> some people s</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">itting by the\r<br></span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">tentation of</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\"> a statesman in </span><sup>[3]</sup>Section "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup>, <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYmlO_IEARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model [not ported yet]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
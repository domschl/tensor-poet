{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_poet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5oK6Ej8OQtf1",
        "48X8-lFFQtf3",
        "zOlVTPtFQtf7",
        "sScszXSqQtf_",
        "WW6LPdlhQtgF"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiOXyvbEAPo",
        "colab_type": "code",
        "outputId": "737c5250-81d9-4f65-e8ad-bcb05e915aee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview # tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3b/21564412886837f45b57067eefa7f9b69e61e032fc188b57496c5391b76b/tf_nightly_gpu_2.0_preview-2.0.0.dev20190517-cp36-cp36m-manylinux1_x86_64.whl (348.7MB)\n",
            "\u001b[K     |████████████████████████████████| 348.7MB 48kB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 19.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/75/5107f9c9c2106e3765a6b22d611005aac4e29c29a5142ee897847776dc17/tensorflow_estimator_2.0_preview-1.14.0.dev2019051700-py2.py3-none-any.whl (427kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 39.4MB/s \n",
            "\u001b[?25hCollecting wrapt>=1.11.1 (from tf-nightly-gpu-2.0-preview)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/33/79/14092647d35fd057bd33c338f59da920b1f9071c384530f03632b12950e8/tb_nightly-1.14.0a20190516-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 33.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: google-pasta, tensorflow-estimator-2.0-preview, wrapt, tb-nightly, tf-nightly-gpu-2.0-preview\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190516 tensorflow-estimator-2.0-preview-1.14.0.dev2019051700 tf-nightly-gpu-2.0-preview-2.0.0.dev20190517 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s78tdpcEAPt",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "# [WIP] Eager Tensor Poet (tf 2.0)\n",
        "\n",
        "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "from urllib.request import urlopen  # Py3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e4yYLii8Qtfw"
      },
      "source": [
        "### Content\n",
        "This notebook contains the following sections:\n",
        "1. TextLibrary: utilities to work with text files\n",
        "  * loading of a list of files (local or URLs)\n",
        "  * encoding for training\n",
        "  * formatted output with quote-highlighting\n",
        "2. Transform text data to tf.data\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "x. Definition of the tensorflow model\n",
        "x. Model and training parameters\n",
        "x. The actual training on the data (required 1. - 3.)\n",
        "  * Training can be restarted, since the model is saved periodically.\n",
        "x. Generation of text from the trained model (requires 1. - 4.)\n",
        "x. In dialog with with the model (requires 1. - 4.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1",
        "colab_type": "text"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n",
        "\n",
        "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.13.x at the time of this writing. the version check below is preliminary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab_type": "code",
        "outputId": "63189f5f-760e-4c0d-e021-bd4e20f42899",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(\"Tensorflow api v2 active.\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P08FdKKnEAP6",
        "colab_type": "text"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWwUUfuEAP7",
        "colab_type": "code",
        "outputId": "47f7452c-99af-4bfd-a0b5-c9c66d8ba784",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        print(\"         Hint: If using Google Colab, set runtime type to TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/job:worker/replica:0/task:0/device:CPU:0', device_type='CPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:CPU:0', device_type='CPU')]\n",
            "GPU -> []\n",
            "TPU -> [LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:0', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:1', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:2', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:3', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:4', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:5', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:6', device_type='TPU'), LogicalDevice(name='/job:worker/replica:0/task:1/device:TPU:7', device_type='TPU')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "# TextLibrary class: text library for training, encoding, batch generation,\n",
        "# and formatted source display\n",
        "\n",
        "\n",
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = descriptor\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = os.path.splitext(os.path.basename(descriptor))[0]\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJsgqNgEAQC",
        "colab_type": "text"
      },
      "source": [
        "### Read text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDo-bIPEAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"TinyShakespeare\",\n",
        "    \"description\": \"Small Shakespeare 'standard' corpus\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        'http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt',\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FI6hU4tNGoma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# if use_tpu is True:\n",
        "#     resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n",
        "#     tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "#     tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68E8t2qEAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = textlib.get_encoded_slice_array(len(textlib.data))\n",
        "textlib_dataset = tf.data.Dataset.from_tensor_slices(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHNgHiH1EAQK",
        "colab_type": "code",
        "outputId": "04f0250a-c81e-4c0f-888b-c85904dcf597",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 179
        }
      },
      "source": [
        "# Quick test\n",
        "n=np.array([])\n",
        "for i in textlib_dataset.take(90):\n",
        "    n=np.append(n,i.numpy())\n",
        "print(n)    \n",
        "print(textlib.decode(n))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11.  8.  6. 12. 13.  6.  3.\n",
            " 14. 15. 16.  9. 17. 18.  6.  9. 19.  4. 20. 21. 22.  6.  8.  6.  9. 23.\n",
            "  4.  3. 24. 16.  9.  4. 25.  9. 23. 26. 22. 22. 26. 27. 20.  9. 28. 18.\n",
            " 27. 24.  6. 16. 21.  6. 27.  3.  6. 29.  9. 13. 30.  9. 23. 26. 22. 22.\n",
            " 26. 27. 20.  0.  1. 28. 18. 27. 24.  6. 16. 21.  6. 27.  3.  6.  0.  1.]\n",
            "\r\n",
            "Project Gutenberg’s The Complete Works of William Shakespeare, by William\r\n",
            "Shakespeare\r\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mnBkXfEAQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 80\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=1024\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "LSTM_UNITS = 1024\n",
        "EMBEDDING_DIM = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o0v62LKEAQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_size=len(data)//SEQUENCE_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgScEoozEAQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=textlib_dataset.batch(SEQUENCE_LEN+1,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg_gWvzqEAQT",
        "colab_type": "code",
        "outputId": "504f0248-46da-45ac-f8a8-70c73a37516a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "# Quick test\n",
        "for arr in sequences.take(3):\n",
        "    n=arr.numpy()\n",
        "    print(arr)\n",
        "    print(\">\"+textlib.decode(n))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11  8  6 12 13  6  3 14 15 16  9 17 18\n",
            "  6  9 19  4 20 21 22  6  8  6  9 23  4  3 24 16  9  4 25  9 23 26 22 22\n",
            " 26 27 20  9 28 18 27 24  6 16 21  6 27  3  6 29  9 13 30  9 23 26 22 22\n",
            " 26 27 20  0  1 28 18 27 24], shape=(81,), dtype=int64)\n",
            ">\r\n",
            "Project Gutenberg’s The Complete Works of William Shakespeare, by William\r\n",
            "Shak\n",
            "tf.Tensor(\n",
            "[ 6 16 21  6 27  3  6  0  1  0  1 17 18 26 16  9  6 31  4  4 24  9 26 16\n",
            "  9 25  4  3  9  8 18  6  9 11 16  6  9  4 25  9 27 12 30  4 12  6  9 27\n",
            " 12 30 32 18  6  3  6  9 26 12  9  8 18  6  9 33 12 26  8  6 34  9 28  8\n",
            " 27  8  6 16  9 27 12 34  0], shape=(81,), dtype=int64)\n",
            ">espeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n",
            "tf.Tensor(\n",
            "[ 1 20  4 16  8  9  4  8 18  6  3  9 21 27  3  8 16  9  4 25  9  8 18  6\n",
            "  9 32  4  3 22 34  9 27  8  9 12  4  9  7  4 16  8  9 27 12 34  9 32 26\n",
            "  8 18  9 27 22 20  4 16  8  9 12  4  9  3  6 16  8  3 26  7  8 26  4 12\n",
            " 16  0  1 32 18 27  8 16  4], shape=(81,), dtype=int64)\n",
            ">\n",
            "most other parts of the world at no cost and with almost no restrictions\r\n",
            "whatso\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWvfNurDEAQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMYWYbjEAQa",
        "colab_type": "code",
        "outputId": "99478764-1e1c-46e7-afb6-4f6caf284786",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Quick test\n",
        "for input_text, output_text in dataset.take(2):\n",
        "    print(\"I:\"+textlib.decode(input_text.numpy()))\n",
        "    print(\"O:\"+textlib.decode(output_text.numpy()))"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:\r\n",
            "Project Gutenberg’s The Complete Works of William Shakespeare, by William\r\n",
            "Sha\n",
            "O:\n",
            "Project Gutenberg’s The Complete Works of William Shakespeare, by William\r\n",
            "Shak\n",
            "I:espeare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\n",
            "O:speare\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere in the United States and\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7tYcLdEAQd",
        "colab_type": "code",
        "outputId": "00077f11-7525-44b3-e407-587a64205f26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 100000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((1024, 80), (1024, 80)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtDLPjGEAQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIxWHMGNKHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "3501c806-baf5-4473-bec3-17bcde383bb3"
      },
      "source": [
        "dev_strings=[]\n",
        "for log_dev in tf.config.experimental.list_logical_devices('TPU'):\n",
        "    dev_strings.append(log_dev.name)\n",
        "print(dev_strings)\n",
        "\n",
        "# for i in range(8):\n",
        "#     dev_strings.append('/TPU:{}'.format(i))\n",
        "# print(dev_strings)\n",
        "    "
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['/job:worker/replica:0/task:1/device:TPU:0', '/job:worker/replica:0/task:1/device:TPU:1', '/job:worker/replica:0/task:1/device:TPU:2', '/job:worker/replica:0/task:1/device:TPU:3', '/job:worker/replica:0/task:1/device:TPU:4', '/job:worker/replica:0/task:1/device:TPU:5', '/job:worker/replica:0/task:1/device:TPU:6', '/job:worker/replica:0/task:1/device:TPU:7']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaykJjEAQl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "42fab9fb-4cdd-453e-ee7e-feb0132ae8c6"
      },
      "source": [
        "if use_tpu is True:\n",
        "    # tpus=tf.config.experimental.list_logical_devices('TPU')\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)    \n",
        "    # mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(dev_strings)\n",
        "    # with mirrored_strategy.scope():\n",
        "    with tpu_strategy.scope():\n",
        "        model = build_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          embedding_dim=EMBEDDING_DIM,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          batch_size=BATCH_SIZE)\n",
        "else:\n",
        "    model = build_model(\n",
        "      vocab_size = len(textlib.i2c),\n",
        "      embedding_dim=EMBEDDING_DIM,\n",
        "      lstm_units=LSTM_UNITS,\n",
        "      batch_size=BATCH_SIZE)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNotFoundError\u001b[0m                             Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-17e62dc5239b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;31m# tpus=tf.config.experimental.list_logical_devices('TPU')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mresolver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcluster_resolver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUClusterResolver\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTPU_ADDRESS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtpu\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minitialize_tpu_system\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mtpu_strategy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTPUStrategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresolver\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(dev_strings)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/tpu/tpu_strategy_util.py\u001b[0m in \u001b[0;36minitialize_tpu_system\u001b[0;34m(cluster_resolver)\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtpu_system_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m       \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tpu_init_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m     \u001b[0mserialized_topology\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1320\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1321\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m    576\u001b[0m     \"\"\"\n\u001b[1;32m    577\u001b[0m     return self._call_flat(\n\u001b[0;32m--> 578\u001b[0;31m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[0m\u001b[1;32m    579\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m    580\u001b[0m                            resource_variable_ops.ResourceVariable))))\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_keras_symbolic_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
            "\u001b[0;31mNotFoundError\u001b[0m: No registered 'ConfigureDistributedTPU' OpKernel for TPU_SYSTEM devices compatible with node {{node ConfigureDistributedTPU}}\n\t.  Registered:  <no registered kernels>\n\n\t [[{{node ConfigureDistributedTPU}}]]\nAdditional GRPC error information:\n{\"created\":\"@1558087736.540085540\",\"description\":\"Error received from peer\",\"file\":\"external/grpc/src/core/lib/surface/call.cc\",\"file_line\":1039,\"grpc_message\":\"No registered 'ConfigureDistributedTPU' OpKernel for TPU_SYSTEM devices compatible with node {{node ConfigureDistributedTPU}}\\n\\t.  Registered:  <no registered kernels>\\n\\n\\t [[{{node ConfigureDistributedTPU}}]]\",\"grpc_status\":5} [Op:__inference__tpu_init_fn_252]"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhF2vqmEAQo",
        "colab_type": "code",
        "outputId": "f6e88bd5-0039-4676-8920-796fa9ac588a",
        "colab": {}
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 80, 107) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab_type": "code",
        "outputId": "38f2cc96-327b-47bd-9316-6fe08747182c",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (256, None, 512)          54784     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (256, None, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (256, None, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (256, None, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (256, None, 107)          109675    \n",
            "=================================================================\n",
            "Total params: 23,245,419\n",
            "Trainable params: 23,245,419\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo54K1lEAQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOQdlNQEAQv",
        "colab_type": "code",
        "outputId": "8222a451-159b-482a-af08-a56439936546",
        "colab": {}
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 43,  11,  24,  29,  96,  32,  98,  71,  51,   8, 105,  87, 102,\n",
              "        25,  25,  33,  97,  44,  95,  73,  95,   5,  73,  41,  96,   3,\n",
              "        96,  13,  49,   3,  61,   5,  75,  77,  44,  12,  48,  62, 100,\n",
              "        56,  85,  93,  64,   2,   7,  82,  97,   1,  36,  20,  21,  71,\n",
              "        35, 105,  93,  27,  99,  61,  29,  15,   9,  65,  22,  64,  71,\n",
              "        12,  52,   2, 103,  61,  55,  22,  85,  56,  65,  27,  82, 104,\n",
              "        71,  29])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4RuDZLEAQy",
        "colab_type": "code",
        "outputId": "904c8188-61eb-422b-ecd7-218d374fbeb9",
        "colab": {}
      },
      "source": [
        "textlib.decode(sampled_indices)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Ouk,çwê‘)t@|\\\\ffUîNâ?âj?:çrçb1rMj5!Nn03\\t9\"é8Pc”î\\n.mp‘v@éa`M,’ Hl8‘nAP/MJl\"9Ha”%‘,'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-0Y2uYEAQ0",
        "colab_type": "code",
        "outputId": "f4ab5499-1fff-4337-fc91-87d90dd05c07",
        "colab": {}
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 80, 107)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.672822\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py9WnmosEAQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKvObcsEAQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "colab_type": "code",
        "outputId": "ca4056d7-f3ca-4907-cc91-703f703cc5a4",
        "colab": {}
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "163/277 [================>.............] - ETA: 2:47 - loss: 2.9477"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvupep98EARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMrNH0eEARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7qRl5EEARE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = len(textlib.i2c),\n",
        "  embedding_dim=EMBEDDING_DIM,\n",
        "  lstm_units=LSTM_UNITS,\n",
        "  batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6JhIGxEARF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLmNIvzEARH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [textlib.c2i[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = .40\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7750ibzEARJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx,id=generate_text(model, start_string=\"ROMEO: \")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mbC9l8EARL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tql-eLGvEARO",
        "colab_type": "code",
        "outputId": "b12b4e88-fbc3-43cc-a704-8853404f960f",
        "colab": {}
      },
      "source": [
        "textlib.decode(id)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'textlib' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-22c1e6dad2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtextlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'textlib' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBil008pEARS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYmlO_IEARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV",
        "colab_type": "text"
      },
      "source": [
        "**below this point not yet ported**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oK6Ej8OQtf1"
      },
      "source": [
        "## 2. Definition of the Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GP4sk47FQtf1",
        "colab": {}
      },
      "source": [
        "# The tensorflow model for text generation\n",
        "class TensorPoetModel:\n",
        "    def __init__(self, params):\n",
        "        self.model_name = params[\"model_name\"]\n",
        "        self.vocab_size = params[\"vocab_size\"]\n",
        "        self.neurons = params[\"neurons\"]\n",
        "        self.layers = params[\"layers\"]\n",
        "        self.learning_rate = params[\"learning_rate\"]\n",
        "        self.steps = params[\"steps\"]\n",
        "        self.logdir = params[\"logdir\"]\n",
        "        self.checkpoint = params[\"checkpoint\"]\n",
        "        # self.clip = -1.0 * params[\"clip\"]\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        # Training & Generating:\n",
        "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "\n",
        "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
        "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
        "\n",
        "        stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(\n",
        "            self.neurons, name='basic_lstm_cell') for _ in range(self.layers)])\n",
        "\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "\n",
        "        self.init_state_0 = stacked_cell.zero_state(batch_size, tf.float32)\n",
        "        self.init_state = self.init_state_0\n",
        "\n",
        "        with tf.variable_scope('rnn') as scope:\n",
        "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X,\n",
        "                                                    initial_state=self.init_state,\n",
        "                                                    dtype=tf.float32)\n",
        "            self.init_state = states\n",
        "\n",
        "        self.final_state = self.init_state\n",
        "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
        "\n",
        "        softmax_w = tf.Variable(tf.random_normal(\n",
        "            [self.neurons, self.vocab_size]), dtype=tf.float32, name='sm_w')\n",
        "        softmax_b = tf.Variable(\n",
        "            [self.vocab_size], dtype=tf.float32, name='sm_b')\n",
        "\n",
        "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
        "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
        "\n",
        "        output_softmax = tf.nn.softmax(logits)\n",
        "\n",
        "        self.temperature = tf.placeholder(tf.float32)\n",
        "        self.output_softmax_temp = tf.nn.softmax(\n",
        "            tf.div(logits, self.temperature))\n",
        "\n",
        "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=onehot_y, logits=logits)\n",
        "\n",
        "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "\n",
        "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
        "\n",
        "        # Clipping isn't necessary, even for really deep networks:\n",
        "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
        "        # minclip = -1.0 * self.clip\n",
        "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) \n",
        "        #     for grad, var in grads]\n",
        "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
        "\n",
        "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
        "        correct_prediction = tf.equal(self.y, self.prediction)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        error = 1.0 - self.accuracy\n",
        "\n",
        "        # Tensorboard\n",
        "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
        "        tf.summary.scalar(\"error\", error)\n",
        "        self.summary_merged = tf.summary.merge_all()\n",
        "\n",
        "        # Init\n",
        "        self.init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "48X8-lFFQtf3"
      },
      "source": [
        "## 3. Parameters for model and training\n",
        "\n",
        "The library description `libdesc` contains a list in `lib` with local filenames of text-files or http, https URLs pointing to text files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhMGIIPEQtf3",
        "colab": {}
      },
      "source": [
        "\n",
        "# Model parameter:\n",
        "modelParamsShakespeare = {\n",
        "    \"model_name\": \"shakespeare\",\n",
        "    \"logdir\": \"tensorlog/shakespeare\",\n",
        "    \"checkpoint\": \"shakespeare.ckpt\",\n",
        "    \"vocab_size\": len(textlib.i2c),\n",
        "    \"neurons\": 512,\n",
        "    \"layers\": 4,\n",
        "    \"learning_rate\": 4.e-4,\n",
        "    \"steps\": 128,\n",
        "}\n",
        "\n",
        "# Look for optional json description of a library:\n",
        "if os.path.exists('bk/lib-phil-deen.json'):\n",
        "    with open('bk/lib-phil-deen.json') as data_file:    \n",
        "        libdescphil = json.load(data_file)\n",
        "        textlib = TextLibrary(libdescphil[\"lib\"])\n",
        "        modelParamsPhil = {\n",
        "            \"model_name\": \"phil\",\n",
        "            \"logdir\": \"tensorlog/phil\",\n",
        "            \"checkpoint\": \"phil.ckpt\",\n",
        "            \"vocab_size\": len(textlib.i2c),\n",
        "            \"neurons\": 256,\n",
        "            \"layers\": 8,\n",
        "            \"learning_rate\": 1.e-3,\n",
        "            \"steps\": 128,\n",
        "        }\n",
        "        model = TensorPoetModel(modelParamsPhil)\n",
        "else:\n",
        "    model = TensorPoetModel(modelParamsShakespeare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ciDOIUiQtf6",
        "colab": {}
      },
      "source": [
        "# Training Parameter:\n",
        "\n",
        "trainParams = {\n",
        "    \"max_iter\": 1000000,\n",
        "    \"restoreCheckpoints\": False,\n",
        "    \"generateDuringTraining\": True,\n",
        "    \"generated_text_size\": 200,\n",
        "    \"verbose\": True,\n",
        "    \"statusEveryNIter\": 500,\n",
        "    \"saveEveryNIter\": 500,\n",
        "    \"batch_size\": 128,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zOlVTPtFQtf7"
      },
      "source": [
        "## 4. The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3x0U5-0dQtf9",
        "colab": {}
      },
      "source": [
        "# Run training:\n",
        "with tf.Session() as sess:\n",
        "    batch_size = trainParams[\"batch_size\"]\n",
        "    epl = len(textlib.data) / (batch_size * model.steps)\n",
        "    model.init.run()\n",
        "    tflogdir = model.logdir\n",
        "    tflogdir = os.path.realpath(tflogdir)\n",
        "    if not os.path.exists(tflogdir):\n",
        "        os.makedirs(tflogdir)\n",
        "    print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
        "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
        "    train_writer.add_graph(sess.graph)\n",
        "    # vl=tf.trainable_variables()\n",
        "    # print(vl)\n",
        "    saver = tf.train.Saver()\n",
        "    checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "    # FFR: tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None,\n",
        "    # saver_def=None, collection_list=None, as_text=False, graph=None, export_scope=None,\n",
        "    # clear_devices=False, **kwargs)\n",
        "    start_iter = 0\n",
        "    if trainParams[\"restoreCheckpoints\"]:\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir,\n",
        "                                              latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "    av_batch_time=0.0\n",
        "    for iteration in range(start_iter, trainParams[\"max_iter\"]):\n",
        "        # Train with batches from the text library:\n",
        "        t1=time.time()\n",
        "        X_batch, y_batch = textlib.get_random_sample_batch(\n",
        "            batch_size, model.steps)\n",
        "        i_state = sess.run([model.init_state_0], feed_dict={model.X: X_batch})\n",
        "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
        "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
        "                                         model.init_state: i_state})\n",
        "        t2=time.time()\n",
        "        if av_batch_time==0.0:\n",
        "            av_batch_time=(t2-t1)*1000.0\n",
        "        else:\n",
        "            av_batch_time=(av_batch_time*5.0+(t2-t1)*1000.0)/6.0\n",
        "        \n",
        "        # Output training statistics every 100 iterations:\n",
        "        if iteration % 200 == 0:\n",
        "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
        "                                                          model.accuracy, model.prediction,\n",
        "                                                          model.summary_merged],\n",
        "                                                         feed_dict={model.X: X_batch, model.y: y_batch})\n",
        "            train_writer.add_summary(summary, iteration)\n",
        "            ep = iteration / epl\n",
        "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f} time per batch: {4:.5f}ms\".format(\n",
        "                ep, iteration, ce, accuracy, av_batch_time))\n",
        "            if trainParams[\"verbose\"]:\n",
        "                for ind in range(1):  # model.batch_size):\n",
        "                    ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
        "                    yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
        "                    print(\"   y:\", ys)\n",
        "                    print(\"  yp:\", yps)\n",
        "\n",
        "        # Generate sample texts for different temperature every ..NIter iterations:\n",
        "        if (iteration+1) % trainParams[\"statusEveryNIter\"] == 0:\n",
        "\n",
        "            # Save training data\n",
        "            # print(\"S>\")\n",
        "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
        "            # print(\"S<\")\n",
        "\n",
        "            if trainParams[\"generateDuringTraining\"]:\n",
        "                # Generate sample\n",
        "                for t in range(4, 11, 3):\n",
        "                    temp = float(t) / 10.0\n",
        "                    xs = ' ' * model.steps\n",
        "                    xso = ''\n",
        "                    doini = True\n",
        "                    for i in range(trainParams[\"generated_text_size\"]):\n",
        "                        X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                        if doini:\n",
        "                            doini = False\n",
        "                            g_state = sess.run(\n",
        "                                [model.init_state_0], feed_dict={model.X: X_new})\n",
        "\n",
        "                        g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                                   feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                              model.temperature: temp})\n",
        "                        inds = list(range(model.vocab_size))\n",
        "                        ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                        nc = textlib.i2c[ind]\n",
        "                        xso += nc\n",
        "                        xs = xs[1:]+nc\n",
        "\n",
        "                    print(\"----------------- temperature =\",\n",
        "                          temp, \"----------------------\")\n",
        "                    # print(xso)\n",
        "                    # 20: minimum quote size detected.\n",
        "                    textlib.source_highlight(xso, 20)\n",
        "                print(\"---------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sScszXSqQtf_"
      },
      "source": [
        "## 5. Generation of text from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P6TrXpkQtf_",
        "colab": {}
      },
      "source": [
        "# Generating text using the model data generated during training.\n",
        "def ghostWriter(textsize, temperature=1.0):\n",
        "    xso = None\n",
        "    with tf.Session() as sess:\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
        "            return None\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return None\n",
        "\n",
        "        xs = ' ' * model.steps\n",
        "        xso = ''\n",
        "        doini = True\n",
        "        for i in range(textsize):\n",
        "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "            if doini:\n",
        "                doini = False\n",
        "                g_state = sess.run([model.init_state_0],\n",
        "                                   feed_dict={model.X: X_new})\n",
        "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                       feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                  model.temperature: temperature})\n",
        "            inds = list(range(model.vocab_size))\n",
        "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "            nc = textlib.i2c[ind]\n",
        "            xso += nc\n",
        "            xs = xs[1:]+nc\n",
        "    return(xso)\n",
        "\n",
        "\n",
        "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5S9gO2iQtgD",
        "colab": {}
      },
      "source": [
        "tgen=ghostWriter(500)\n",
        "detectPlagiarism(tgen, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
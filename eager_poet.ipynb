{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "@webio": {
      "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
      "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
    },
    "accelerator": "GPU",
    "colab": {
      "name": "Copy of eager_poet.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fEXNOWhCEAPk"
      },
      "source": [
        "# Eager Tensor Poet (Tensorflow 2.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wM8iyPzWuc8G"
      },
      "source": [
        "### Only execute next block, if you want to test with different TF runtime.\n",
        "\n",
        "Remember to restart runtime after installing new software."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b7hgtQU725ap",
        "colab": {}
      },
      "source": [
        "# %pip install -U tensorflow-gpu tensorflow-addons tensorflow-federated tensorboard # tf-nightly-gpu  # Currently not useful."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "laU7LRAguts6"
      },
      "source": [
        "### Select TF version (in colab)\n",
        "\n",
        "This should be the default starting point for working with the standard environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jtpy59Yq-Qfz",
        "outputId": "df58060f-357b-4db0-e8c4-6583206422d1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "## Import TensorFlow\n",
        "## from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "try:\n",
        "    ## %tensorflow_version only exists in Colab.\n",
        "    %tensorflow_version 2.x\n",
        "except Exception:\n",
        "    ## Non colab people need to make sure that tf 2 is installed.\n",
        "    pass\n",
        "import tensorflow as tf"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 2.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "## Preliminary\n",
        "\n",
        "**THIS IS WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation\n",
        "\n",
        "This code can use either CPU, GPU or TPU when running on Google Colab.\n",
        "\n",
        "Select the corresponding runtime (menu: Runtime / Change runtime type)\n",
        "\n",
        "Note: TPU support is 'kind of' working. The secret was moving the Embedding layer to CPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rum30R2JzeEl",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import sys\n",
        "import json\n",
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import tensorflow as tf\n",
        "\n",
        "try:\n",
        "    from urllib.request import urlopen  # Py3\n",
        "except:\n",
        "    print(\"This notebook requires Python 3.\")\n",
        "try:\n",
        "    import pathlib\n",
        "except:\n",
        "    print(\"At least python 3.5 is needed.\")\n",
        "    \n",
        "try: # Colab instance?\n",
        "    from google.colab import drive\n",
        "except: # Not? ignore.\n",
        "    pass\n",
        "\n",
        "from IPython.core.display import display, HTML"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sfZg31sMEAP1"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "llPw84PkEAP2",
        "outputId": "1bf2cecf-ee67-4c26-817e-e342e84a007f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(f\"Tensorflow api v2 active: {tf.__version__}\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active: 2.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "P08FdKKnEAP6"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJOrQMUxO5WE",
        "colab_type": "text"
      },
      "source": [
        "This notebook can either run on a local jupyter server, or on google cloud.\n",
        "If a GPU/TPU is available, it will be used for training.\n",
        "\n",
        "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
        "\n",
        "Similarily, the text corpora that are used for training, can be cached on drive or locally."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "txvC_JXHO5WF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Define where snapshots of training data are stored:\n",
        "colab_google_drive_snapshots=True\n",
        "\n",
        "# Define if training data (the texts downloaded from internet) are cached:\n",
        "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
        "local_jupyter_data_cache=True       # In local jupyter mode cache to local path\n",
        "\n",
        "is_colab_notebook = 'google.colab' in sys.modules"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "gjWwUUfuEAP7",
        "outputId": "c4c02331-2edd-45cc-e9e3-dd33322fe827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "use_eager = True\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tpu_is_init = False\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        if is_colab_notebook is True:\n",
        "            print(\"         Hint: In Colab Runtime / Set runtime type, set runtime type to GPU or TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n",
        "else:\n",
        "    use_eager = False  # Eager mode cannot be used with TPUs.\n",
        "    tf.compat.v1.disable_eager_execution()\n",
        "    print(\"DISABLING eager execution because TPUs do not support dynamic execution.\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
            "GPU -> [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
            "TPU -> []\n",
            "GPUs: ['/device:XLA_GPU:0', '/device:GPU:0']\n",
            "[('/device:CPU:0', ''), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:XLA_GPU:0', 'device: XLA_GPU device'), ('/device:GPU:0', 'device: 0, name: Tesla P100-PCIE-16GB, pci bus id: 0000:00:04.0, compute capability: 6.0')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I1nN1tbCO5WN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if is_colab_notebook:\n",
        "    if colab_google_drive_snapshots:\n",
        "        mountpoint='/content/drive'\n",
        "        root_path='/content/drive/My Drive'\n",
        "        if not os.path.exists(root_path):\n",
        "            drive.mount(mountpoint)\n",
        "        if not os.path.exists(root_path):\n",
        "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
        "            colab_google_drive_snapshots=False\n",
        "    else:\n",
        "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
        "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
        "else:\n",
        "    root_path='.'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library\n",
        "\n",
        "`TextLibrary` class: text library for training, encoding, batch generation,\n",
        "and formatted source display. It read some books from Project Gutenberg\n",
        "and supports creation of training batches. The output functions support\n",
        "highlighting to allow to compare generated texts with the actual sources\n",
        "to help to identify identical (memorized) parts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjkelBcNO5WV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "use_dark_mode=False  # Set to false for white background"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.cache_dir=text_data_cache_directory\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        self.total_size=0\n",
        "        index = 1\n",
        "        for descriptor, author, title in descriptors:\n",
        "            fd = {}\n",
        "            cache_name=self.get_cache_name(author, title)\n",
        "            if os.path.exists(cache_name):\n",
        "                is_cached=True\n",
        "            else:\n",
        "                is_cached=False\n",
        "            valid=False\n",
        "            if descriptor[:4] == 'http' and is_cached is False:\n",
        "                try:\n",
        "                    print(f\"Downloading {cache_name}\")\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
        "                    self.data += dat\n",
        "                    self.total_size += len(dat)\n",
        "                    fd[\"title\"] = title\n",
        "                    fd[\"author\"] = author\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    valid=True\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"title\"] = title\n",
        "                fd[\"author\"] = author\n",
        "                try:\n",
        "                    if is_cached is True:\n",
        "                        print(f\"Reading {cache_name} from cache\")\n",
        "                        f = open(cache_name)\n",
        "                    else:    \n",
        "                        f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    self.total_size += len(dat)\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                    valid=True\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
        "                try:\n",
        "                    print(f\"Caching {cache_name}\")\n",
        "                    f = open(cache_name, 'w')\n",
        "                    f.write(dat)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
        "                \n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def get_cache_name(self, author, title):\n",
        "        if self.cache_dir is None:\n",
        "            return None\n",
        "        cname=f\"{author} - {title}.txt\"\n",
        "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
        "        return cache_filepath\n",
        "        \n",
        "    def display_colored_html(self, textlist, dark_mode=False, pre='', post=''):\n",
        "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
        "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
        "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
        "        if dark_mode is False:\n",
        "            bgcolors=bgcolorsWht\n",
        "        else:\n",
        "            bgcolors=bgcolorsDrk\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f\"{f['author']}: {f['title']}\"\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out, dark_mode=dark_mode)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, dark_mode=dark_mode, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return np.array(smpX), np.array(smpy)\n",
        "    \n",
        "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
        "        X, y = self.get_random_sample_batch(batch_size, length)\n",
        "        # xoh = one_hot(X,len(self.i2c))\n",
        "        xoh = tf.keras.backend.one_hot(X, len(self.i2c))\n",
        "        return xoh, y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ucJsgqNgEAQC"
      },
      "source": [
        "### Data sources\n",
        "\n",
        "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
        "\n",
        "The name given will be use as directory name for both snapshots and model data caches.\n",
        "\n",
        "Each entry in the lib array contains of:\n",
        "\n",
        "1. a local filename or https(s) link,\n",
        "2. an Author's name\n",
        "3. a title"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tZDo-bIPEAQD",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Women-Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Wolf', 'Voyage out'),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma')\n",
        "    ]\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mfjcZdU9O5Wi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if is_colab_notebook:\n",
        "    if colab_google_drive_data_cache is True:\n",
        "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
        "    else:\n",
        "        data_cache_path=None\n",
        "else:\n",
        "    if local_jupyter_data_cache is True:\n",
        "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
        "    else:\n",
        "        data_cache_path=None\n",
        "\n",
        "if data_cache_path is not None:\n",
        "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
        "    if not os.path.exists(data_cache_path):\n",
        "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
        "    else:\n",
        "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
        "            json.dump(libdesc,f,indent=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUK3G2KhO5Wl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "1ff0f39c-bfb9-465e-c053-e8531dd6c683"
      },
      "source": [
        "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)\n",
        "print(f\"Total size of texts: {textlib.total_size}\")"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading /content/drive/My Drive/Colab Notebooks/Women-Writers/Data/Jane Austen - Pride and Prejudice.txt from cache\n",
            "Reading /content/drive/My Drive/Colab Notebooks/Women-Writers/Data/Emily Brontë - Wuthering Heights.txt from cache\n",
            "Reading /content/drive/My Drive/Colab Notebooks/Women-Writers/Data/Virginia Wolf - Voyage out.txt from cache\n",
            "Reading /content/drive/My Drive/Colab Notebooks/Women-Writers/Data/Jane Austen - Emma.txt from cache\n",
            "Total size of texts: 2536902\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "30hi0UPtEAQG"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jY3hUuhQYzdT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "6cf52129-29f4-4808-cc4f-b0a028f3a04b"
      },
      "source": [
        "SEQUENCE_LEN = 60\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=256\n",
        "    use_tpu_model_for_tpu=True\n",
        "    STATEFUL=False\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "    STATEFUL = True\n",
        "LSTM_UNITS = 1024\n",
        "# EMBEDDING_DIM = 64 # 120\n",
        "LSTM_LAYERS = 8\n",
        "NUM_BATCHES=256  # int(textlib.total_size/BATCH_SIZE/SEQUENCE_LEN)\n",
        "print(NUM_BATCHES)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EeB7jugCV4lI",
        "colab": {}
      },
      "source": [
        "dx=[]\n",
        "dy=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "    x,y=textlib.get_random_onehot_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
        "    dx.append(x)\n",
        "    dy.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3whYiFuwS8q4",
        "colab": {}
      },
      "source": [
        "data_xy=(dx,dy) # tf.keras.backend.constant(np.array([dx,dy]))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DCy7WmQyS9T-",
        "colab": {}
      },
      "source": [
        "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "boow8wR7sLwi",
        "outputId": "628c74df-7938-49c3-a56c-ddcaced5b963",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "shuffle_buffer=10000\n",
        "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
        "dataset.take(1)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "BLtDLPjGEAQi",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
        "    model = tf.keras.Sequential([\n",
        "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "        #                          batch_input_shape=[batch_size, None]),\n",
        "        # tf.keras.layers.Flatten(),\n",
        "        tf.keras.layers.LSTM(lstm_units,\n",
        "                            # input_shape=(timesteps, data_dim)\n",
        "                            batch_input_shape=[batch_size, None, vocab_size],\n",
        "                            return_sequences=True,\n",
        "                            stateful=stateful,\n",
        "                            recurrent_initializer='glorot_uniform'),\n",
        "        # *[tf.keras.layers.LSTM(lstm_units,\n",
        "        #                     return_sequences=True,\n",
        "        #                     stateful=stateful,\n",
        "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
        "        tf.keras.layers.Dense(vocab_size)\n",
        "        ])\n",
        "    return model\n",
        "\n",
        "def build_tpu_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size, stateful=True):\n",
        "    print(\"NOT ADAPTED!\")\n",
        "    with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
        "        embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, SEQUENCE_LEN])\n",
        "    with tpu_strategy.scope():\n",
        "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=stateful,\n",
        "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
        "#     tf.keras.layers.LSTM(lstm_units,\n",
        "#                          return_sequences=True,\n",
        "#                          stateful=stateful,\n",
        "#                          # recurrent_initializer='glorot_uniform',\n",
        "#                         unroll=True)\n",
        "    dense = tf.keras.layers.Dense(vocab_size)\n",
        "    \n",
        "    model = tf.keras.Sequential([\n",
        "        embedded,\n",
        "        *lstm,\n",
        "        dense\n",
        "        ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oQIxWHMGNKHk",
        "colab": {}
      },
      "source": [
        "if use_tpu:\n",
        "    print(TPU_ADDRESS)\n",
        "    os.environ['COLAB_TPU_ADDR']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "NMsaykJjEAQl",
        "colab": {}
      },
      "source": [
        "if use_tpu is True and not tpu_is_init:\n",
        "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
        "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
        "    tpu_is_init=True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nf-NHZ326NqJ",
        "outputId": "132d1006-34c1-4e60-afc9-ebc1261c58f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if use_tpu is True:\n",
        "    if use_tpu_model_for_tpu is True:\n",
        "        print(\"tpu, simple model\")\n",
        "        # with tpu_strategy.scope():\n",
        "        model = build_tpu_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          embedding_dim=EMBEDDING_DIM,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          lstm_layers=LSTM_LAYERS,\n",
        "          batch_size=BATCH_SIZE,\n",
        "          stateful=STATEFUL)\n",
        "    else:\n",
        "        print(\"tpu, default model\")\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_model(\n",
        "              vocab_size = len(textlib.i2c),\n",
        "              embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE,\n",
        "              stateful=STATEFUL)        \n",
        "else:\n",
        "    print(\"non-tpu mode\")\n",
        "    model = build_model(\n",
        "        vocab_size = len(textlib.i2c),\n",
        "        # embedding_dim=EMBEDDING_DIM,\n",
        "        steps=SEQUENCE_LEN,\n",
        "        lstm_units=LSTM_UNITS,\n",
        "        lstm_layers=LSTM_LAYERS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        stateful=STATEFUL)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "non-tpu mode\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GQ7inpxLveap"
      },
      "source": [
        "### Some sanity checks of the (untrained!) model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Fb_Q2TYO5XI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d2bc0943-4286-4e05-86e2-75c4807f8983"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GhhF2vqmEAQo",
        "outputId": "492f8786-9142-41c3-830d-2b4edde37435",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "if use_eager is True:  # no sanity for TPU, since eager not supported:\n",
        "    for input_example_batch, target_example_batch in dataset.take(1):\n",
        "        model.reset_states()\n",
        "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
        "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 60, 88) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8vxZF0wOEAQr",
        "outputId": "8827599a-eae2-40a0-adef-cfd46f043dc9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (256, None, 1024)         4558848   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 88)           90200     \n",
            "=================================================================\n",
            "Total params: 4,649,048\n",
            "Trainable params: 4,649,048\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LYZs8Ss2947k",
        "outputId": "533aae03-e7cd-4bf2-d2a4-663097c8e9d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fKo54K1lEAQt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "3e47585a-cb58-453a-f37f-2cb89731a0ba"
      },
      "source": [
        "if use_eager is True:\n",
        "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "    print(sampled_indices)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[74 26 35 35 62 24 22 39 12 72 23 28 83 22 34 78 81 46 80 36 18 45 86 44\n",
            " 13 45 42 68  4 13 35 52 13 17 14 24  5 13 60 24 33 33 24 11 71  8 47 20\n",
            " 55 56 47 83 57 84 11 37 70 23 20 31]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ic4RuDZLEAQy",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "445b93c2-9681-4f8a-d3fc-d1dd47c6fd5a"
      },
      "source": [
        "if use_eager is True:\n",
        "    print(textlib.decode(sampled_indices))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ";\n",
            "--(l,RnqpK@,Yz8[QLf0}2b01WPb-CbkglrbNl..lu&c#dOF#@H$u:6pdm\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "soB-Q8YXvndE"
      },
      "source": [
        "### Loss function, optimizer, tensorboard output"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "6i-0Y2uYEAQ0",
        "outputId": "58053df6-96e6-48cc-e4c9-6269208a02f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "if use_eager is True:\n",
        "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 60, 88)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4801426\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "py9WnmosEAQ3",
        "colab": {}
      },
      "source": [
        "opti = tf.keras.optimizers.Adam(lr=0.003, clipvalue=1.0)\n",
        "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
        "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
        "\n",
        "def scalar_loss(labels, logits):\n",
        "    bl=loss(labels, logits)\n",
        "    return tf.reduce_mean(bl)\n",
        "\n",
        "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5SKvObcsEAQ5",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)\n",
        "\n",
        "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='batch', histogram_freq=1) # update_freq='epoch', "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o0Ew6pgWzeFj",
        "outputId": "1e487dc5-6318-4d8d-f317-c313e7d1b651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 854
        }
      },
      "source": [
        "!kill 2195\n",
        "%tensorboard --logdir logs"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/bin/bash: line 0: kill: (2195) - No such process\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Reusing TensorBoard on port 6006 (pid 2195), started 0:06:56 ago. (Use '!kill 2195' to kill it.)"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "  (async () => {\n",
              "      const url = await google.colab.kernel.proxyPort(6006, {\"cache\": true});\n",
              "      const iframe = document.createElement('iframe');\n",
              "      iframe.src = url;\n",
              "      iframe.setAttribute('width', '100%');\n",
              "      iframe.setAttribute('height', '800');\n",
              "      iframe.setAttribute('frameborder', 0);\n",
              "      document.body.appendChild(iframe);\n",
              "  })();\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kDFbZcN0vxOB"
      },
      "source": [
        "## The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kh2yUKBoEAQ8",
        "colab": {}
      },
      "source": [
        "EPOCHS=20"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RLbsTmtnEAQ-",
        "outputId": "5143f3ab-1dcb-4949-bcd5-25e3e52bbc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback, tensorboard_callback])"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train for 256 steps\n",
            "Epoch 1/20\n",
            "256/256 [==============================] - 23s 88ms/step - loss: 1.8437 - scalar_loss: 1.8437\n",
            "Epoch 2/20\n",
            "256/256 [==============================] - 22s 87ms/step - loss: 1.7008 - scalar_loss: 1.7008\n",
            "Epoch 3/20\n",
            " 44/256 [====>.........................] - ETA: 24s - loss: 1.6348 - scalar_loss: 1.6348"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-45-f1ecf82861c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensorboard_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 819\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    820\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    821\u001b[0m   def evaluate(self,\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/util/nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m   return pack_sequence_as(\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mstructure\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m       expand_composites=expand_composites)\n\u001b[1;32m    570\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36m_non_none_constant_value\u001b[0;34m(v)\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_non_none_constant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 130\u001b[0;31m   \u001b[0mconstant_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtensor_util\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstant_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mconstant_value\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/tensor_util.py\u001b[0m in \u001b[0;36mconstant_value\u001b[0;34m(tensor, partial)\u001b[0m\n\u001b[1;32m    820\u001b[0m   \"\"\"\n\u001b[1;32m    821\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 822\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    823\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    824\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36mnumpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    940\u001b[0m     \"\"\"\n\u001b[1;32m    941\u001b[0m     \u001b[0;31m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m     \u001b[0mmaybe_arr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaybe_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmaybe_arr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tensorflow-2.1.0/python3.6/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    906\u001b[0m     \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    907\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 908\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_numpy_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    909\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    910\u001b[0m       \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rvupep98EARA",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "mnMrNH0eEARC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d48813aa-c69d-4cae-a809-407c08e851f5"
      },
      "source": [
        "\n",
        "checkpoint_dir = './training_checkpoints'  # duplicate\n",
        "\n",
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-pvmuA-8aTx3",
        "colab": {}
      },
      "source": [
        "use_tpu_for_generation=False"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "qA7qRl5EEARE",
        "colab": {}
      },
      "source": [
        "if not use_tpu_for_generation:\n",
        "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        steps=SEQUENCE_LEN,\n",
        "        lstm_units=LSTM_UNITS,\n",
        "        lstm_layers=LSTM_LAYERS,\n",
        "        batch_size=1)\n",
        "else:\n",
        "    gen_model = build_tpu_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          embedding_dim=EMBEDDING_DIM,\n",
        "          steps=SEQUENCE_LEN,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          lstm_layers=LSTM_LAYERS,\n",
        "          batch_size=1,\n",
        "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yFRyOB8we2YA",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "379c9ac3-e5e9-48c8-aa07-c0cecfbb8f64"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_20'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "KCunzXAQKBI6",
        "outputId": "85bf1238-87b5-4ea6-ced5-669eefaa0874",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fbeca47cf60>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YJbQqPFEevop",
        "colab": {}
      },
      "source": [
        "gen_model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nQ6JhIGxEARF",
        "outputId": "a32fd59a-c386-4a9c-e7b8-640f9e9d064c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "gen_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm_1 (LSTM)                (1, None, 256)            353280    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 88)             22616     \n",
            "=================================================================\n",
            "Total params: 375,896\n",
            "Trainable params: 375,896\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "geLmNIvzEARH",
        "colab": {}
      },
      "source": [
        "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 128\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
        "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
        "      if not use_tpu:\n",
        "          predicted_id=predicted_tensor.numpy()\n",
        "      else:\n",
        "          predicted_id=predicted_tensor.eval()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "      print(\"out:\"+''.join(text_generated))\n",
        "\n",
        "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
        "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
        "      input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)\n",
        "\n",
        "def generate_text(model, start_string, temp=0.6):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 128\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  cutstr=start_string # [0:SEQUENCE_LEN] # \n",
        "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
        "  input_eval_1 = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = temp\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model.predict(input_eval, batch_size=1)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
        "      predicted_id=predicted_tensor.numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval_1 = tf.expand_dims([predicted_id], 0)\n",
        "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
        "  return (''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x2Fl4hJzO5YM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b6fb41a-c4d7-427a-9baf-dee07ffc9dbc"
      },
      "source": [
        "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
        "len(start_string[0:SEQUENCE_LEN])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "H7750ibzEARJ",
        "outputId": "7247735f-b51c-47e6-aee9-e087af7de1d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "if use_tpu_for_generation:\n",
        "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
        "    with sess.as_default():\n",
        "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
        "else:\n",
        "    tf.compat.v1.enable_eager_execution()\n",
        "    if not tf.executing_eagerly():\n",
        "        print(\"Eager engine stall.\")\n",
        "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
        "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
        "    print(tx)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " of her fult.  And she share some how very surpless\n",
            "to dise't ald a mesticulabel and goon\n",
            "or to you was quite hel\n",
            "stead, and the\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "b-mbC9l8EARL",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Tql-eLGvEARO",
        "colab": {},
        "outputId": "8f0f3188-8635-4ad5-c1ae-0929ef42533d"
      },
      "source": [
        "txt=textlib.decode(id)\n",
        "txti=txt.split('\\r\\n')\n",
        "for t in txti:\n",
        "    print(t)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " of her he batk on a make her beand aspansion_ vigyed out to her forded here poor spald mode that clmations stirf.  But mo stlee\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZBil008pEARS",
        "colab": {},
        "outputId": "687b78db-883f-437c-992a-e9bd5b9adcde"
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span style=\"background-color:#ebdef0;\"> of her he</span><sup>[2]</sup> batk on a<span style=\"background-color:#d8daef;\"> make her </span><sup>[1]</sup>beand aspansion_ vigy<span style=\"background-color:#e2d7d5;\">ed out to her </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">forded her</span><sup>[4]</sup>e poor spald mode that clmations stirf.  But mo stlee"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "lXYmlO_IEARU",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yWE_ZZMKEARV"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model [not ported yet]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_poet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5oK6Ej8OQtf1",
        "48X8-lFFQtf3",
        "zOlVTPtFQtf7",
        "sScszXSqQtf_",
        "WW6LPdlhQtgF"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiOXyvbEAPo",
        "colab_type": "code",
        "outputId": "40dd80ee-6a6e-402d-b7ac-4221a7ef3c0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 739
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview # tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/3b/21564412886837f45b57067eefa7f9b69e61e032fc188b57496c5391b76b/tf_nightly_gpu_2.0_preview-2.0.0.dev20190517-cp36-cp36m-manylinux1_x86_64.whl (348.7MB)\n",
            "\u001b[K     |████████████████████████████████| 348.7MB 48kB/s \n",
            "\u001b[?25hRequirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/12/75/5107f9c9c2106e3765a6b22d611005aac4e29c29a5142ee897847776dc17/tensorflow_estimator_2.0_preview-1.14.0.dev2019051700-py2.py3-none-any.whl (427kB)\n",
            "\u001b[K     |████████████████████████████████| 430kB 49.0MB/s \n",
            "\u001b[?25hCollecting google-pasta>=0.1.6 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f9/68/a14620bfb042691f532dcde8576ff82ee82e4c003cdc0a3dbee5f289cee6/google_pasta-0.1.6-py3-none-any.whl (51kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 31.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Collecting tb-nightly<1.15.0a0,>=1.14.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3f/50/35c33d833f79f8b7b8c21a8e5414487adea342b8dfa21f2e20190cb2c578/tb_nightly-1.14.0a20190517-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 45.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Collecting wrapt>=1.11.1 (from tf-nightly-gpu-2.0-preview)\n",
            "  Downloading https://files.pythonhosted.org/packages/67/b2/0f71ca90b0ade7fad27e3d20327c996c6252a2ffe88f50a95bba7434eda9/wrapt-1.11.1.tar.gz\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Building wheels for collected packages: wrapt\n",
            "  Building wheel for wrapt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/89/67/41/63cbf0f6ac0a6156588b9587be4db5565f8c6d8ccef98202fc\n",
            "Successfully built wrapt\n",
            "\u001b[31mERROR: thinc 6.12.1 has requirement wrapt<1.11.0,>=1.10.0, but you'll have wrapt 1.11.1 which is incompatible.\u001b[0m\n",
            "Installing collected packages: tensorflow-estimator-2.0-preview, google-pasta, tb-nightly, wrapt, tf-nightly-gpu-2.0-preview\n",
            "  Found existing installation: wrapt 1.10.11\n",
            "    Uninstalling wrapt-1.10.11:\n",
            "      Successfully uninstalled wrapt-1.10.11\n",
            "Successfully installed google-pasta-0.1.6 tb-nightly-1.14.0a20190517 tensorflow-estimator-2.0-preview-1.14.0.dev2019051700 tf-nightly-gpu-2.0-preview-2.0.0.dev20190517 wrapt-1.11.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s78tdpcEAPt",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "# [WIP] Eager Tensor Poet (tf 2.0)\n",
        "\n",
        "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "from urllib.request import urlopen  # Py3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e4yYLii8Qtfw"
      },
      "source": [
        "### Content\n",
        "This notebook contains the following sections:\n",
        "1. TextLibrary: utilities to work with text files\n",
        "  * loading of a list of files (local or URLs)\n",
        "  * encoding for training\n",
        "  * formatted output with quote-highlighting\n",
        "2. Transform text data to tf.data\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "x. Definition of the tensorflow model\n",
        "x. Model and training parameters\n",
        "x. The actual training on the data (required 1. - 3.)\n",
        "  * Training can be restarted, since the model is saved periodically.\n",
        "x. Generation of text from the trained model (requires 1. - 4.)\n",
        "x. In dialog with with the model (requires 1. - 4.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1",
        "colab_type": "text"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n",
        "\n",
        "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.13.x at the time of this writing. the version check below is preliminary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab_type": "code",
        "outputId": "7e4511af-4937-4623-a16a-0c80d556347a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(\"Tensorflow api v2 active.\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P08FdKKnEAP6",
        "colab_type": "text"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWwUUfuEAP7",
        "colab_type": "code",
        "outputId": "f66ba56a-4bd0-4c36-8d76-fe3799adaef8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        print(\"         Hint: If using Google Colab, set runtime type to TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')]\n",
            "GPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU')]\n",
            "TPU -> []\n",
            "GPUs: ['/device:XLA_GPU:0', '/device:GPU:0']\n",
            "[('/device:CPU:0', ''), ('/device:XLA_GPU:0', 'device: XLA_GPU device'), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:GPU:0', 'device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "# TextLibrary class: text library for training, encoding, batch generation,\n",
        "# and formatted source display\n",
        "\n",
        "\n",
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJsgqNgEAQC",
        "colab_type": "text"
      },
      "source": [
        "### Read text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDo-bIPEAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68E8t2qEAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = textlib.get_encoded_slice_array(len(textlib.data))\n",
        "textlib_dataset = tf.data.Dataset.from_tensor_slices(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHNgHiH1EAQK",
        "colab_type": "code",
        "outputId": "65f2c6a9-af5e-4e38-93f8-a298bf24a3aa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Quick test\n",
        "n=np.array([])\n",
        "for i in textlib_dataset.take(90):\n",
        "    n=np.append(n,i.numpy())\n",
        "print(n)    \n",
        "print(textlib.decode(n))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  2.  8.  9.  3. 10. 11.  9.  2. 12. 13.\n",
            "  2.  5. 14.  3. 15. 16.  6.  6. 17.  3.  6. 18.  3.  4.  5. 19. 20.  2.\n",
            "  3. 21. 12. 20.  3.  4.  5.  2.  7. 11. 20. 19.  8.  2. 22.  3. 21.  3.\n",
            " 23. 24. 21. 25. 22.  3. 13. 25. 26. 27. 28. 21.  5. 25.  3. 29.  2. 19.\n",
            "  9.  1.  3. 28.  2. 20. 13.  2.  5. 25.  3. 28. 21.  8. 17. 21. 25.  2.]\n",
            "The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medbery Mackaye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mnBkXfEAQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 80\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=1024\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "LSTM_UNITS = 1024\n",
        "EMBEDDING_DIM = 512"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o0v62LKEAQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_size=len(data)//SEQUENCE_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgScEoozEAQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=textlib_dataset.batch(SEQUENCE_LEN+1,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg_gWvzqEAQT",
        "colab_type": "code",
        "outputId": "f41a369e-4dd5-4a8b-a554-eae8e010977a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "# Quick test\n",
        "for arr in sequences.take(3):\n",
        "    n=arr.numpy()\n",
        "    print(arr)\n",
        "    print(\">\"+textlib.decode(n))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0  1  2  3  4  5  6  7  2  8  9  3 10 11  9  2 12 13  2  5 14  3 15 16\n",
            "  6  6 17  3  6 18  3  4  5 19 20  2  3 21 12 20  3  4  5  2  7 11 20 19\n",
            "  8  2 22  3 21  3 23 24 21 25 22  3 13 25 26 27 28 21  5 25  3 29  2 19\n",
            "  9  1  3 28  2 20 13  2  5], shape=(81,), dtype=int64)\n",
            ">The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medber\n",
            "tf.Tensor(\n",
            "[25  3 28 21  8 17 21 25  2 26 27 26 27  0  1 19 30  3  2 16  6  6 17  3\n",
            " 19 30  3 18  6  5  3  9  1  2  3 11 30  2  3  6 18  3 21 12 25  6 12  2\n",
            "  3 21 12 25 31  1  2  5  2  3 21  9  3 12  6  3  8  6 30  9  3 21 12 20\n",
            "  3 31 19  9  1 26 27 21 24], shape=(81,), dtype=int64)\n",
            ">y Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "al\n",
            "tf.Tensor(\n",
            "[32  6 30  9  3 12  6  3  5  2 30  9  5 19  8  9 19  6 12 30  3 31  1 21\n",
            "  9 30  6  2 33  2  5 34  3  3 35  6 11  3 32 21 25  3  8  6 23 25  3 19\n",
            "  9 22  3 14 19 33  2  3 19  9  3 21 31 21 25  3  6  5 26 27  5  2 36 11\n",
            " 30  2  3 19  9  3 11 12 20], shape=(81,), dtype=int64)\n",
            ">most no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it und\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWvfNurDEAQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMYWYbjEAQa",
        "colab_type": "code",
        "outputId": "ec47adb3-e68d-4dcb-e1c6-35cc7c265742",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Quick test\n",
        "for input_text, output_text in dataset.take(2):\n",
        "    print(\"I:\"+textlib.decode(input_text.numpy()))\n",
        "    print(\"O:\"+textlib.decode(output_text.numpy()))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medbe\n",
            "O:he Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medber\n",
            "I:y Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "a\n",
            "O: Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "al\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7tYcLdEAQd",
        "colab_type": "code",
        "outputId": "5e1c3db6-b052-427f-b271-50105e6873b4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 100000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 80), (256, 80)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtDLPjGEAQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform'),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIxWHMGNKHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1772869f-b800-4b86-dbd1-8c7f09ac4662"
      },
      "source": [
        "dev_strings=[]\n",
        "for log_dev in tf.config.experimental.list_logical_devices('TPU'):\n",
        "    dev_strings.append(log_dev.name)\n",
        "print(dev_strings)\n",
        "\n",
        "# for i in range(8):\n",
        "#     dev_strings.append('/TPU:{}'.format(i))\n",
        "# print(dev_strings)\n",
        "    "
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaykJjEAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_tpu is True:\n",
        "    # tpus=tf.config.experimental.list_logical_devices('TPU')\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)    \n",
        "    # mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(dev_strings)\n",
        "    # with mirrored_strategy.scope():\n",
        "    with tpu_strategy.scope():\n",
        "        model = build_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          embedding_dim=EMBEDDING_DIM,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          batch_size=BATCH_SIZE)\n",
        "else:\n",
        "    model = build_model(\n",
        "      vocab_size = len(textlib.i2c),\n",
        "      embedding_dim=EMBEDDING_DIM,\n",
        "      lstm_units=LSTM_UNITS,\n",
        "      batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhF2vqmEAQo",
        "colab_type": "code",
        "outputId": "587b1a61-eac5-46cf-f84e-479384949c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 80, 89) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab_type": "code",
        "outputId": "8c927c4b-a5ba-4d94-c536-2c4fb8f759d2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (256, None, 512)          45568     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (256, None, 1024)         6295552   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (256, None, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (256, None, 1024)         8392704   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 89)           91225     \n",
            "=================================================================\n",
            "Total params: 23,217,753\n",
            "Trainable params: 23,217,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo54K1lEAQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOQdlNQEAQv",
        "colab_type": "code",
        "outputId": "5afd3255-7693-4b42-a1a2-889ea3d4026d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  3, 21,  2, 21, 58, 30, 34, 74, 54, 83, 41,  1, 74, 43, 46, 82,\n",
              "       61, 13, 30, 69, 75, 64, 35, 43, 48,  6, 48, 72, 32, 68, 14,  2, 48,\n",
              "       27, 75, 15, 49, 47, 62, 16, 32, 71, 42, 41, 48, 41, 17, 79, 20, 23,\n",
              "       81,  0, 38, 57, 62, 53, 11, 69, 43, 11,  0, 60, 59, 18, 14, 27, 18,\n",
              "       85, 79, 75, 56,  0, 39, 10, 87, 34, 71, 14, 87])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4RuDZLEAQy",
        "colab_type": "code",
        "outputId": "f3bf150c-61f6-40c7-8168-7086a6453f5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "textlib.decode(sampled_indices)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'  aeaHs.xI%Dhx108NbsW;)Y1#o#&mVge#\\n;E3[/Bm6SD#DkzdpQT:F/CuW1uTUJfg\\nf$z;OTAG}.6g}'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-0Y2uYEAQ0",
        "colab_type": "code",
        "outputId": "12fce92b-61db-4cb5-a52d-af4cc1d09ca6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 80, 89)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.488147\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py9WnmosEAQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKvObcsEAQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "colab_type": "code",
        "outputId": "e65dc378-9428-4c98-f867-54eda08ce50b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "124/124 [==============================] - 203s 2s/step - loss: 3.0736\n",
            "Epoch 2/15\n",
            "124/124 [==============================] - 199s 2s/step - loss: 2.1193\n",
            "Epoch 3/15\n",
            "124/124 [==============================] - 198s 2s/step - loss: 1.6384\n",
            "Epoch 4/15\n",
            "124/124 [==============================] - 199s 2s/step - loss: 1.4175\n",
            "Epoch 5/15\n",
            "124/124 [==============================] - 199s 2s/step - loss: 1.3167\n",
            "Epoch 6/15\n",
            "124/124 [==============================] - 199s 2s/step - loss: 1.2547\n",
            "Epoch 7/15\n",
            "103/124 [=======================>......] - ETA: 33s - loss: 1.2114"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvupep98EARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMrNH0eEARC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "cc4b6167-2c6e-48fe-8fe3-70b892ccf24b"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_3'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7qRl5EEARE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = len(textlib.i2c),\n",
        "  embedding_dim=EMBEDDING_DIM,\n",
        "  lstm_units=LSTM_UNITS,\n",
        "  batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6JhIGxEARF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 341
        },
        "outputId": "1f23ae14-0b54-404e-fea3-e48bc387d3cb"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 512)            45568     \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (1, None, 1024)           6295552   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (1, None, 1024)           8392704   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 89)             91225     \n",
            "=================================================================\n",
            "Total params: 23,217,753\n",
            "Trainable params: 23,217,753\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLmNIvzEARH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [textlib.c2i[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = .40\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7750ibzEARJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx,id=generate_text(model, start_string=\"Good\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mbC9l8EARL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tql-eLGvEARO",
        "colab_type": "code",
        "outputId": "13010e75-bc12-48db-f7c4-6a9efbc7dcb8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "textlib.decode(id)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "' the every one conceared in the except of the langer of into the same of the house, and he\\r\\ncame better the present of the sonsider of seements of the project Gutenberg-tm setters of her own contristed to the spent of the sense of the sense of the present of the serving of the country and the\\r\\nthought it was not have a demined at the world and the\\r\\nfrom himself to her hear against the darker was answered, and he was not continued by the sease of her pleasure seemed to see a dright to the door, and then he had not been the man the proses of the greature of the person of the suppose of the\\r\\nlittle better to be into the stood of his prosessesses on the same of my else in the sight of her with the sense of the sance of the sense of the same interested her proceeded and considered she are seemed to have been all and speeking in his\\r\\nfinder of properite of the eight of the\\r\\nseate of the strange of the house, and into her been to her\\r\\nand infortune to the was on the service of the world from '"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBil008pEARS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 283
        },
        "outputId": "2debda5a-ed43-465a-ea2c-2646b8cb49ae"
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "Good th<span style=\"background-color:#eadbd8;\">e every one</span><sup>[3]</sup> conc<span style=\"background-color:#ebdef0;\">eared in the </span><sup>[2]</sup>exce<span style=\"background-color:#d8daef;\">pt of the </span><sup>[1]</sup>l<span style=\"background-color:#e2d7d5;\">anger of in</span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">to the same </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">of the house, and </span><sup>[4]</sup>he\r<br>ca<span style=\"background-color:#d8daef;\">me better th</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">e present o</span><sup>[2]</sup>f the sonsider of se<span style=\"background-color:#eadbd8;\">ements of the </span><sup>[3]</sup>p<span style=\"background-color:#d8daef;\">roject Gutenberg-tm </span><sup>[1]</sup>s<span style=\"background-color:#ebdef0;\">etters of </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">her own con</span><sup>[2]</sup>tris<span style=\"background-color:#ebdef0;\">ted to the s</span><sup>[2]</sup>p<span style=\"background-color:#eadbd8;\">ent of the se</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">nse of the s</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">ense of the </span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">present of </span><sup>[4]</sup>the <span style=\"background-color:#eadbd8;\">serving of </span><sup>[3]</sup><span style=\"background-color:#d8daef;\">the country a</span><sup>[1]</sup>nd<span style=\"background-color:#e2d7d5;\"> the\r<br>thought </span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">it was not h</span><sup>[2]</sup>ave a demi<span style=\"background-color:#d8daef;\">ned at the </span><sup>[1]</sup><span style=\"background-color:#eadbd8;\">world and the</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">\r<br>from him</span><sup>[4]</sup><span style=\"background-color:#d8daef;\">self to her</span><sup>[1]</sup> hea<span style=\"background-color:#eadbd8;\">r against the </span><sup>[3]</sup>darker was<span style=\"background-color:#ebdef0;\"> answered, and </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">he was not c</span><sup>[3]</sup>ontinu<span style=\"background-color:#d8daef;\">ed by the s</span><sup>[1]</sup>ea<span style=\"background-color:#ebdef0;\">se of her </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">pleasure s</span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">eemed to see a</span><sup>[3]</sup> d<span style=\"background-color:#eadbd8;\">right to the</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\"> door, and then </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">he had not been </span><sup>[3]</sup>t<span style=\"background-color:#eadbd8;\">he man the</span><sup>[3]</sup> pr<span style=\"background-color:#d8daef;\">oses of the g</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">reature of the </span><sup>[2]</sup><span style=\"background-color:#d8daef;\">person of </span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">the suppos</span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">e of the\r<br>little </span><sup>[4]</sup><span style=\"background-color:#d8daef;\">better to </span><sup>[1]</sup>b<span style=\"background-color:#ebdef0;\">e into the s</span><sup>[2]</sup>to<span style=\"background-color:#e2d7d5;\">od of his </span><sup>[4]</sup>prosessess<span style=\"background-color:#ebdef0;\">es on the s</span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">ame of my </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">else in the </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">sight of her </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">with the sens</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">e of the sa</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">nce of the se</span><sup>[4]</sup><span style=\"background-color:#eadbd8;\">nse of the s</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">ame interest</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">ed her pro</span><sup>[2]</sup>cee<span style=\"background-color:#eadbd8;\">ded and co</span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">nsidered s</span><sup>[3]</sup>he a<span style=\"background-color:#eadbd8;\">re seemed to h</span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">ave been all </span><sup>[2]</sup>and spee<span style=\"background-color:#ebdef0;\">king in his</span><sup>[2]</sup>\r<br>finder<span style=\"background-color:#ebdef0;\"> of proper</span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">ite of the </span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">eight of the</span><sup>[3]</sup>\r<br>se<span style=\"background-color:#eadbd8;\">ate of the s</span><sup>[3]</sup>t<span style=\"background-color:#ebdef0;\">range of the </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">house, and </span><sup>[2]</sup><span style=\"background-color:#ebdef0;\">into her be</span><sup>[2]</sup>e<span style=\"background-color:#ebdef0;\">n to her\r<br></span><sup>[2]</sup>and in<span style=\"background-color:#ebdef0;\">fortune to </span><sup>[2]</sup>t<span style=\"background-color:#eadbd8;\">he was on the </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">service of </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">the world f</span><sup>[3]</sup>rom "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt</span><sup>[3]</sup>, <span style=\"background-color:#ebdef0;\">http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt</span><sup>[1]</sup>, <span style=\"background-color:#e2d7d5;\">http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt</span><sup>[4]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYmlO_IEARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV",
        "colab_type": "text"
      },
      "source": [
        "**below this point not yet ported**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oK6Ej8OQtf1"
      },
      "source": [
        "## 2. Definition of the Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GP4sk47FQtf1",
        "colab": {}
      },
      "source": [
        "# The tensorflow model for text generation\n",
        "class TensorPoetModel:\n",
        "    def __init__(self, params):\n",
        "        self.model_name = params[\"model_name\"]\n",
        "        self.vocab_size = params[\"vocab_size\"]\n",
        "        self.neurons = params[\"neurons\"]\n",
        "        self.layers = params[\"layers\"]\n",
        "        self.learning_rate = params[\"learning_rate\"]\n",
        "        self.steps = params[\"steps\"]\n",
        "        self.logdir = params[\"logdir\"]\n",
        "        self.checkpoint = params[\"checkpoint\"]\n",
        "        # self.clip = -1.0 * params[\"clip\"]\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        # Training & Generating:\n",
        "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "\n",
        "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
        "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
        "\n",
        "        stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(\n",
        "            self.neurons, name='basic_lstm_cell') for _ in range(self.layers)])\n",
        "\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "\n",
        "        self.init_state_0 = stacked_cell.zero_state(batch_size, tf.float32)\n",
        "        self.init_state = self.init_state_0\n",
        "\n",
        "        with tf.variable_scope('rnn') as scope:\n",
        "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X,\n",
        "                                                    initial_state=self.init_state,\n",
        "                                                    dtype=tf.float32)\n",
        "            self.init_state = states\n",
        "\n",
        "        self.final_state = self.init_state\n",
        "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
        "\n",
        "        softmax_w = tf.Variable(tf.random_normal(\n",
        "            [self.neurons, self.vocab_size]), dtype=tf.float32, name='sm_w')\n",
        "        softmax_b = tf.Variable(\n",
        "            [self.vocab_size], dtype=tf.float32, name='sm_b')\n",
        "\n",
        "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
        "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
        "\n",
        "        output_softmax = tf.nn.softmax(logits)\n",
        "\n",
        "        self.temperature = tf.placeholder(tf.float32)\n",
        "        self.output_softmax_temp = tf.nn.softmax(\n",
        "            tf.div(logits, self.temperature))\n",
        "\n",
        "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=onehot_y, logits=logits)\n",
        "\n",
        "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "\n",
        "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
        "\n",
        "        # Clipping isn't necessary, even for really deep networks:\n",
        "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
        "        # minclip = -1.0 * self.clip\n",
        "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) \n",
        "        #     for grad, var in grads]\n",
        "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
        "\n",
        "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
        "        correct_prediction = tf.equal(self.y, self.prediction)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        error = 1.0 - self.accuracy\n",
        "\n",
        "        # Tensorboard\n",
        "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
        "        tf.summary.scalar(\"error\", error)\n",
        "        self.summary_merged = tf.summary.merge_all()\n",
        "\n",
        "        # Init\n",
        "        self.init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "48X8-lFFQtf3"
      },
      "source": [
        "## 3. Parameters for model and training\n",
        "\n",
        "The library description `libdesc` contains a list in `lib` with local filenames of text-files or http, https URLs pointing to text files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhMGIIPEQtf3",
        "colab": {}
      },
      "source": [
        "\n",
        "# Model parameter:\n",
        "modelParamsShakespeare = {\n",
        "    \"model_name\": \"shakespeare\",\n",
        "    \"logdir\": \"tensorlog/shakespeare\",\n",
        "    \"checkpoint\": \"shakespeare.ckpt\",\n",
        "    \"vocab_size\": len(textlib.i2c),\n",
        "    \"neurons\": 512,\n",
        "    \"layers\": 4,\n",
        "    \"learning_rate\": 4.e-4,\n",
        "    \"steps\": 128,\n",
        "}\n",
        "\n",
        "# Look for optional json description of a library:\n",
        "if os.path.exists('bk/lib-phil-deen.json'):\n",
        "    with open('bk/lib-phil-deen.json') as data_file:    \n",
        "        libdescphil = json.load(data_file)\n",
        "        textlib = TextLibrary(libdescphil[\"lib\"])\n",
        "        modelParamsPhil = {\n",
        "            \"model_name\": \"phil\",\n",
        "            \"logdir\": \"tensorlog/phil\",\n",
        "            \"checkpoint\": \"phil.ckpt\",\n",
        "            \"vocab_size\": len(textlib.i2c),\n",
        "            \"neurons\": 256,\n",
        "            \"layers\": 8,\n",
        "            \"learning_rate\": 1.e-3,\n",
        "            \"steps\": 128,\n",
        "        }\n",
        "        model = TensorPoetModel(modelParamsPhil)\n",
        "else:\n",
        "    model = TensorPoetModel(modelParamsShakespeare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ciDOIUiQtf6",
        "colab": {}
      },
      "source": [
        "# Training Parameter:\n",
        "\n",
        "trainParams = {\n",
        "    \"max_iter\": 1000000,\n",
        "    \"restoreCheckpoints\": False,\n",
        "    \"generateDuringTraining\": True,\n",
        "    \"generated_text_size\": 200,\n",
        "    \"verbose\": True,\n",
        "    \"statusEveryNIter\": 500,\n",
        "    \"saveEveryNIter\": 500,\n",
        "    \"batch_size\": 128,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zOlVTPtFQtf7"
      },
      "source": [
        "## 4. The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3x0U5-0dQtf9",
        "colab": {}
      },
      "source": [
        "# Run training:\n",
        "with tf.Session() as sess:\n",
        "    batch_size = trainParams[\"batch_size\"]\n",
        "    epl = len(textlib.data) / (batch_size * model.steps)\n",
        "    model.init.run()\n",
        "    tflogdir = model.logdir\n",
        "    tflogdir = os.path.realpath(tflogdir)\n",
        "    if not os.path.exists(tflogdir):\n",
        "        os.makedirs(tflogdir)\n",
        "    print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
        "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
        "    train_writer.add_graph(sess.graph)\n",
        "    # vl=tf.trainable_variables()\n",
        "    # print(vl)\n",
        "    saver = tf.train.Saver()\n",
        "    checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "    # FFR: tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None,\n",
        "    # saver_def=None, collection_list=None, as_text=False, graph=None, export_scope=None,\n",
        "    # clear_devices=False, **kwargs)\n",
        "    start_iter = 0\n",
        "    if trainParams[\"restoreCheckpoints\"]:\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir,\n",
        "                                              latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "    av_batch_time=0.0\n",
        "    for iteration in range(start_iter, trainParams[\"max_iter\"]):\n",
        "        # Train with batches from the text library:\n",
        "        t1=time.time()\n",
        "        X_batch, y_batch = textlib.get_random_sample_batch(\n",
        "            batch_size, model.steps)\n",
        "        i_state = sess.run([model.init_state_0], feed_dict={model.X: X_batch})\n",
        "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
        "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
        "                                         model.init_state: i_state})\n",
        "        t2=time.time()\n",
        "        if av_batch_time==0.0:\n",
        "            av_batch_time=(t2-t1)*1000.0\n",
        "        else:\n",
        "            av_batch_time=(av_batch_time*5.0+(t2-t1)*1000.0)/6.0\n",
        "        \n",
        "        # Output training statistics every 100 iterations:\n",
        "        if iteration % 200 == 0:\n",
        "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
        "                                                          model.accuracy, model.prediction,\n",
        "                                                          model.summary_merged],\n",
        "                                                         feed_dict={model.X: X_batch, model.y: y_batch})\n",
        "            train_writer.add_summary(summary, iteration)\n",
        "            ep = iteration / epl\n",
        "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f} time per batch: {4:.5f}ms\".format(\n",
        "                ep, iteration, ce, accuracy, av_batch_time))\n",
        "            if trainParams[\"verbose\"]:\n",
        "                for ind in range(1):  # model.batch_size):\n",
        "                    ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
        "                    yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
        "                    print(\"   y:\", ys)\n",
        "                    print(\"  yp:\", yps)\n",
        "\n",
        "        # Generate sample texts for different temperature every ..NIter iterations:\n",
        "        if (iteration+1) % trainParams[\"statusEveryNIter\"] == 0:\n",
        "\n",
        "            # Save training data\n",
        "            # print(\"S>\")\n",
        "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
        "            # print(\"S<\")\n",
        "\n",
        "            if trainParams[\"generateDuringTraining\"]:\n",
        "                # Generate sample\n",
        "                for t in range(4, 11, 3):\n",
        "                    temp = float(t) / 10.0\n",
        "                    xs = ' ' * model.steps\n",
        "                    xso = ''\n",
        "                    doini = True\n",
        "                    for i in range(trainParams[\"generated_text_size\"]):\n",
        "                        X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                        if doini:\n",
        "                            doini = False\n",
        "                            g_state = sess.run(\n",
        "                                [model.init_state_0], feed_dict={model.X: X_new})\n",
        "\n",
        "                        g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                                   feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                              model.temperature: temp})\n",
        "                        inds = list(range(model.vocab_size))\n",
        "                        ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                        nc = textlib.i2c[ind]\n",
        "                        xso += nc\n",
        "                        xs = xs[1:]+nc\n",
        "\n",
        "                    print(\"----------------- temperature =\",\n",
        "                          temp, \"----------------------\")\n",
        "                    # print(xso)\n",
        "                    # 20: minimum quote size detected.\n",
        "                    textlib.source_highlight(xso, 20)\n",
        "                print(\"---------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sScszXSqQtf_"
      },
      "source": [
        "## 5. Generation of text from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P6TrXpkQtf_",
        "colab": {}
      },
      "source": [
        "# Generating text using the model data generated during training.\n",
        "def ghostWriter(textsize, temperature=1.0):\n",
        "    xso = None\n",
        "    with tf.Session() as sess:\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
        "            return None\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return None\n",
        "\n",
        "        xs = ' ' * model.steps\n",
        "        xso = ''\n",
        "        doini = True\n",
        "        for i in range(textsize):\n",
        "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "            if doini:\n",
        "                doini = False\n",
        "                g_state = sess.run([model.init_state_0],\n",
        "                                   feed_dict={model.X: X_new})\n",
        "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                       feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                  model.temperature: temperature})\n",
        "            inds = list(range(model.vocab_size))\n",
        "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "            nc = textlib.i2c[ind]\n",
        "            xso += nc\n",
        "            xs = xs[1:]+nc\n",
        "    return(xso)\n",
        "\n",
        "\n",
        "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5S9gO2iQtgD",
        "colab": {}
      },
      "source": [
        "tgen=ghostWriter(500)\n",
        "detectPlagiarism(tgen, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_poet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiOXyvbEAPo",
        "colab_type": "code",
        "outputId": "e0d62525-9ecc-4759-8da7-c8b210543103",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 637
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-gpu-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/18/4b0e66e75573b97c696ea7e43694ac9f030a1172bb68e6da75b209246a55/tf_nightly_gpu_2.0_preview-2.0.0.dev20190730-cp36-cp36m-manylinux1_x86_64.whl (378.8MB)\n",
            "\u001b[K     |████████████████████████████████| 378.8MB 47kB/s \n",
            "\u001b[?25hRequirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.8)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Collecting opt-einsum>=2.3.2 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f6/d6/44792ec668bcda7d91913c75237314e688f70415ab2acd7172c845f0b24f/opt_einsum-2.3.2.tar.gz (59kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 26.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.7)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.8.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Collecting tb-nightly<1.16.0a0,>=1.15.0a0 (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c2/9b/656b60df782ab795cb998139399f5e52595bab61826aa94e622a0756e3db/tb_nightly-1.15.0a20190730-py3-none-any.whl (4.1MB)\n",
            "\u001b[K     |████████████████████████████████| 4.1MB 40.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-gpu-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/da/ab/ac8592c015272baf0cdf9ed6f19f69dd7c70f7f5fe293f9b68babf848eb8/tensorflow_estimator_2.0_preview-1.14.0.dev2019073000-py2.py3-none-any.whl (448kB)\n",
            "\u001b[K     |████████████████████████████████| 450kB 39.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.2)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.8->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (0.15.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.16.0a0,>=1.15.0a0->tf-nightly-gpu-2.0-preview) (3.1.1)\n",
            "Building wheels for collected packages: opt-einsum\n",
            "  Building wheel for opt-einsum (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Stored in directory: /root/.cache/pip/wheels/51/3e/a3/b351fae0cbf15373c2136a54a70f43fea5fe91d8168a5faaa4\n",
            "Successfully built opt-einsum\n",
            "Installing collected packages: opt-einsum, tb-nightly, tensorflow-estimator-2.0-preview, tf-nightly-gpu-2.0-preview\n",
            "Successfully installed opt-einsum-2.3.2 tb-nightly-1.15.0a20190730 tensorflow-estimator-2.0-preview-1.14.0.dev2019073000 tf-nightly-gpu-2.0-preview-2.0.0.dev20190730\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "# [WIP] Eager Tensor Poet (tf 2.0)\n",
        "\n",
        "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation\n",
        "\n",
        "This code can use either CPU, GPU or TPU when running on Google Colab.\n",
        "\n",
        "Select the corresponding runtime (menu: Runtime / Change runtime type)\n",
        "\n",
        "Note: TPU support is not yet working."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "from urllib.request import urlopen  # Py3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1",
        "colab_type": "text"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n",
        "\n",
        "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.14.x at the time of this writing. the version check below is preliminary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab_type": "code",
        "outputId": "023e5786-df83-4bdc-d5b1-2b95158d2f3c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(\"Tensorflow api v2 active.\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P08FdKKnEAP6",
        "colab_type": "text"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWwUUfuEAP7",
        "colab_type": "code",
        "outputId": "5b44199f-4150-4866-a555-7011ca682f9e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        print(\"         Hint: If using Google Colab, set runtime type to TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')]\n",
            "GPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU')]\n",
            "TPU -> []\n",
            "GPUs: ['/device:XLA_GPU:0', '/device:GPU:0']\n",
            "[('/device:CPU:0', ''), ('/device:XLA_GPU:0', 'device: XLA_GPU device'), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:GPU:0', 'device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "# TextLibrary class: text library for training, encoding, batch generation,\n",
        "# and formatted source display\n",
        "\n",
        "\n",
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJsgqNgEAQC",
        "colab_type": "text"
      },
      "source": [
        "### Read text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDo-bIPEAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jY3hUuhQYzdT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 80\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=256\n",
        "    use_simple_model_for_tpu=False\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "LSTM_UNITS = 768\n",
        "EMBEDDING_DIM = 120\n",
        "LSTM_LAYERS = 2\n",
        "NUM_BATCHES=500"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EeB7jugCV4lI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dx=[]\n",
        "dy=[]\n",
        "for i in range(NUM_BATCHES):\n",
        "    x,y=textlib.get_random_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
        "    dx.append(x)\n",
        "    dy.append(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3whYiFuwS8q4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data_xy=(dx,dy)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DCy7WmQyS9T-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "boow8wR7sLwi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "34159259-7500-4171-ef91-d4af656c4b06"
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 80), (256, 80)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtDLPjGEAQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    *[tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform') for _ in range(lstm_layers)],\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n",
        "def build_simple_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, SEQUENCE_LEN]),\n",
        "    tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform',\n",
        "                        unroll=True),\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIxWHMGNKHk",
        "colab_type": "code",
        "outputId": "06925adf-b833-4df5-e6ab-e5cdc9276334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "\n",
        "dev_strings=[]\n",
        "for log_dev in tf.config.experimental.list_logical_devices('TPU'):\n",
        "    dev_strings.append(log_dev.name)\n",
        "print(dev_strings)\n",
        "\n",
        "# for i in range(8):\n",
        "#     dev_strings.append('/TPU:{}'.format(i))\n",
        "# print(dev_strings)\n",
        "    "
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaykJjEAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_tpu is True:\n",
        "    # tpus=tf.config.experimental.list_logical_devices('TPU')\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)    \n",
        "    # mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(dev_strings)\n",
        "    # with mirrored_strategy.scope():\n",
        "    if use_simple_model_for_tpu is True:\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_simple_model(\n",
        "              vocab_size = len(textlib.i2c),\n",
        "              embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE)\n",
        "    else:\n",
        "        with tpu_strategy.scope():\n",
        "            model = build_model(\n",
        "              vocab_size = len(textlib.i2c),\n",
        "              embedding_dim=EMBEDDING_DIM,\n",
        "              lstm_units=LSTM_UNITS,\n",
        "              lstm_layers=LSTM_LAYERS,\n",
        "              batch_size=BATCH_SIZE)        \n",
        "else:\n",
        "    model = build_model(\n",
        "      vocab_size = len(textlib.i2c),\n",
        "      embedding_dim=EMBEDDING_DIM,\n",
        "      lstm_units=LSTM_UNITS,\n",
        "      lstm_layers=LSTM_LAYERS,\n",
        "      batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhF2vqmEAQo",
        "colab_type": "code",
        "outputId": "dea8339c-9f70-46ac-b095-a9d4ac8cb94c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 80, 89) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab_type": "code",
        "outputId": "ca65b2b6-50cc-42fd-8939-05ef4c01a325",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 291
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (256, None, 120)          10680     \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (256, None, 768)          2731008   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (256, None, 89)           68441     \n",
            "=================================================================\n",
            "Total params: 7,531,793\n",
            "Trainable params: 7,531,793\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LYZs8Ss2947k",
        "colab_type": "code",
        "outputId": "c8b624f2-7270-44e6-cd8e-edec072c45a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "dataset.take(1)"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TakeDataset shapes: ((256, 80), (256, 80)), types: (tf.int32, tf.int32)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo54K1lEAQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOQdlNQEAQv",
        "colab_type": "code",
        "outputId": "0bafc4ff-82ab-40ee-ee45-aca27a44d9a3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([14, 77, 69, 14, 23, 49, 88, 26, 50, 82, 13, 71, 49, 63, 49, 34, 38,\n",
              "       71, 37, 46, 73, 49, 71, 50, 78, 42, 76, 85,  5, 16, 31, 64,  0,  1,\n",
              "       60, 75, 55, 70, 74, 34, 18, 58,  4, 69, 65, 74, 10, 23, 11, 44, 84,\n",
              "       11, 80, 18, 55, 87, 69, 81, 14, 55, 60, 40,  5, 34,  2, 73, 25, 45,\n",
              "       71, 64,  2, 26, 44,  8, 37, 20, 34, 35, 26, 44])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4RuDZLEAQy",
        "colab_type": "code",
        "outputId": "a2e88add-bc64-4daf-f00b-f5840ccd3e27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "textlib.decode(sampled_indices)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gZWgp3`\\r78b63(3.:6L0q367XS!$rBw)ThU;*9x.fHPW_xGpu5@u?f*}WQg*URr.eqy26)e\\r5cLd.Y\\r5'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-0Y2uYEAQ0",
        "colab_type": "code",
        "outputId": "145f630d-7b3f-4f04-850d-afd77547b53b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 80, 89)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.487939\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py9WnmosEAQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKvObcsEAQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aVq0V47lzBW5",
        "colab_type": "text"
      },
      "source": [
        "The following currently crashes on TPU with a recursion error. It does work with CPU and GPU."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "colab_type": "code",
        "outputId": "f2f69571-d1fa-4c1b-842a-23b7c3e293bc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0731 11:02:25.550608 139722506491776 training_utils.py:1513] Expected a shuffled dataset but input dataset `x` is not shuffled. Please invoke `shuffle()` on input dataset.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0731 11:02:26.189514 139722506491776 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py:457: BaseResourceVariable.constraint (from tensorflow.python.ops.resource_variable_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Apply a constraint manually following the optimizer update step.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "500/500 [==============================] - 154s 308ms/step - loss: 2.2909\n",
            "Epoch 2/100\n",
            "379/500 [=====================>........] - ETA: 37s - loss: 1.4362"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvupep98EARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMrNH0eEARC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7qRl5EEARE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = len(textlib.i2c),\n",
        "  embedding_dim=EMBEDDING_DIM,\n",
        "  lstm_units=LSTM_UNITS,\n",
        "  lstm_layers=LSTM_LAYERS,\n",
        "  batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6JhIGxEARF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLmNIvzEARH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [textlib.c2i[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7750ibzEARJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx,id=generate_text(model, start_string=\"Good\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mbC9l8EARL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tql-eLGvEARO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "textlib.decode(id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBil008pEARS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYmlO_IEARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model [not ported yet]"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
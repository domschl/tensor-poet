{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "eager_poet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "5oK6Ej8OQtf1",
        "48X8-lFFQtf3",
        "zOlVTPtFQtf7",
        "sScszXSqQtf_",
        "WW6LPdlhQtgF"
      ],
      "include_colab_link": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fEXNOWhCEAPk",
        "colab_type": "text"
      },
      "source": [
        "## Install TF 2.0, if necessary. This currently needs to be done when running from Colab."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ohiOXyvbEAPo",
        "colab_type": "code",
        "outputId": "70144170-77ed-4bbb-a94e-22acad94e5b5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        }
      },
      "source": [
        "!pip install tf-nightly-gpu-2.0-preview # tensorflow-gpu==2.0.0-alpha0"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-nightly-gpu-2.0-preview in /usr/local/lib/python3.6/dist-packages (2.0.0.dev20190517)\n",
            "Requirement already satisfied: tensorflow-estimator-2.0-preview in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0.dev2019051700)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.33.4)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.7)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.2.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (3.7.1)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a0,>=1.14.0a0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.14.0a20190517)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.11.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.0.9)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.1.6)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.16.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-gpu-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-gpu-2.0-preview) (2.8.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-gpu-2.0-preview) (41.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (3.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.15.0a0,>=1.14.0a0->tf-nightly-gpu-2.0-preview) (0.15.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4s78tdpcEAPt",
        "colab_type": "text"
      },
      "source": [
        "## References:\n",
        "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
        "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "VmWbteSFQtfq"
      },
      "source": [
        "# [WIP] Eager Tensor Poet (tf 2.0)\n",
        "\n",
        "**THIS IS UNFINISHED WORK IN PROGRESS**\n",
        "\n",
        "A tensorflow deep LSTM model for text generation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EgLLjG4yQtft",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import os\n",
        "import json\n",
        "import time\n",
        "import random\n",
        "import tensorflow as tf\n",
        "from IPython.core.display import display, HTML\n",
        "\n",
        "from urllib.request import urlopen  # Py3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e4yYLii8Qtfw"
      },
      "source": [
        "### Content\n",
        "This notebook contains the following sections:\n",
        "1. TextLibrary: utilities to work with text files\n",
        "  * loading of a list of files (local or URLs)\n",
        "  * encoding for training\n",
        "  * formatted output with quote-highlighting\n",
        "2. Transform text data to tf.data\n",
        "\n",
        "\n",
        "...\n",
        "\n",
        "\n",
        "x. Definition of the tensorflow model\n",
        "x. Model and training parameters\n",
        "x. The actual training on the data (required 1. - 3.)\n",
        "  * Training can be restarted, since the model is saved periodically.\n",
        "x. Generation of text from the trained model (requires 1. - 4.)\n",
        "x. In dialog with with the model (requires 1. - 4.)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sfZg31sMEAP1",
        "colab_type": "text"
      },
      "source": [
        "## 0. Check system\n",
        "\n",
        "### Tensorflow api version check\n",
        "\n",
        "Temporary note: currently, this is tested against the master build of tensorflow, which still has a version tag 1.13.x at the time of this writing. the version check below is preliminary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llPw84PkEAP2",
        "colab_type": "code",
        "outputId": "05d28c08-8122-471e-a81e-9e0bc879872f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "try:\n",
        "    if 'api.v2' in tf.version.__name__:\n",
        "        print(\"Tensorflow api v2 active.\")\n",
        "    else:\n",
        "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
        "except:\n",
        "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tensorflow api v2 active.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P08FdKKnEAP6",
        "colab_type": "text"
      },
      "source": [
        "### GPU/TPU check"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gjWwUUfuEAP7",
        "colab_type": "code",
        "outputId": "7b26c2c0-26b3-440c-81a3-20f3ad988e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "from tensorflow.python.client import device_lib\n",
        "\n",
        "use_tpu = False\n",
        "use_gpu = False\n",
        "\n",
        "try:\n",
        "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "    use_tpu = True\n",
        "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
        "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
        "except:\n",
        "    print(\"No TPU available\")\n",
        "\n",
        "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
        "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
        "    print(\"{} -> {}\".format(hw,hwlist))\n",
        "\n",
        "\n",
        "if use_tpu is False:\n",
        "    def get_available_devs_of_type(type):\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [x.name for x in local_device_protos if type in x.name]\n",
        "\n",
        "    def get_dev_desc():\n",
        "        local_device_protos = device_lib.list_local_devices()\n",
        "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
        "\n",
        "    def get_available_gpus():\n",
        "        return get_available_devs_of_type('GPU')\n",
        "\n",
        "    dl = get_available_gpus()\n",
        "    if len(dl)==0:\n",
        "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
        "        print(\"         Hint: If using Google Colab, set runtime type to TPU.\")\n",
        "        print(get_available_devs_of_type(''))\n",
        "    else:\n",
        "        use_gpu = True\n",
        "        print(f\"GPUs: {dl}\")\n",
        "        print(get_dev_desc())\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "No TPU available\n",
            "CPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:CPU:0', device_type='CPU')]\n",
            "GPU -> [LogicalDevice(name='/job:localhost/replica:0/task:0/device:GPU:0', device_type='GPU')]\n",
            "TPU -> []\n",
            "GPUs: ['/device:XLA_GPU:0', '/device:GPU:0']\n",
            "[('/device:CPU:0', ''), ('/device:XLA_GPU:0', 'device: XLA_GPU device'), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:GPU:0', 'device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AIkcYcEuQtfx"
      },
      "source": [
        "##  1. Text library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-Pz4xVgaQtfy",
        "colab": {}
      },
      "source": [
        "# TextLibrary class: text library for training, encoding, batch generation,\n",
        "# and formatted source display\n",
        "\n",
        "\n",
        "class TextLibrary:\n",
        "    def __init__(self, descriptors, max=100000000):\n",
        "        self.descriptors = descriptors\n",
        "        self.data = ''\n",
        "        self.files = []\n",
        "        self.c2i = {}\n",
        "        self.i2c = {}\n",
        "        index = 1\n",
        "        for descriptor, name in descriptors:\n",
        "            fd = {}\n",
        "            if descriptor[:4] == 'http':\n",
        "                try:\n",
        "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
        "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
        "                        dat=dat[1:]\n",
        "                    self.data += dat\n",
        "                    fd[\"name\"] = name\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                except Exception as e:\n",
        "                    print(f\"Can't download {descriptor}: {e}\")\n",
        "            else:\n",
        "                fd[\"name\"] = name\n",
        "                try:\n",
        "                    f = open(descriptor)\n",
        "                    dat = f.read(max)\n",
        "                    self.data += dat\n",
        "                    fd[\"data\"] = dat\n",
        "                    fd[\"index\"] = index\n",
        "                    index += 1\n",
        "                    self.files.append(fd)\n",
        "                    f.close()\n",
        "                except Exception as e:\n",
        "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
        "        ind = 0\n",
        "        for c in self.data:  # sets are not deterministic\n",
        "            if c not in self.c2i:\n",
        "                self.c2i[c] = ind\n",
        "                self.i2c[ind] = c\n",
        "                ind += 1\n",
        "        self.ptr = 0\n",
        "\n",
        "    def display_colored_html(self, textlist, pre='', post=''):\n",
        "        bgcolors = ['#d4e6f1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
        "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
        "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
        "        out = ''\n",
        "        for txt, ind in textlist:\n",
        "            txt = txt.replace('\\n', '<br>')\n",
        "            if ind == 0:\n",
        "                out += txt\n",
        "            else:\n",
        "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
        "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
        "        display(HTML(pre+out+post))\n",
        "\n",
        "    def source_highlight(self, txt, minQuoteSize=10):\n",
        "        tx = txt\n",
        "        out = []\n",
        "        qts = []\n",
        "        txsrc = [(\"Sources: \", 0)]\n",
        "        sc = False\n",
        "        noquote = ''\n",
        "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
        "            mxQ = 0\n",
        "            mxI = 0\n",
        "            mxN = ''\n",
        "            found = False\n",
        "            for f in self.files:  # find longest quote in all texts\n",
        "                p = minQuoteSize\n",
        "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                    p = minQuoteSize + 1\n",
        "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
        "                        p += 1\n",
        "                    if p-1 > mxQ:\n",
        "                        mxQ = p-1\n",
        "                        mxI = f[\"index\"]\n",
        "                        mxN = f[\"name\"]\n",
        "                        found = True\n",
        "            if found:  # save longest quote for colorizing\n",
        "                if len(noquote) > 0:\n",
        "                    out.append((noquote, 0))\n",
        "                    noquote = ''\n",
        "                out.append((tx[:mxQ], mxI))\n",
        "                tx = tx[mxQ:]\n",
        "                if mxI not in qts:  # create a new reference, if first occurence\n",
        "                    qts.append(mxI)\n",
        "                    if sc:\n",
        "                        txsrc.append((\", \", 0))\n",
        "                    sc = True\n",
        "                    txsrc.append((mxN, mxI))\n",
        "            else:\n",
        "                noquote += tx[0]\n",
        "                tx = tx[1:]\n",
        "        if len(noquote) > 0:\n",
        "            out.append((noquote, 0))\n",
        "            noquote = ''\n",
        "        self.display_colored_html(out)\n",
        "        if len(qts) > 0:  # print references, if there is at least one source\n",
        "            self.display_colored_html(txsrc, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
        "                                     post=\"</p></small>\")\n",
        "\n",
        "    def get_slice(self, length):\n",
        "        if (self.ptr + length >= len(self.data)):\n",
        "            self.ptr = 0\n",
        "        if self.ptr == 0:\n",
        "            rst = True\n",
        "        else:\n",
        "            rst = False\n",
        "        sl = self.data[self.ptr:self.ptr+length]\n",
        "        self.ptr += length\n",
        "        return sl, rst\n",
        "\n",
        "    def decode(self, ar):\n",
        "        return ''.join([self.i2c[ic] for ic in ar])\n",
        "\n",
        "    def get_random_slice(self, length):\n",
        "        p = random.randrange(0, len(self.data)-length)\n",
        "        sl = self.data[p:p+length]\n",
        "        return sl\n",
        "\n",
        "    def get_slice_array(self, length):\n",
        "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
        "        return ar\n",
        "\n",
        "    def get_encoded_slice(self, length):\n",
        "        s, rst = self.get_slice(length)\n",
        "        X = [self.c2i[c] for c in s]\n",
        "        return X\n",
        "        \n",
        "    def get_encoded_slice_array(self, length):\n",
        "        return np.array(self.get_encoded_slice(length))\n",
        "\n",
        "    def get_sample(self, length):\n",
        "        s, rst = self.get_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y, rst)\n",
        "\n",
        "    def get_random_sample(self, length):\n",
        "        s = self.get_random_slice(length+1)\n",
        "        X = [self.c2i[c] for c in s[:-1]]\n",
        "        y = [self.c2i[c] for c in s[1:]]\n",
        "        return (X, y)\n",
        "\n",
        "    def get_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi, rst = self.get_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy, rst\n",
        "\n",
        "    def get_random_sample_batch(self, batch_size, length):\n",
        "        smpX = []\n",
        "        smpy = []\n",
        "        for i in range(batch_size):\n",
        "            Xi, yi = self.get_random_sample(length)\n",
        "            smpX.append(Xi)\n",
        "            smpy.append(yi)\n",
        "        return smpX, smpy\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucJsgqNgEAQC",
        "colab_type": "text"
      },
      "source": [
        "### Read text data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tZDo-bIPEAQD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "libdesc = {\n",
        "    \"name\": \"Woman Writers\",\n",
        "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
        "    \"lib\": [\n",
        "        # 'data/tiny-shakespeare.txt',\n",
        "        # since project gutenberg blocks the entire country of Germany, we use a mirror:\n",
        "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', \"Shakespeare: Collected Works\"\n",
        "        #  Project Gutenberg: Pride and Prejudice_ by Jane Austen, Wuthering Heights by Emily Brontë, The Voyage Out by Virginia Woolf and Emma_by Jane Austen\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', \"Jane Austen: Pride and Prejudice\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', \"Emily Brontë: Wuthering Heights\"),         \n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', \"Virginia Wolf: Voyage out\"),\n",
        "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', \"Jane Austen: Emma\")\n",
        "    ]\n",
        "}\n",
        "\n",
        "textlib = TextLibrary(libdesc[\"lib\"])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30hi0UPtEAQG",
        "colab_type": "text"
      },
      "source": [
        "## 2. Use tf.data for texts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z68E8t2qEAQH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = textlib.get_encoded_slice_array(len(textlib.data))\n",
        "textlib_dataset = tf.data.Dataset.from_tensor_slices(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHNgHiH1EAQK",
        "colab_type": "code",
        "outputId": "e6506005-6211-4fd5-ef22-42625b05c3b9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        }
      },
      "source": [
        "# Quick test\n",
        "n=np.array([])\n",
        "for i in textlib_dataset.take(90):\n",
        "    n=np.append(n,i.numpy())\n",
        "print(n)    \n",
        "print(textlib.decode(n))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0.  1.  2.  3.  4.  5.  6.  7.  2.  8.  9.  3. 10. 11.  9.  2. 12. 13.\n",
            "  2.  5. 14.  3. 15. 16.  6.  6. 17.  3.  6. 18.  3.  4.  5. 19. 20.  2.\n",
            "  3. 21. 12. 20.  3.  4.  5.  2.  7. 11. 20. 19.  8.  2. 22.  3. 21.  3.\n",
            " 23. 24. 21. 25. 22.  3. 13. 25. 26. 27. 28. 21.  5. 25.  3. 29.  2. 19.\n",
            "  9.  1.  3. 28.  2. 20. 13.  2.  5. 25.  3. 28. 21.  8. 17. 21. 25.  2.]\n",
            "The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medbery Mackaye\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0mnBkXfEAQN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "SEQUENCE_LEN = 80\n",
        "if use_tpu is True:\n",
        "    BATCH_SIZE=1024\n",
        "else:\n",
        "    BATCH_SIZE = 256\n",
        "LSTM_UNITS = 768\n",
        "EMBEDDING_DIM = 120\n",
        "LSTM_LAYERS = 6"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9o0v62LKEAQP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_size=len(data)//SEQUENCE_LEN"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xgScEoozEAQR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sequences=textlib_dataset.batch(SEQUENCE_LEN+1,drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rg_gWvzqEAQT",
        "colab_type": "code",
        "outputId": "8cb498f4-1ce3-46d5-ca24-0fb3b9150247",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 431
        }
      },
      "source": [
        "# Quick test\n",
        "for arr in sequences.take(3):\n",
        "    n=arr.numpy()\n",
        "    print(arr)\n",
        "    print(\">\"+textlib.decode(n))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tf.Tensor(\n",
            "[ 0  1  2  3  4  5  6  7  2  8  9  3 10 11  9  2 12 13  2  5 14  3 15 16\n",
            "  6  6 17  3  6 18  3  4  5 19 20  2  3 21 12 20  3  4  5  2  7 11 20 19\n",
            "  8  2 22  3 21  3 23 24 21 25 22  3 13 25 26 27 28 21  5 25  3 29  2 19\n",
            "  9  1  3 28  2 20 13  2  5], shape=(81,), dtype=int64)\n",
            ">The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medber\n",
            "tf.Tensor(\n",
            "[25  3 28 21  8 17 21 25  2 26 27 26 27  0  1 19 30  3  2 16  6  6 17  3\n",
            " 19 30  3 18  6  5  3  9  1  2  3 11 30  2  3  6 18  3 21 12 25  6 12  2\n",
            "  3 21 12 25 31  1  2  5  2  3 21  9  3 12  6  3  8  6 30  9  3 21 12 20\n",
            "  3 31 19  9  1 26 27 21 24], shape=(81,), dtype=int64)\n",
            ">y Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "al\n",
            "tf.Tensor(\n",
            "[32  6 30  9  3 12  6  3  5  2 30  9  5 19  8  9 19  6 12 30  3 31  1 21\n",
            "  9 30  6  2 33  2  5 34  3  3 35  6 11  3 32 21 25  3  8  6 23 25  3 19\n",
            "  9 22  3 14 19 33  2  3 19  9  3 21 31 21 25  3  6  5 26 27  5  2 36 11\n",
            " 30  2  3 19  9  3 11 12 20], shape=(81,), dtype=int64)\n",
            ">most no restrictions whatsoever.  You may copy it, give it away or\r\n",
            "re-use it und\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWvfNurDEAQW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "dataset = sequences.map(split_input_target)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gxMYWYbjEAQa",
        "colab_type": "code",
        "outputId": "c372e9cb-4b7e-4d5a-bb30-d338fb99be0c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        }
      },
      "source": [
        "# Quick test\n",
        "for input_text, output_text in dataset.take(2):\n",
        "    print(\"I:\"+textlib.decode(input_text.numpy()))\n",
        "    print(\"O:\"+textlib.decode(output_text.numpy()))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I:The Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medbe\n",
            "O:he Project Gutenberg EBook of Pride and Prejudice, a play, by\r\n",
            "Mary Keith Medber\n",
            "I:y Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "a\n",
            "O: Mackaye\r\n",
            "\r\n",
            "This eBook is for the use of anyone anywhere at no cost and with\r\n",
            "al\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iE7tYcLdEAQd",
        "colab_type": "code",
        "outputId": "1e471470-3d72-4e86-dd91-bf7bcba4c9b0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Buffer size to shuffle the dataset\n",
        "# (TF data is designed to work with possibly infinite sequences,\n",
        "# so it doesn't attempt to shuffle the entire sequence in memory. Instead,\n",
        "# it maintains a buffer in which it shuffles elements).\n",
        "BUFFER_SIZE = 100000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<BatchDataset shapes: ((256, 80), (256, 80)), types: (tf.int64, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLtDLPjGEAQi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size):\n",
        "  model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
        "                              batch_input_shape=[batch_size, None]),\n",
        "    *[tf.keras.layers.LSTM(lstm_units,\n",
        "                        return_sequences=True,\n",
        "                        stateful=True,\n",
        "                        recurrent_initializer='glorot_uniform') for _ in range(lstm_layers)],\n",
        "    tf.keras.layers.Dense(vocab_size)\n",
        "  ])\n",
        "  return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oQIxWHMGNKHk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "9a583879-5a26-4f64-d1fa-89d9a3d7a7ce"
      },
      "source": [
        "dev_strings=[]\n",
        "for log_dev in tf.config.experimental.list_logical_devices('TPU'):\n",
        "    dev_strings.append(log_dev.name)\n",
        "print(dev_strings)\n",
        "\n",
        "# for i in range(8):\n",
        "#     dev_strings.append('/TPU:{}'.format(i))\n",
        "# print(dev_strings)\n",
        "    "
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NMsaykJjEAQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if use_tpu is True:\n",
        "    # tpus=tf.config.experimental.list_logical_devices('TPU')\n",
        "    resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
        "    tf.tpu.experimental.initialize_tpu_system(resolver)\n",
        "    tpu_strategy = tf.distribute.experimental.TPUStrategy(resolver)    \n",
        "    # mirrored_strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(dev_strings)\n",
        "    # with mirrored_strategy.scope():\n",
        "    with tpu_strategy.scope():\n",
        "        model = build_model(\n",
        "          vocab_size = len(textlib.i2c),\n",
        "          embedding_dim=EMBEDDING_DIM,\n",
        "          lstm_units=LSTM_UNITS,\n",
        "          lstm_layers=LSTM_LAYERS,\n",
        "          batch_size=BATCH_SIZE)\n",
        "else:\n",
        "    model = build_model(\n",
        "      vocab_size = len(textlib.i2c),\n",
        "      embedding_dim=EMBEDDING_DIM,\n",
        "      lstm_units=LSTM_UNITS,\n",
        "      lstm_layers=LSTM_LAYERS,\n",
        "      batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhF2vqmEAQo",
        "colab_type": "code",
        "outputId": "4ec965ad-2f90-4d40-f52e-e10a34eb0943",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(256, 80, 89) # (batch_size, sequence_length, vocab_size)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vxZF0wOEAQr",
        "colab_type": "code",
        "outputId": "e6613c5a-b1fb-479e-f711-f999882bd071",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        }
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (256, None, 120)          10680     \n",
            "_________________________________________________________________\n",
            "lstm (LSTM)                  (256, None, 768)          2731008   \n",
            "_________________________________________________________________\n",
            "lstm_1 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_2 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_3 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_4 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "lstm_5 (LSTM)                (256, None, 768)          4721664   \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (256, None, 89)           68441     \n",
            "=================================================================\n",
            "Total params: 26,418,449\n",
            "Trainable params: 26,418,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fKo54K1lEAQt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHOQdlNQEAQv",
        "colab_type": "code",
        "outputId": "8e472093-ce4b-4f4b-d1f4-fa9068887e25",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        }
      },
      "source": [
        "sampled_indices"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([64, 55, 66, 14, 35, 12, 51,  9, 86, 56, 46,  2, 87, 17, 17, 42, 37,\n",
              "        2, 52, 55, 66, 54,  5,  8, 71, 48, 55,  0, 44, 35, 57, 30, 56, 57,\n",
              "       48, 67, 43, 54,  8, 46, 71, 55, 32, 10, 39, 75, 19, 30, 67,  3,  2,\n",
              "       19, 26,  1, 41, 21, 16, 54, 47, 74, 28,  2, 25, 10, 59, 11, 76,  6,\n",
              "       22, 29, 60, 62, 24,  4, 76, 78, 77, 81, 16,  8])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ic4RuDZLEAQy",
        "colab_type": "code",
        "outputId": "254fa8e0-85c5-4fc8-86ce-61c42867edc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "textlib.decode(sampled_indices)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "')*\"gYn4t{O0e}kkSLe]*\"Irc6#*T5YFsOF#\\'1Ic06*mGA;is\\' ei\\rhDaBI[xMeyGJu!o,KU/lP!XZQBc'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6i-0Y2uYEAQ0",
        "colab_type": "code",
        "outputId": "6d141a89-e5f6-4e6a-c193-f2fd929ba576",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "def loss(labels, logits):\n",
        "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Prediction shape:  (256, 80, 89)  # (batch_size, sequence_length, vocab_size)\n",
            "scalar_loss:       4.4886584\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "py9WnmosEAQ3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5SKvObcsEAQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Directory where the checkpoints will be saved\n",
        "checkpoint_dir = './training_checkpoints'\n",
        "# Name of the checkpoint files\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
        "\n",
        "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=checkpoint_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kh2yUKBoEAQ8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "EPOCHS=15"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLbsTmtnEAQ-",
        "colab_type": "code",
        "outputId": "35d17225-a659-4b7b-81f7-9a521c5ca8d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 737
        }
      },
      "source": [
        "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "124/124 [==============================] - 313s 3s/step - loss: 3.2364\n",
            "Epoch 2/15\n",
            "124/124 [==============================] - 306s 2s/step - loss: 3.1816\n",
            "Epoch 3/15\n",
            "124/124 [==============================] - 306s 2s/step - loss: 2.6952\n",
            "Epoch 4/15\n",
            "124/124 [==============================] - 301s 2s/step - loss: 2.1990\n",
            "Epoch 5/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.9101\n",
            "Epoch 6/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.7038\n",
            "Epoch 7/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.5469\n",
            "Epoch 8/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.4619\n",
            "Epoch 9/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.4036\n",
            "Epoch 10/15\n",
            "124/124 [==============================] - 296s 2s/step - loss: 1.3606\n",
            "Epoch 11/15\n",
            "106/124 [========================>.....] - ETA: 42s - loss: 1.3298"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-28-25e345c13e8b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcheckpoint_callback\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    692\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    693\u001b[0m           \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 694\u001b[0;31m           initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    695\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    696\u001b[0m     \u001b[0;31m# Case 3: Symbolic tensors or Numpy array-like.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1431\u001b[0m         \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1432\u001b[0m         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1433\u001b[0;31m         steps_name='steps_per_epoch')\n\u001b[0m\u001b[1;32m   1434\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1435\u001b[0m   def evaluate_generator(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mmodel_iteration\u001b[0;34m(model, data, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, validation_freq, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch, mode, batch_size, steps_name, **kwargs)\u001b[0m\n\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m       \u001b[0mis_deferred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m       \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mbatch_outs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics)\u001b[0m\n\u001b[1;32m   1173\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_update_sample_weight_modes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3441\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3442\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3443\u001b[0;31m     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3444\u001b[0m     return nest.pack_sequence_as(\n\u001b[1;32m   3445\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_outputs_structure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[1;32m    560\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[0;32m--> 561\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    658\u001b[0m     \u001b[0;31m# Only need to override the gradient in graph mode and when we have outputs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    659\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 660\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    661\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    662\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_register_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args)\u001b[0m\n\u001b[1;32m    432\u001b[0m             attrs=(\"executor_type\", executor_type,\n\u001b[1;32m    433\u001b[0m                    \"config_proto\", config),\n\u001b[0;32m--> 434\u001b[0;31m             ctx=ctx)\n\u001b[0m\u001b[1;32m    435\u001b[0m       \u001b[0;31m# Replace empty list with None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rvupep98EARA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnMrNH0eEARC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8cd84236-888a-4429-9520-5c79f2bbeb83"
      },
      "source": [
        "tf.train.latest_checkpoint(checkpoint_dir)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'./training_checkpoints/ckpt_10'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qA7qRl5EEARE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = build_model(vocab_size = len(textlib.i2c),\n",
        "  embedding_dim=EMBEDDING_DIM,\n",
        "  lstm_units=LSTM_UNITS,\n",
        "  lstm_layers=LSTM_LAYERS,\n",
        "  batch_size=1)\n",
        "model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))\n",
        "\n",
        "model.build(tf.TensorShape([1, None]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQ6JhIGxEARF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "outputId": "2304d904-0ee0-43e7-cf7d-cc346d13b1ce"
      },
      "source": [
        "model.summary()"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding_1 (Embedding)      (1, None, 120)            10680     \n",
            "_________________________________________________________________\n",
            "lstm_6 (LSTM)                (1, None, 768)            2731008   \n",
            "_________________________________________________________________\n",
            "lstm_7 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_8 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_9 (LSTM)                (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_10 (LSTM)               (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "lstm_11 (LSTM)               (1, None, 768)            4721664   \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (1, None, 89)             68441     \n",
            "=================================================================\n",
            "Total params: 26,418,449\n",
            "Trainable params: 26,418,449\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "geLmNIvzEARH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def generate_text(model, start_string):\n",
        "  # Evaluation step (generating text using the learned model)\n",
        "\n",
        "  # Number of characters to generate\n",
        "  num_generate = 1000\n",
        "\n",
        "  # Converting our start string to numbers (vectorizing)\n",
        "  input_eval = [textlib.c2i[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Empty string to store our results\n",
        "  text_generated = []\n",
        "  ids=[]\n",
        "\n",
        "  # Low temperatures results in more predictable text.\n",
        "  # Higher temperatures results in more surprising text.\n",
        "  # Experiment to find the best setting.\n",
        "  temperature = 1.0\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(num_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the word returned by the model\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "      ids.append(predicted_id)\n",
        "\n",
        "      # We pass the predicted word as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(textlib.i2c[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated), ids)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H7750ibzEARJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tx,id=generate_text(model, start_string=\"Good\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b-mbC9l8EARL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(tx, minQuoteLength)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tql-eLGvEARO",
        "colab_type": "code",
        "outputId": "851c2c86-a71a-452a-c0c4-7e2e7221eb5f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "textlib.decode(id)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'QR[[Qu Which, who walk in it, om now, Elizabethed\\r\\nwith pointing from Reint of much bed dickined exactly waid that\\'s sorry\\r\\n\\r\\n\\r\\n\\rAF only for her-humels.  Catherine soured died faster\\r\\nmuch aloud priticated besides by sudden\\r\\nhim, however, it unceapinary, the earth bad turned\\r\\nwhat can on the way to project Gutenbergerger the frout boy; it would\\r\\nhardly\\r\\nelogtecred milm; she would asside to be all theYe daughter--Mrs. Elton\\'s twelty\\r\\nstable. Refwockied me by their very girls, when I had no curchence in equallant represented me. At all! and Arthur\\r\\nfor Mr. Elton\\'s speaking you.\"\\r\\n\\r\\nAfter _no palm of every unf of it about it appounted to the first to have had seen his\\r\\ndomail blood? Lurdon Mr. Knightley, \"and,\\' he had estored each the commence to complehely than so arside by the good\\r\\nquite totechen when I should have embagement.  Terms was his sentence.\\'\\r\\n\\r\\n\\'If you mayed with the mountain\": streener conceited; \"be loss to give, she loot, must\\r\\nis come into the night; but the compission\\r\\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZBil008pEARS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 413
        },
        "outputId": "ee58c54e-4c5d-4683-f1b9-7c29702683b7"
      },
      "source": [
        "detectPlagiarism(tx, textlib)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "GoodQR[[Qu Whi<span style=\"background-color:#eadbd8;\">ch, who wa</span><sup>[3]</sup>lk in it, om now<span style=\"background-color:#d8daef;\">, Elizabeth</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">ed\r<br>with p</span><sup>[2]</sup>oi<span style=\"background-color:#ebdef0;\">nting from </span><sup>[2]</sup>Reint<span style=\"background-color:#e2d7d5;\"> of much be</span><sup>[4]</sup>d dickin<span style=\"background-color:#eadbd8;\">ed exactly w</span><sup>[3]</sup>ai<span style=\"background-color:#e2d7d5;\">d that's so</span><sup>[4]</sup>rry\r<br>\r<br>\r<br>\rAF<span style=\"background-color:#d8daef;\"> only for </span><sup>[1]</sup>her-humel<span style=\"background-color:#ebdef0;\">s.  Catherine</span><sup>[2]</sup> soured died faster\r<br>much aloud priticate<span style=\"background-color:#d8daef;\">d besides </span><sup>[1]</sup>b<span style=\"background-color:#e2d7d5;\">y sudden\r<br></span><sup>[4]</sup><span style=\"background-color:#ebdef0;\">him, however, </span><sup>[2]</sup>it unceapinary,<span style=\"background-color:#ebdef0;\"> the earth b</span><sup>[2]</sup><span style=\"background-color:#eadbd8;\">ad turned\r<br></span><sup>[3]</sup>what c<span style=\"background-color:#eadbd8;\">an on the </span><sup>[3]</sup>way to p<span style=\"background-color:#d8daef;\">roject Gutenberg</span><sup>[1]</sup>erg<span style=\"background-color:#ebdef0;\">er the fro</span><sup>[2]</sup>ut bo<span style=\"background-color:#eadbd8;\">y; it would</span><sup>[3]</sup>\r<br>hardly\r<br>elogtecred milm<span style=\"background-color:#ebdef0;\">; she would </span><sup>[2]</sup>ass<span style=\"background-color:#ebdef0;\">ide to be </span><sup>[2]</sup>all theY<span style=\"background-color:#d8daef;\">e daughter</span><sup>[1]</sup><span style=\"background-color:#e2d7d5;\">--Mrs. Elton</span><sup>[4]</sup>'s twelty\r<br>stable. Refwockied<span style=\"background-color:#ebdef0;\"> me by the</span><sup>[2]</sup>ir very<span style=\"background-color:#d8daef;\"> girls, wh</span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">en I had no </span><sup>[2]</sup>curchence in equallan<span style=\"background-color:#eadbd8;\">t represented </span><sup>[3]</sup>me. At all!<span style=\"background-color:#eadbd8;\"> and Arthur</span><sup>[3]</sup><span style=\"background-color:#e2d7d5;\">\r<br>for Mr. Elton</span><sup>[4]</sup>'<span style=\"background-color:#eadbd8;\">s speaking </span><sup>[3]</sup><span style=\"background-color:#eadbd8;\">you.\"\r<br>\r<br>A</span><sup>[3]</sup>fter _no pal<span style=\"background-color:#e2d7d5;\">m of every </span><sup>[4]</sup>unf of i<span style=\"background-color:#ebdef0;\">t about it </span><sup>[2]</sup>app<span style=\"background-color:#ebdef0;\">ounted to the </span><sup>[2]</sup>fir<span style=\"background-color:#ebdef0;\">st to have </span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">had seen hi</span><sup>[4]</sup>s\r<br>domail blood? Lurd<span style=\"background-color:#e2d7d5;\">on Mr. Knightley</span><sup>[4]</sup>, \"and,' he had estored<span style=\"background-color:#eadbd8;\"> each the </span><sup>[3]</sup><span style=\"background-color:#ebdef0;\">commence t</span><sup>[2]</sup>o complehely than so ars<span style=\"background-color:#e2d7d5;\">ide by the </span><sup>[4]</sup>good\r<br>quite toteche<span style=\"background-color:#ebdef0;\">n when I s</span><sup>[2]</sup><span style=\"background-color:#e2d7d5;\">hould have e</span><sup>[4]</sup>mbagement.  Terms<span style=\"background-color:#ebdef0;\"> was his s</span><sup>[2]</sup>ente<span style=\"background-color:#ebdef0;\">nce.'\r<br>\r<br>'</span><sup>[2]</sup>If you m<span style=\"background-color:#eadbd8;\">ayed with the</span><sup>[3]</sup> mountain\": streener<span style=\"background-color:#e2d7d5;\"> conceited; </span><sup>[4]</sup>\"b<span style=\"background-color:#d8daef;\">e loss to </span><sup>[1]</sup>giv<span style=\"background-color:#eadbd8;\">e, she loo</span><sup>[3]</sup>t, must\r<br>is<span style=\"background-color:#d8daef;\"> come into the </span><sup>[1]</sup><span style=\"background-color:#ebdef0;\">night; but the</span><sup>[2]</sup> compission\r<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#eadbd8;\">Virginia Wolf: Voyage out</span><sup>[3]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup></p></small>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXYmlO_IEARU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWE_ZZMKEARV",
        "colab_type": "text"
      },
      "source": [
        "**below this point not yet ported**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5oK6Ej8OQtf1"
      },
      "source": [
        "## 2. Definition of the Tensorflow model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "GP4sk47FQtf1",
        "colab": {}
      },
      "source": [
        "# The tensorflow model for text generation\n",
        "class TensorPoetModel:\n",
        "    def __init__(self, params):\n",
        "        self.model_name = params[\"model_name\"]\n",
        "        self.vocab_size = params[\"vocab_size\"]\n",
        "        self.neurons = params[\"neurons\"]\n",
        "        self.layers = params[\"layers\"]\n",
        "        self.learning_rate = params[\"learning_rate\"]\n",
        "        self.steps = params[\"steps\"]\n",
        "        self.logdir = params[\"logdir\"]\n",
        "        self.checkpoint = params[\"checkpoint\"]\n",
        "        # self.clip = -1.0 * params[\"clip\"]\n",
        "\n",
        "        tf.reset_default_graph()\n",
        "\n",
        "        # Training & Generating:\n",
        "        self.X = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "        self.y = tf.placeholder(tf.int32, shape=[None, self.steps])\n",
        "\n",
        "        onehot_X = tf.one_hot(self.X, self.vocab_size)\n",
        "        onehot_y = tf.one_hot(self.y, self.vocab_size)\n",
        "\n",
        "        stacked_cell = tf.contrib.rnn.MultiRNNCell([tf.nn.rnn_cell.LSTMCell(\n",
        "            self.neurons, name='basic_lstm_cell') for _ in range(self.layers)])\n",
        "\n",
        "        batch_size = tf.shape(self.X)[0]\n",
        "\n",
        "        self.init_state_0 = stacked_cell.zero_state(batch_size, tf.float32)\n",
        "        self.init_state = self.init_state_0\n",
        "\n",
        "        with tf.variable_scope('rnn') as scope:\n",
        "            rnn_outputs, states = tf.nn.dynamic_rnn(stacked_cell, onehot_X,\n",
        "                                                    initial_state=self.init_state,\n",
        "                                                    dtype=tf.float32)\n",
        "            self.init_state = states\n",
        "\n",
        "        self.final_state = self.init_state\n",
        "        stacked_rnn_outputs = tf.reshape(rnn_outputs, [-1, self.neurons])\n",
        "\n",
        "        softmax_w = tf.Variable(tf.random_normal(\n",
        "            [self.neurons, self.vocab_size]), dtype=tf.float32, name='sm_w')\n",
        "        softmax_b = tf.Variable(\n",
        "            [self.vocab_size], dtype=tf.float32, name='sm_b')\n",
        "\n",
        "        logits_raw = tf.matmul(stacked_rnn_outputs, softmax_w) + softmax_b\n",
        "        logits = tf.reshape(logits_raw, [-1, self.steps, self.vocab_size])\n",
        "\n",
        "        output_softmax = tf.nn.softmax(logits)\n",
        "\n",
        "        self.temperature = tf.placeholder(tf.float32)\n",
        "        self.output_softmax_temp = tf.nn.softmax(\n",
        "            tf.div(logits, self.temperature))\n",
        "\n",
        "        softmax_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
        "            labels=onehot_y, logits=logits)\n",
        "\n",
        "        self.cross_entropy = tf.reduce_mean(softmax_entropy)\n",
        "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
        "\n",
        "        self.training_op = optimizer.minimize(self.cross_entropy)\n",
        "\n",
        "        # Clipping isn't necessary, even for really deep networks:\n",
        "        # grads = optimizer.compute_gradients(self.cross_entropy)\n",
        "        # minclip = -1.0 * self.clip\n",
        "        # capped_grads = [(tf.clip_by_value(grad, minclip, self.clip), var) \n",
        "        #     for grad, var in grads]\n",
        "        # self.training_op = optimizer.apply_gradients(capped_grads)\n",
        "\n",
        "        self.prediction = tf.cast(tf.argmax(output_softmax, -1), tf.int32)\n",
        "        correct_prediction = tf.equal(self.y, self.prediction)\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "        error = 1.0 - self.accuracy\n",
        "\n",
        "        # Tensorboard\n",
        "        tf.summary.scalar(\"cross-entropy\", self.cross_entropy)\n",
        "        tf.summary.scalar(\"error\", error)\n",
        "        self.summary_merged = tf.summary.merge_all()\n",
        "\n",
        "        # Init\n",
        "        self.init = tf.global_variables_initializer()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "48X8-lFFQtf3"
      },
      "source": [
        "## 3. Parameters for model and training\n",
        "\n",
        "The library description `libdesc` contains a list in `lib` with local filenames of text-files or http, https URLs pointing to text files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "FhMGIIPEQtf3",
        "colab": {}
      },
      "source": [
        "\n",
        "# Model parameter:\n",
        "modelParamsShakespeare = {\n",
        "    \"model_name\": \"shakespeare\",\n",
        "    \"logdir\": \"tensorlog/shakespeare\",\n",
        "    \"checkpoint\": \"shakespeare.ckpt\",\n",
        "    \"vocab_size\": len(textlib.i2c),\n",
        "    \"neurons\": 512,\n",
        "    \"layers\": 4,\n",
        "    \"learning_rate\": 4.e-4,\n",
        "    \"steps\": 128,\n",
        "}\n",
        "\n",
        "# Look for optional json description of a library:\n",
        "if os.path.exists('bk/lib-phil-deen.json'):\n",
        "    with open('bk/lib-phil-deen.json') as data_file:    \n",
        "        libdescphil = json.load(data_file)\n",
        "        textlib = TextLibrary(libdescphil[\"lib\"])\n",
        "        modelParamsPhil = {\n",
        "            \"model_name\": \"phil\",\n",
        "            \"logdir\": \"tensorlog/phil\",\n",
        "            \"checkpoint\": \"phil.ckpt\",\n",
        "            \"vocab_size\": len(textlib.i2c),\n",
        "            \"neurons\": 256,\n",
        "            \"layers\": 8,\n",
        "            \"learning_rate\": 1.e-3,\n",
        "            \"steps\": 128,\n",
        "        }\n",
        "        model = TensorPoetModel(modelParamsPhil)\n",
        "else:\n",
        "    model = TensorPoetModel(modelParamsShakespeare)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "5ciDOIUiQtf6",
        "colab": {}
      },
      "source": [
        "# Training Parameter:\n",
        "\n",
        "trainParams = {\n",
        "    \"max_iter\": 1000000,\n",
        "    \"restoreCheckpoints\": False,\n",
        "    \"generateDuringTraining\": True,\n",
        "    \"generated_text_size\": 200,\n",
        "    \"verbose\": True,\n",
        "    \"statusEveryNIter\": 500,\n",
        "    \"saveEveryNIter\": 500,\n",
        "    \"batch_size\": 128,\n",
        "}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zOlVTPtFQtf7"
      },
      "source": [
        "## 4. The actual training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3x0U5-0dQtf9",
        "colab": {}
      },
      "source": [
        "# Run training:\n",
        "with tf.Session() as sess:\n",
        "    batch_size = trainParams[\"batch_size\"]\n",
        "    epl = len(textlib.data) / (batch_size * model.steps)\n",
        "    model.init.run()\n",
        "    tflogdir = model.logdir\n",
        "    tflogdir = os.path.realpath(tflogdir)\n",
        "    if not os.path.exists(tflogdir):\n",
        "        os.makedirs(tflogdir)\n",
        "    print(\"Tensorboard: 'tensorboard --logdir {}'\".format(tflogdir))\n",
        "    train_writer = tf.summary.FileWriter(tflogdir, sess.graph)\n",
        "    train_writer.add_graph(sess.graph)\n",
        "    # vl=tf.trainable_variables()\n",
        "    # print(vl)\n",
        "    saver = tf.train.Saver()\n",
        "    checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "    # FFR: tf.train.export_meta_graph(filename=None, meta_info_def=None, graph_def=None,\n",
        "    # saver_def=None, collection_list=None, as_text=False, graph=None, export_scope=None,\n",
        "    # clear_devices=False, **kwargs)\n",
        "    start_iter = 0\n",
        "    if trainParams[\"restoreCheckpoints\"]:\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir,\n",
        "                                              latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "    av_batch_time=0.0\n",
        "    for iteration in range(start_iter, trainParams[\"max_iter\"]):\n",
        "        # Train with batches from the text library:\n",
        "        t1=time.time()\n",
        "        X_batch, y_batch = textlib.get_random_sample_batch(\n",
        "            batch_size, model.steps)\n",
        "        i_state = sess.run([model.init_state_0], feed_dict={model.X: X_batch})\n",
        "        i_state, _ = sess.run([model.final_state, model.training_op],\n",
        "                              feed_dict={model.X: X_batch, model.y: y_batch,\n",
        "                                         model.init_state: i_state})\n",
        "        t2=time.time()\n",
        "        if av_batch_time==0.0:\n",
        "            av_batch_time=(t2-t1)*1000.0\n",
        "        else:\n",
        "            av_batch_time=(av_batch_time*5.0+(t2-t1)*1000.0)/6.0\n",
        "        \n",
        "        # Output training statistics every 100 iterations:\n",
        "        if iteration % 200 == 0:\n",
        "            ce, accuracy, prediction, summary = sess.run([model.cross_entropy,\n",
        "                                                          model.accuracy, model.prediction,\n",
        "                                                          model.summary_merged],\n",
        "                                                         feed_dict={model.X: X_batch, model.y: y_batch})\n",
        "            train_writer.add_summary(summary, iteration)\n",
        "            ep = iteration / epl\n",
        "            print(\"Epoch: {0:.2f}, iter: {1:d}, cross-entropy: {2:.3f}, accuracy: {3:.5f} time per batch: {4:.5f}ms\".format(\n",
        "                ep, iteration, ce, accuracy, av_batch_time))\n",
        "            if trainParams[\"verbose\"]:\n",
        "                for ind in range(1):  # model.batch_size):\n",
        "                    ys = textlib.decode(y_batch[ind]).replace('\\n', ' | ')\n",
        "                    yps = textlib.decode(prediction[ind]).replace('\\n', ' | ')\n",
        "                    print(\"   y:\", ys)\n",
        "                    print(\"  yp:\", yps)\n",
        "\n",
        "        # Generate sample texts for different temperature every ..NIter iterations:\n",
        "        if (iteration+1) % trainParams[\"statusEveryNIter\"] == 0:\n",
        "\n",
        "            # Save training data\n",
        "            # print(\"S>\")\n",
        "            saver.save(sess, checkpoint_file, global_step=iteration+1)\n",
        "            # print(\"S<\")\n",
        "\n",
        "            if trainParams[\"generateDuringTraining\"]:\n",
        "                # Generate sample\n",
        "                for t in range(4, 11, 3):\n",
        "                    temp = float(t) / 10.0\n",
        "                    xs = ' ' * model.steps\n",
        "                    xso = ''\n",
        "                    doini = True\n",
        "                    for i in range(trainParams[\"generated_text_size\"]):\n",
        "                        X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                        if doini:\n",
        "                            doini = False\n",
        "                            g_state = sess.run(\n",
        "                                [model.init_state_0], feed_dict={model.X: X_new})\n",
        "\n",
        "                        g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                                   feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                              model.temperature: temp})\n",
        "                        inds = list(range(model.vocab_size))\n",
        "                        ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                        nc = textlib.i2c[ind]\n",
        "                        xso += nc\n",
        "                        xs = xs[1:]+nc\n",
        "\n",
        "                    print(\"----------------- temperature =\",\n",
        "                          temp, \"----------------------\")\n",
        "                    # print(xso)\n",
        "                    # 20: minimum quote size detected.\n",
        "                    textlib.source_highlight(xso, 20)\n",
        "                print(\"---------------------------------------\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "sScszXSqQtf_"
      },
      "source": [
        "## 5. Generation of text from the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1P6TrXpkQtf_",
        "colab": {}
      },
      "source": [
        "# Generating text using the model data generated during training.\n",
        "def ghostWriter(textsize, temperature=1.0):\n",
        "    xso = None\n",
        "    with tf.Session() as sess:\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(tflogdir))\n",
        "            return None\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return None\n",
        "\n",
        "        xs = ' ' * model.steps\n",
        "        xso = ''\n",
        "        doini = True\n",
        "        for i in range(textsize):\n",
        "            X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "            if doini:\n",
        "                doini = False\n",
        "                g_state = sess.run([model.init_state_0],\n",
        "                                   feed_dict={model.X: X_new})\n",
        "            g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                       feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                  model.temperature: temperature})\n",
        "            inds = list(range(model.vocab_size))\n",
        "            ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "            nc = textlib.i2c[ind]\n",
        "            xso += nc\n",
        "            xs = xs[1:]+nc\n",
        "    return(xso)\n",
        "\n",
        "\n",
        "def detectPlagiarism(generatedtext, textlibrary, minQuoteLength=10):\n",
        "    textlibrary.source_highlight(generatedtext, minQuoteLength)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "o5S9gO2iQtgD",
        "colab": {}
      },
      "source": [
        "tgen=ghostWriter(500)\n",
        "detectPlagiarism(tgen, textlib)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WW6LPdlhQtgF"
      },
      "source": [
        "## 6. A dialog with the trained model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uxDNYZiEQtgF",
        "colab": {}
      },
      "source": [
        "# Do a dialog with the recursive neural net trained above:\n",
        "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
        "# maxAnswerSize=512, temperature=1.0):\n",
        "\n",
        "\n",
        "def doDialog():\n",
        "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
        "    temperature = 0.6\n",
        "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
        "    # look for number of maxEndPrompts until answer is finished.\n",
        "    maxEndPrompts = 4\n",
        "    maxAnswerSize = 2048  # Maximum length of the answer\n",
        "    minAnswerSize = 64  # Minimum length of the answer\n",
        "\n",
        "    with tf.Session() as sess:\n",
        "        print(\"Please enter some dialog.\")\n",
        "        print(\"The net will answer according to your input.\")\n",
        "        print(\"'bye' for end,\")\n",
        "        print(\"'reset' to reset the conversation context,\")\n",
        "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
        "        print(\"    to change character of the dialog.\")\n",
        "        print(\"    Current temperature={}.\".format(temperature))\n",
        "        print()\n",
        "        xso = None\n",
        "        bye = False\n",
        "        model.init.run()\n",
        "\n",
        "        tflogdir = os.path.realpath(model.logdir)\n",
        "        if not os.path.exists(tflogdir):\n",
        "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
        "                trainParams[\"logdir\"]))\n",
        "            return\n",
        "\n",
        "        # Used for saving the training parameters periodically\n",
        "        saver = tf.train.Saver()\n",
        "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
        "\n",
        "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
        "        if lastSave is not None:\n",
        "            pt = lastSave.rfind('-')\n",
        "            if pt != -1:\n",
        "                pt += 1\n",
        "                start_iter = int(lastSave[pt:])\n",
        "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
        "            saver.restore(sess, lastSave)\n",
        "        else:\n",
        "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
        "            return\n",
        "\n",
        "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "        doini = True\n",
        "\n",
        "        bye = False\n",
        "        while not bye:\n",
        "            print(\"> \", end=\"\")\n",
        "            prompt = input()\n",
        "            if prompt == 'bye':\n",
        "                bye = True\n",
        "                print(\"Good bye!\")\n",
        "                continue\n",
        "            if prompt == 'reset':\n",
        "                doini = True\n",
        "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
        "                print(\"(conversation context marked for reset)\")\n",
        "                continue\n",
        "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
        "                t = float(prompt[len(\"temperature=\"):])\n",
        "                if t > 0.05 and t < 1.4:\n",
        "                    temperature = t\n",
        "                    print(\"(generator temperature now {})\".format(t))\n",
        "                    print()\n",
        "                    continue\n",
        "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
        "                continue\n",
        "            xs = ' ' * model.steps\n",
        "            xso = ''\n",
        "            for rep in range(1):\n",
        "                for i in range(len(prompt)):\n",
        "                    xs = xs[1:]+prompt[i]\n",
        "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                    if doini:\n",
        "                        doini = False\n",
        "                        g_state = sess.run(\n",
        "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
        "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                          model.temperature: temperature})\n",
        "            ans = 0\n",
        "            numEndPrompts = 0\n",
        "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
        "\n",
        "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
        "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
        "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
        "                                                      model.temperature: temperature})\n",
        "                inds = list(range(model.vocab_size))\n",
        "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
        "                nc = textlib.i2c[ind]\n",
        "                if nc == endPrompt:\n",
        "                    numEndPrompts += 1\n",
        "                xso += nc\n",
        "                xs = xs[1:]+nc\n",
        "                ans += 1\n",
        "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
        "            textlib.source_highlight(xso, 13)\n",
        "    return"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0JEPK2WIQtgI",
        "colab": {}
      },
      "source": [
        "# Talk to the net!\n",
        "doDialog()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
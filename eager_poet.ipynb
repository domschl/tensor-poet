{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/domschl/tensor-poet/blob/master/eager_poet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fEXNOWhCEAPk"
   },
   "source": [
    "# Eager Tensor Poet (Tensorflow 2.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wM8iyPzWuc8G"
   },
   "source": [
    "### Only execute next block, if you want to test with different TF runtime.\n",
    "\n",
    "Remember to restart runtime after installing new software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7hgtQU725ap"
   },
   "outputs": [],
   "source": [
    "# %pip install -U tensorflow-gpu tensorflow-addons tensorflow-federated tensorboard # tf-nightly-gpu  # Currently not useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "laU7LRAguts6"
   },
   "source": [
    "### Select TF version (in colab)\n",
    "\n",
    "This should be the default starting point for working with the standard environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "jtpy59Yq-Qfz",
    "outputId": "ad351f44-e061-4468-a288-01cc93cc7240"
   },
   "outputs": [],
   "source": [
    "## Import TensorFlow\n",
    "## from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "try:\n",
    "    ## %tensorflow_version only exists in Colab.\n",
    "    %tensorflow_version 2.x\n",
    "except Exception:\n",
    "    ## Non colab people need to make sure that tf 2 is installed.\n",
    "    pass\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VmWbteSFQtfq"
   },
   "source": [
    "## Preliminary\n",
    "\n",
    "**THIS IS WORK IN PROGRESS**\n",
    "\n",
    "A tensorflow deep LSTM model for text generation\n",
    "\n",
    "This code can use either CPU, GPU or TPU when running on Google Colab.\n",
    "\n",
    "Select the corresponding runtime (menu: Runtime / Change runtime type)\n",
    "\n",
    "Note: TPU support is 'kind of' working. The secret was moving the Embedding layer to CPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rum30R2JzeEl"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EgLLjG4yQtft"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import time\n",
    "import datetime\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "try:\n",
    "    from urllib.request import urlopen  # Py3\n",
    "except:\n",
    "    print(\"This notebook requires Python 3.\")\n",
    "try:\n",
    "    import pathlib\n",
    "except:\n",
    "    print(\"At least python 3.5 is needed.\")\n",
    "    \n",
    "try: # Colab instance?\n",
    "    from google.colab import drive\n",
    "except: # Not? ignore.\n",
    "    pass\n",
    "\n",
    "from IPython.core.display import display, HTML"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfZg31sMEAP1"
   },
   "source": [
    "## 0. Check system\n",
    "\n",
    "### Tensorflow api version check\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "llPw84PkEAP2",
    "outputId": "0265954d-27be-4b2b-9c4a-653322d2aff2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow api v2 active: 2.1.0\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if 'api.v2' in tf.version.__name__:\n",
    "        print(f\"Tensorflow api v2 active: {tf.__version__}\")\n",
    "    else:\n",
    "        print(\"Tensorflow api v2 not found. This will not work.\")\n",
    "except:\n",
    "    print(\"Failed to check for Tensorflow api v2. This will not work.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P08FdKKnEAP6"
   },
   "source": [
    "### GPU/TPU check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook can either run on a local jupyter server, or on google cloud.\n",
    "If a GPU/TPU is available, it will be used for training.\n",
    "\n",
    "By default snapshots of the trained net are stored locally for jupyter instances, and on user's google drive for Google Colab instances. The snapshots allow the restart of training or inference at any time, e.g. after the Colab session was terminated.\n",
    "\n",
    "Similarily, the text corpora that are used for training, can be cached on drive or locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define where snapshots of training data are stored:\n",
    "colab_google_drive_snapshots=True\n",
    "\n",
    "# Define if training data (the texts downloaded from internet) are cached:\n",
    "colab_google_drive_data_cache=True  # In colab mode cache to google drive\n",
    "local_jupyter_data_cache=True       # In local jupyter mode cache to local path\n",
    "\n",
    "is_colab_notebook = 'google.colab' in sys.modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "gjWwUUfuEAP7",
    "outputId": "f138c532-bf85-4bdf-c04d-bdde5ae2d964"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No TPU available\n",
      "CPU -> [LogicalDevice(name='/device:CPU:0', device_type='CPU')]\n",
      "GPU -> [LogicalDevice(name='/device:GPU:0', device_type='GPU')]\n",
      "TPU -> []\n",
      "GPUs: ['/device:GPU:0', '/device:XLA_GPU:0']\n",
      "[('/device:CPU:0', ''), ('/device:XLA_CPU:0', 'device: XLA_CPU device'), ('/device:GPU:0', 'device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:01:00.0, compute capability: 6.1'), ('/device:XLA_GPU:0', 'device: XLA_GPU device')]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "use_tpu = False\n",
    "use_gpu = False\n",
    "use_eager = True\n",
    "\n",
    "try:\n",
    "    TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
    "    use_tpu = True\n",
    "    tpu_is_init = False\n",
    "    tf.config.experimental_connect_to_host(TPU_ADDRESS)\n",
    "    print(\"TPU available at {}\".format(TPU_ADDRESS))\n",
    "except:\n",
    "    print(\"No TPU available\")\n",
    "\n",
    "for hw in [\"CPU\", \"GPU\", \"TPU\"]:\n",
    "    hwlist=tf.config.experimental.list_logical_devices(hw)\n",
    "    print(\"{} -> {}\".format(hw,hwlist))\n",
    "\n",
    "\n",
    "if use_tpu is False:\n",
    "    def get_available_devs_of_type(type):\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [x.name for x in local_device_protos if type in x.name]\n",
    "\n",
    "    def get_dev_desc():\n",
    "        local_device_protos = device_lib.list_local_devices()\n",
    "        return [(x.name, x.physical_device_desc) for x in local_device_protos]\n",
    "\n",
    "    def get_available_gpus():\n",
    "        return get_available_devs_of_type('GPU')\n",
    "\n",
    "    dl = get_available_gpus()\n",
    "    if len(dl)==0:\n",
    "        print(\"WARNING: You have neither TPU nor GPU, this is going to be very slow!\")\n",
    "        if is_colab_notebook is True:\n",
    "            print(\"         Hint: In Colab Runtime / Set runtime type, set runtime type to GPU or TPU.\")\n",
    "        print(get_available_devs_of_type(''))\n",
    "    else:\n",
    "        use_gpu = True\n",
    "        print(f\"GPUs: {dl}\")\n",
    "        print(get_dev_desc())\n",
    "else:\n",
    "    use_eager = False  # Eager mode cannot be used with TPUs.\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    print(\"DISABLING eager execution because TPUs do not support dynamic execution.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_snapshots:\n",
    "        mountpoint='/content/drive'\n",
    "        root_path='/content/drive/My Drive'\n",
    "        if not os.path.exists(root_path):\n",
    "            drive.mount(mountpoint)\n",
    "        if not os.path.exists(root_path):\n",
    "            print(\"Something went wrong with Google Drive access. Cannot save snapshots to GD.\")\n",
    "            colab_google_drive_snapshots=False\n",
    "    else:\n",
    "        print(\"Since google drive snapshots are not active, training data will be lost as soon as the Colab session terminates!\")\n",
    "        print(\"Set `colab_google_drive_snapshots` to `True` to make training data persistent.\")\n",
    "else:\n",
    "    root_path='.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(p, dim):\n",
    "    o=np.zeros(p.shape+(dim,), dtype=np.int32)\n",
    "    for y in range(p.shape[0]):\n",
    "        for x in range(p.shape[1]):\n",
    "            o[y,x,p[y,x]]=1\n",
    "    return o"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AIkcYcEuQtfx"
   },
   "source": [
    "##  1. Text library\n",
    "\n",
    "`TextLibrary` class: text library for training, encoding, batch generation,\n",
    "and formatted source display. It read some books from Project Gutenberg\n",
    "and supports creation of training batches. The output functions support\n",
    "highlighting to allow to compare generated texts with the actual sources\n",
    "to help to identify identical (memorized) parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_dark_mode=False  # Set to false for white background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Pz4xVgaQtfy"
   },
   "outputs": [],
   "source": [
    "class TextLibrary:\n",
    "    def __init__(self, descriptors, text_data_cache_directory=None, max=100000000):\n",
    "        self.descriptors = descriptors\n",
    "        self.data = ''\n",
    "        self.cache_dir=text_data_cache_directory\n",
    "        self.files = []\n",
    "        self.c2i = {}\n",
    "        self.i2c = {}\n",
    "        index = 1\n",
    "        for descriptor, author, title in descriptors:\n",
    "            fd = {}\n",
    "            cache_name=self.get_cache_name(author, title)\n",
    "            if os.path.exists(cache_name):\n",
    "                is_cached=True\n",
    "            else:\n",
    "                is_cached=False\n",
    "            valid=False\n",
    "            if descriptor[:4] == 'http' and is_cached is False:\n",
    "                try:\n",
    "                    print(f\"Downloading {cache_name}\")\n",
    "                    dat = urlopen(descriptor).read().decode('utf-8')\n",
    "                    if dat[0]=='\\ufeff':  # Ignore BOM\n",
    "                        dat=dat[1:]\n",
    "                    dat=dat.replace('\\r', '')  # get rid of pesky LFs \n",
    "                    self.data += dat\n",
    "                    fd[\"title\"] = title\n",
    "                    fd[\"author\"] = author\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    valid=True\n",
    "                    self.files.append(fd)\n",
    "                except Exception as e:\n",
    "                    print(f\"Can't download {descriptor}: {e}\")\n",
    "            else:\n",
    "                fd[\"title\"] = title\n",
    "                fd[\"author\"] = author\n",
    "                try:\n",
    "                    if is_cached is True:\n",
    "                        print(f\"Reading {cache_name} from cache\")\n",
    "                        f = open(cache_name)\n",
    "                    else:    \n",
    "                        f = open(descriptor)\n",
    "                    dat = f.read(max)\n",
    "                    self.data += dat\n",
    "                    fd[\"data\"] = dat\n",
    "                    fd[\"index\"] = index\n",
    "                    index += 1\n",
    "                    self.files.append(fd)\n",
    "                    f.close()\n",
    "                    valid=True\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: Cannot read: {filename}: {e}\")\n",
    "            if valid is True and is_cached is False and self.cache_dir is not None:\n",
    "                try:\n",
    "                    print(f\"Caching {cache_name}\")\n",
    "                    f = open(cache_name, 'w')\n",
    "                    f.write(dat)\n",
    "                    f.close()\n",
    "                except Exception as e:\n",
    "                    print(f\"ERROR: failed to save cache {cache_name}: {e}\")\n",
    "                \n",
    "        ind = 0\n",
    "        for c in self.data:  # sets are not deterministic\n",
    "            if c not in self.c2i:\n",
    "                self.c2i[c] = ind\n",
    "                self.i2c[ind] = c\n",
    "                ind += 1\n",
    "        self.ptr = 0\n",
    "\n",
    "    def get_cache_name(self, author, title):\n",
    "        if self.cache_dir is None:\n",
    "            return None\n",
    "        cname=f\"{author} - {title}.txt\"\n",
    "        cache_filepath=os.path.join(self.cache_dir , cname)\n",
    "        return cache_filepath\n",
    "        \n",
    "    def display_colored_html(self, textlist, dark_mode=False, pre='', post=''):\n",
    "        bgcolorsWht = ['#d4e6e1', '#d8daef', '#ebdef0', '#eadbd8', '#e2d7d5', '#edebd0',\n",
    "                    '#ecf3cf', '#d4efdf', '#d0ece7', '#d6eaf8', '#d4e6f1', '#d6dbdf',\n",
    "                    '#f6ddcc', '#fae5d3', '#fdebd0', '#e5e8e8', '#eaeded', '#A9CCE3']\n",
    "        bgcolorsDrk = ['#342621','#483a2f', '#3b4e20', '#2a3b48', '#324745', '#3d3b30',\n",
    "                    '#3c235f', '#443f4f', '#403c37', '#463a28', '#443621', '#364b5f',\n",
    "                    '#264d4c', '#2a3553', '#3d2b40', '#354838', '#3a3d4d', '#594C23']\n",
    "        if dark_mode is False:\n",
    "            bgcolors=bgcolorsWht\n",
    "        else:\n",
    "            bgcolors=bgcolorsDrk\n",
    "        out = ''\n",
    "        for txt, ind in textlist:\n",
    "            txt = txt.replace('\\n', '<br>')\n",
    "            if ind == 0:\n",
    "                out += txt\n",
    "            else:\n",
    "                out += \"<span style=\\\"background-color:\"+bgcolors[ind % 16]+\";\\\">\" + \\\n",
    "                       txt + \"</span>\"+\"<sup>[\" + str(ind) + \"]</sup>\"\n",
    "        display(HTML(pre+out+post))\n",
    "\n",
    "    def source_highlight(self, txt, minQuoteSize=10, dark_mode=False):\n",
    "        tx = txt\n",
    "        out = []\n",
    "        qts = []\n",
    "        txsrc = [(\"Sources: \", 0)]\n",
    "        sc = False\n",
    "        noquote = ''\n",
    "        while len(tx) > 0:  # search all library files for quote 'txt'\n",
    "            mxQ = 0\n",
    "            mxI = 0\n",
    "            mxN = ''\n",
    "            found = False\n",
    "            for f in self.files:  # find longest quote in all texts\n",
    "                p = minQuoteSize\n",
    "                if p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                    p = minQuoteSize + 1\n",
    "                    while p <= len(tx) and tx[:p] in f[\"data\"]:\n",
    "                        p += 1\n",
    "                    if p-1 > mxQ:\n",
    "                        mxQ = p-1\n",
    "                        mxI = f[\"index\"]\n",
    "                        mxN = f\"{f['author']}: {f['title']}\"\n",
    "                        found = True\n",
    "            if found:  # save longest quote for colorizing\n",
    "                if len(noquote) > 0:\n",
    "                    out.append((noquote, 0))\n",
    "                    noquote = ''\n",
    "                out.append((tx[:mxQ], mxI))\n",
    "                tx = tx[mxQ:]\n",
    "                if mxI not in qts:  # create a new reference, if first occurence\n",
    "                    qts.append(mxI)\n",
    "                    if sc:\n",
    "                        txsrc.append((\", \", 0))\n",
    "                    sc = True\n",
    "                    txsrc.append((mxN, mxI))\n",
    "            else:\n",
    "                noquote += tx[0]\n",
    "                tx = tx[1:]\n",
    "        if len(noquote) > 0:\n",
    "            out.append((noquote, 0))\n",
    "            noquote = ''\n",
    "        self.display_colored_html(out, dark_mode=dark_mode)\n",
    "        if len(qts) > 0:  # print references, if there is at least one source\n",
    "            self.display_colored_html(txsrc, dark_mode=dark_mode, pre=\"<small><p style=\\\"text-align:right;\\\">\",\n",
    "                                     post=\"</p></small>\")\n",
    "\n",
    "    def get_slice(self, length):\n",
    "        if (self.ptr + length >= len(self.data)):\n",
    "            self.ptr = 0\n",
    "        if self.ptr == 0:\n",
    "            rst = True\n",
    "        else:\n",
    "            rst = False\n",
    "        sl = self.data[self.ptr:self.ptr+length]\n",
    "        self.ptr += length\n",
    "        return sl, rst\n",
    "\n",
    "    def decode(self, ar):\n",
    "        return ''.join([self.i2c[ic] for ic in ar])\n",
    "\n",
    "    def get_random_slice(self, length):\n",
    "        p = random.randrange(0, len(self.data)-length)\n",
    "        sl = self.data[p:p+length]\n",
    "        return sl\n",
    "\n",
    "    def get_slice_array(self, length):\n",
    "        ar = np.array([c for c in self.get_slice(length)[0]])\n",
    "        return ar\n",
    "\n",
    "    def get_encoded_slice(self, length):\n",
    "        s, rst = self.get_slice(length)\n",
    "        X = [self.c2i[c] for c in s]\n",
    "        return X\n",
    "        \n",
    "    def get_encoded_slice_array(self, length):\n",
    "        return np.array(self.get_encoded_slice(length))\n",
    "\n",
    "    def get_sample(self, length):\n",
    "        s, rst = self.get_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y, rst)\n",
    "\n",
    "    def get_random_sample(self, length):\n",
    "        s = self.get_random_slice(length+1)\n",
    "        X = [self.c2i[c] for c in s[:-1]]\n",
    "        y = [self.c2i[c] for c in s[1:]]\n",
    "        return (X, y)\n",
    "\n",
    "    def get_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi, rst = self.get_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return smpX, smpy, rst\n",
    "\n",
    "    def get_random_sample_batch(self, batch_size, length):\n",
    "        smpX = []\n",
    "        smpy = []\n",
    "        for i in range(batch_size):\n",
    "            Xi, yi = self.get_random_sample(length)\n",
    "            smpX.append(Xi)\n",
    "            smpy.append(yi)\n",
    "        return np.array(smpX), np.array(smpy)\n",
    "    \n",
    "    def get_random_onehot_sample_batch(self, batch_size, length):\n",
    "        X, y = self.get_random_sample_batch(batch_size, length)\n",
    "        # xoh = one_hot(X,len(self.i2c))\n",
    "        xoh = tf.keras.backend.one_hot(X, len(self.i2c))\n",
    "        return xoh, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ucJsgqNgEAQC"
   },
   "source": [
    "### Data sources\n",
    "\n",
    "Data sources can either be files from local filesystem, or for colab notebooks from google drive, or http(s) links.\n",
    "\n",
    "The name given will be use as directory name for both snapshots and model data caches.\n",
    "\n",
    "Each entry in the lib array contains of:\n",
    "\n",
    "1. a local filename or https(s) link,\n",
    "2. an Author's name\n",
    "3. a title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tZDo-bIPEAQD"
   },
   "outputs": [],
   "source": [
    "libdesc = {\n",
    "    \"name\": \"Women-Writers\",\n",
    "    \"description\": \"A collection of works of Woolf, Austen and Brontë\",\n",
    "    \"lib\": [\n",
    "        # ('data/tiny-shakespeare.txt', 'William Shakespeare', 'Some parts'),   # local file example\n",
    "        # ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/0/100/100-0.txt', 'Shakespeare', 'Collected Works'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/3/7/4/3/37431/37431.txt', 'Jane Austen', 'Pride and Prejudice'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/7/6/768/768.txt', 'Emily Brontë', 'Wuthering Heights'),         \n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/4/144/144.txt', 'Virginia Wolf', 'Voyage out'),\n",
    "        ('http://www.mirrorservice.org/sites/ftp.ibiblio.org/pub/docs/books/gutenberg/1/5/158/158.txt', 'Jane Austen', 'Emma')\n",
    "    ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "if is_colab_notebook:\n",
    "    if colab_google_drive_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"Colab Notebooks/{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "else:\n",
    "    if local_jupyter_data_cache is True:\n",
    "        data_cache_path=os.path.join(root_path,f\"{libdesc['name']}/Data\")\n",
    "    else:\n",
    "        data_cache_path=None\n",
    "\n",
    "if data_cache_path is not None:\n",
    "    pathlib.Path(data_cache_path).mkdir(parents=True, exist_ok=True)\n",
    "    if not os.path.exists(data_cache_path):\n",
    "        print(\"ERROR, the cache directory does not exist. This will fail.\")\n",
    "    else:\n",
    "        with open(os.path.join(data_cache_path,'libdesc.json'),'w') as f:\n",
    "            json.dump(libdesc,f,indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading ./Women-Writers/Data/Jane Austen - Pride and Prejudice.txt from cache\n",
      "Reading ./Women-Writers/Data/Emily Brontë - Wuthering Heights.txt from cache\n",
      "Reading ./Women-Writers/Data/Virginia Wolf - Voyage out.txt from cache\n",
      "Reading ./Women-Writers/Data/Jane Austen - Emma.txt from cache\n"
     ]
    }
   ],
   "source": [
    "textlib = TextLibrary(libdesc[\"lib\"], text_data_cache_directory=data_cache_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "30hi0UPtEAQG"
   },
   "source": [
    "## 2. Use tf.data for texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jY3hUuhQYzdT"
   },
   "outputs": [],
   "source": [
    "SEQUENCE_LEN = 60\n",
    "if use_tpu is True:\n",
    "    BATCH_SIZE=256\n",
    "    use_tpu_model_for_tpu=True\n",
    "    STATEFUL=False\n",
    "else:\n",
    "    BATCH_SIZE = 256\n",
    "    STATEFUL = True\n",
    "LSTM_UNITS = 256\n",
    "EMBEDDING_DIM = 64 # 120\n",
    "LSTM_LAYERS = 4\n",
    "NUM_BATCHES=32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EeB7jugCV4lI"
   },
   "outputs": [],
   "source": [
    "dx=[]\n",
    "dy=[]\n",
    "for i in range(NUM_BATCHES):\n",
    "    x,y=textlib.get_random_onehot_sample_batch(BATCH_SIZE,SEQUENCE_LEN)\n",
    "    dx.append(x)\n",
    "    dy.append(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "BATCH_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3whYiFuwS8q4"
   },
   "outputs": [],
   "source": [
    "data_xy=(dx,dy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DCy7WmQyS9T-"
   },
   "outputs": [],
   "source": [
    "textlib_dataset=tf.data.Dataset.from_tensor_slices(data_xy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "boow8wR7sLwi",
    "outputId": "0dbcf7cd-d0da-455a-8755-a122bd850ec9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 277,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shuffle_buffer=10000\n",
    "dataset=textlib_dataset.shuffle(shuffle_buffer)  \n",
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BLtDLPjGEAQi"
   },
   "outputs": [],
   "source": [
    "def build_model(vocab_size, embedding_dim, steps, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    model = tf.keras.Sequential([\n",
    "        # tf.keras.layers.Embedding(vocab_size, embedding_dim,\n",
    "        #                          batch_input_shape=[batch_size, None]),\n",
    "        # tf.keras.layers.Flatten(),\n",
    "        tf.keras.layers.LSTM(lstm_units,\n",
    "                            # input_shape=(timesteps, data_dim)\n",
    "                            batch_input_shape=[batch_size, None, vocab_size],\n",
    "                            return_sequences=True,\n",
    "                            stateful=stateful,\n",
    "                            recurrent_initializer='glorot_uniform'),\n",
    "        # *[tf.keras.layers.LSTM(lstm_units,\n",
    "        #                     return_sequences=True,\n",
    "        #                     stateful=stateful,\n",
    "        #                     recurrent_initializer='glorot_uniform') for _ in range(lstm_layers-1)],\n",
    "        tf.keras.layers.Dense(vocab_size)\n",
    "        ])\n",
    "    return model\n",
    "\n",
    "def build_tpu_model(vocab_size, embedding_dim, lstm_units, lstm_layers, batch_size, stateful=True):\n",
    "    print(\"NOT ADAPTED!\")\n",
    "    with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):\n",
    "        embedded = tf.keras.layers.Embedding(vocab_size, embedding_dim, embeddings_initializer='uniform', batch_input_shape=[batch_size, SEQUENCE_LEN])\n",
    "    with tpu_strategy.scope():\n",
    "        lstm = [tf.keras.layers.LSTM(lstm_units,\n",
    "                        return_sequences=True,\n",
    "                        stateful=stateful,\n",
    "                        recurrent_initializer='glorot_uniform', unroll=True) for _ in range(lstm_layers)]\n",
    "#     tf.keras.layers.LSTM(lstm_units,\n",
    "#                          return_sequences=True,\n",
    "#                          stateful=stateful,\n",
    "#                          # recurrent_initializer='glorot_uniform',\n",
    "#                         unroll=True)\n",
    "    dense = tf.keras.layers.Dense(vocab_size)\n",
    "    \n",
    "    model = tf.keras.Sequential([\n",
    "        embedded,\n",
    "        *lstm,\n",
    "        dense\n",
    "        ])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQIxWHMGNKHk"
   },
   "outputs": [],
   "source": [
    "if use_tpu:\n",
    "    print(TPU_ADDRESS)\n",
    "    os.environ['COLAB_TPU_ADDR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NMsaykJjEAQl"
   },
   "outputs": [],
   "source": [
    "if use_tpu is True and not tpu_is_init:\n",
    "    cluster_resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu=TPU_ADDRESS)\n",
    "    tf.config.experimental_connect_to_cluster(cluster_resolver) # host(cluster_resolver.master())\n",
    "    tf.tpu.experimental.initialize_tpu_system(cluster_resolver)\n",
    "    tpu_strategy = tf.distribute.experimental.TPUStrategy(cluster_resolver)    \n",
    "    tpu_is_init=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "nf-NHZ326NqJ",
    "outputId": "7269c111-c6b2-44f7-af26-30660d727c27"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "non-tpu mode\n"
     ]
    }
   ],
   "source": [
    "if use_tpu is True:\n",
    "    if use_tpu_model_for_tpu is True:\n",
    "        print(\"tpu, simple model\")\n",
    "        # with tpu_strategy.scope():\n",
    "        model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          embedding_dim=EMBEDDING_DIM,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=BATCH_SIZE,\n",
    "          stateful=STATEFUL)\n",
    "    else:\n",
    "        print(\"tpu, default model\")\n",
    "        with tpu_strategy.scope():\n",
    "            model = build_model(\n",
    "              vocab_size = len(textlib.i2c),\n",
    "              embedding_dim=EMBEDDING_DIM,\n",
    "              lstm_units=LSTM_UNITS,\n",
    "              lstm_layers=LSTM_LAYERS,\n",
    "              batch_size=BATCH_SIZE,\n",
    "              stateful=STATEFUL)        \n",
    "else:\n",
    "    print(\"non-tpu mode\")\n",
    "    model = build_model(\n",
    "        vocab_size = len(textlib.i2c),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        stateful=STATEFUL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GQ7inpxLveap"
   },
   "source": [
    "### Some sanity checks of the (untrained!) model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "GhhF2vqmEAQo",
    "outputId": "b90ff4b9-f258-4743-ab24-6b927c651f0c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 60, 88) # (batch_size, sequence_length, vocab_size)\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:  # no sanity for TPU, since eager not supported:\n",
    "    for input_example_batch, target_example_batch in dataset.take(1):\n",
    "        model.reset_states()\n",
    "        example_batch_predictions = model.predict(input_example_batch, batch_size=256)\n",
    "        print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "8vxZF0wOEAQr",
    "outputId": "862f6065-0baa-4ddc-ec19-7850e3f3925f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_42 (LSTM)               (256, None, 256)          353280    \n",
      "_________________________________________________________________\n",
      "dense_18 (Dense)             (256, None, 88)           22616     \n",
      "=================================================================\n",
      "Total params: 375,896\n",
      "Trainable params: 375,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "LYZs8Ss2947k",
    "outputId": "2a6dd3de-5bfa-40ec-8381-51df3804d58d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<TakeDataset shapes: ((256, 60, 88), (256, 60)), types: (tf.float32, tf.int32)>"
      ]
     },
     "execution_count": 335,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fKo54K1lEAQt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[43 32 35 83 69 34 37 35 81  0 72  7 18 31 76 32 50 28 34 80 30 15  1 19\n",
      " 77 85 12 16 63 47 20  5 22 68 28 68 58 70 21 85 66 66 33 22 48 80  5 62\n",
      " 53 59 28 52 78 51 47 19 80 58 22 13]\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:\n",
    "    sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
    "    sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "    print(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ic4RuDZLEAQy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5v-@9Y:-8TqjfmZv4KYQwEhiX{nB)#dr,WKWJ6a{''.,3Qr(IUKCz]#iQJ,b\n"
     ]
    }
   ],
   "source": [
    "if use_eager is True:\n",
    "    print(textlib.decode(sampled_indices))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "soB-Q8YXvndE"
   },
   "source": [
    "### Loss function, optimizer, tensorboard output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "6i-0Y2uYEAQ0",
    "outputId": "fbe02ce3-e9da-4aba-993b-8af2b85de51e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction shape:  (256, 60, 88)  # (batch_size, sequence_length, vocab_size)\n",
      "scalar_loss:       4.484447\n"
     ]
    }
   ],
   "source": [
    "def loss(labels, logits):\n",
    "  return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
    "\n",
    "if use_eager is True:\n",
    "    example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
    "    print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
    "    print(\"scalar_loss:      \", example_batch_loss.numpy().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "py9WnmosEAQ3"
   },
   "outputs": [],
   "source": [
    "opti = tf.keras.optimizers.Adam(lr=0.003, clipvalue=1.0)\n",
    "# opti = tf.keras.optimizers.Adam(clipvalue=0.5)\n",
    "# opti=tf.keras.optimizers.SGD(lr=0.003)\n",
    "\n",
    "def scalar_loss(labels, logits):\n",
    "    bl=loss(labels, logits)\n",
    "    return tf.reduce_mean(bl)\n",
    "\n",
    "model.compile(optimizer=opti, loss=loss, metrics=[scalar_loss])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5SKvObcsEAQ5"
   },
   "outputs": [],
   "source": [
    "# Directory where the checkpoints will be saved\n",
    "checkpoint_dir = './training_checkpoints'\n",
    "# Name of the checkpoint files\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt_{epoch}\")\n",
    "\n",
    "checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_prefix,\n",
    "    save_weights_only=True)\n",
    "\n",
    "logdir = os.path.join(\"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir, update_freq='epoch', histogram_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "colab_type": "code",
    "id": "o0Ew6pgWzeFj",
    "outputId": "efe4a9e3-85f0-4c39-d8ba-fb21ebfab5f4"
   },
   "outputs": [],
   "source": [
    "# broken with latest python: %tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kDFbZcN0vxOB"
   },
   "source": [
    "## The actual training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kh2yUKBoEAQ8"
   },
   "outputs": [],
   "source": [
    "EPOCHS=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 725
    },
    "colab_type": "code",
    "id": "RLbsTmtnEAQ-",
    "outputId": "3fa0782c-824e-4087-ee9f-f18ae2556d00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 32 steps\n",
      "Epoch 1/20\n",
      "32/32 [==============================] - 1s 39ms/step - loss: 3.2671 - scalar_loss: 3.2671\n",
      "Epoch 2/20\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 3.0264 - scalar_loss: 3.0264\n",
      "Epoch 3/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 2.7953 - scalar_loss: 2.7953\n",
      "Epoch 4/20\n",
      "32/32 [==============================] - 1s 21ms/step - loss: 2.5627 - scalar_loss: 2.5627\n",
      "Epoch 5/20\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 2.4305 - scalar_loss: 2.4305\n",
      "Epoch 6/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.3435 - scalar_loss: 2.3435\n",
      "Epoch 7/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.2762 - scalar_loss: 2.2762\n",
      "Epoch 8/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.2255 - scalar_loss: 2.2255\n",
      "Epoch 9/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.1770 - scalar_loss: 2.1770\n",
      "Epoch 10/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.1306 - scalar_loss: 2.1306\n",
      "Epoch 11/20\n",
      "32/32 [==============================] - 1s 19ms/step - loss: 2.0926 - scalar_loss: 2.0926\n",
      "Epoch 12/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 2.0544 - scalar_loss: 2.0544\n",
      "Epoch 13/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 2.0235 - scalar_loss: 2.0235\n",
      "Epoch 14/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 1.9938 - scalar_loss: 1.9938\n",
      "Epoch 15/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.9648 - scalar_loss: 1.9648\n",
      "Epoch 16/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 1.9426 - scalar_loss: 1.9426\n",
      "Epoch 17/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.9187 - scalar_loss: 1.9187\n",
      "Epoch 18/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 1.8965 - scalar_loss: 1.8965\n",
      "Epoch 19/20\n",
      "32/32 [==============================] - 1s 17ms/step - loss: 1.8771 - scalar_loss: 1.8771\n",
      "Epoch 20/20\n",
      "32/32 [==============================] - 1s 18ms/step - loss: 1.8576 - scalar_loss: 1.8576\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback]) #, tensorboard_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rvupep98EARA"
   },
   "outputs": [],
   "source": [
    "# Generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mnMrNH0eEARC"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_20'"
      ]
     },
     "execution_count": 345,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "checkpoint_dir = './training_checkpoints'  # duplicate\n",
    "\n",
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-pvmuA-8aTx3"
   },
   "outputs": [],
   "source": [
    "use_tpu_for_generation=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qA7qRl5EEARE"
   },
   "outputs": [],
   "source": [
    "if not use_tpu_for_generation:\n",
    "    gen_model = build_model(vocab_size = len(textlib.i2c),\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        steps=SEQUENCE_LEN,\n",
    "        lstm_units=LSTM_UNITS,\n",
    "        lstm_layers=LSTM_LAYERS,\n",
    "        batch_size=1)\n",
    "else:\n",
    "    gen_model = build_tpu_model(\n",
    "          vocab_size = len(textlib.i2c),\n",
    "          embedding_dim=EMBEDDING_DIM,\n",
    "          steps=SEQUENCE_LEN,\n",
    "          lstm_units=LSTM_UNITS,\n",
    "          lstm_layers=LSTM_LAYERS,\n",
    "          batch_size=1,\n",
    "          stateful=STATEFUL)  # TPUs can't handle stateful=True, and that's deadly for text generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yFRyOB8we2YA"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./training_checkpoints/ckpt_20'"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.train.latest_checkpoint(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 312
    },
    "colab_type": "code",
    "id": "KCunzXAQKBI6",
    "outputId": "68deeaf2-e1e1-41cd-a400-dce709c5eb4a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f1a9d1adca0>"
      ]
     },
     "execution_count": 349,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_model.load_weights(tf.train.latest_checkpoint(checkpoint_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YJbQqPFEevop"
   },
   "outputs": [],
   "source": [
    "gen_model.build(tf.TensorShape([1, None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 363
    },
    "colab_type": "code",
    "id": "nQ6JhIGxEARF",
    "outputId": "ff411f69-319f-4170-e1d1-325d0d72a650"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_43 (LSTM)               (1, None, 256)            353280    \n",
      "_________________________________________________________________\n",
      "dense_19 (Dense)             (1, None, 88)             22616     \n",
      "=================================================================\n",
      "Total params: 375,896\n",
      "Trainable params: 375,896\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gen_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "geLmNIvzEARH"
   },
   "outputs": [],
   "source": [
    "def generate_text_with_tpu(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string[-SEQUENCE_LEN:]  # Tpus need the whole history of exactly secuence_len chars, not less, not more.\n",
    "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model(input_eval)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      if not use_tpu:\n",
    "          predicted_id=predicted_tensor.numpy()\n",
    "      else:\n",
    "          predicted_id=predicted_tensor.eval()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "      print(\"out:\"+''.join(text_generated))\n",
    "\n",
    "      cutstr=(start_string+''.join(text_generated))[-SEQUENCE_LEN:]  # Restore the entire history if last SEQUENCE_LEN chars, to be \"stateless\"\n",
    "      input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "      input_eval = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  return (start_string + ''.join(text_generated), ids)\n",
    "\n",
    "def generate_text(model, start_string, temp=0.6):\n",
    "  # Evaluation step (generating text using the learned model)\n",
    "\n",
    "  # Number of characters to generate\n",
    "  num_generate = 128\n",
    "\n",
    "  # Converting our start string to numbers (vectorizing)\n",
    "  cutstr=start_string # [0:SEQUENCE_LEN] # \n",
    "  input_eval = [textlib.c2i[s] for s in cutstr]\n",
    "  input_eval_1 = tf.expand_dims(input_eval, 0)\n",
    "\n",
    "  input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "\n",
    "  # Empty string to store our results\n",
    "  text_generated = []\n",
    "  ids=[]\n",
    "\n",
    "  # Low temperatures results in more predictable text.\n",
    "  # Higher temperatures results in more surprising text.\n",
    "  # Experiment to find the best setting.\n",
    "  temperature = temp\n",
    "\n",
    "  # Here batch size == 1\n",
    "  model.reset_states()\n",
    "  for i in range(num_generate):\n",
    "      predictions = model.predict(input_eval, batch_size=1)\n",
    "      # remove the batch dimension\n",
    "      predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "      # using a categorical distribution to predict the word returned by the model\n",
    "      predictions = predictions / temperature\n",
    "      predicted_tensor = tf.random.categorical(predictions, num_samples=1)[-1,0]\n",
    "      predicted_id=predicted_tensor.numpy()\n",
    "      ids.append(predicted_id)\n",
    "\n",
    "      text_generated.append(textlib.i2c[predicted_id])\n",
    "\n",
    "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))\n",
    "      # We pass the predicted word as the next input to the model\n",
    "      # along with the previous hidden state\n",
    "      input_eval_1 = tf.expand_dims([predicted_id], 0)\n",
    "      input_eval = tf.keras.backend.one_hot(input_eval_1, len(textlib.i2c))    \n",
    "  return (''.join(text_generated), ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\"\n",
    "len(start_string[0:SEQUENCE_LEN])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "H7750ibzEARJ",
    "outputId": "95ffb18e-7482-42c3-f558-493a723a4b3f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " I may no love to meth plotrest a greess with dis her behaghedg and wise the gadrsect, a disting but for of mery beon the clange\n"
     ]
    }
   ],
   "source": [
    "if use_tpu_for_generation:\n",
    "    sess=tf.compat.v1.keras.backend.get_session() # tf.compat.v1.get_default_session()\n",
    "    with sess.as_default():\n",
    "        tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "else:\n",
    "    tf.compat.v1.enable_eager_execution()\n",
    "    if not tf.executing_eagerly():\n",
    "        print(\"Eager engine stall.\")\n",
    "    # with tf.device('/job:localhost/replica:0/task:0/device:CPU:0'):  # Speed is about same gpu/cpu\n",
    "    tx,id=generate_text(gen_model, start_string=\"With the clarity of thought of an artificial life form, the discussion went on:\", temp=0.8)\n",
    "    print(tx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-mbC9l8EARL"
   },
   "outputs": [],
   "source": [
    "def detectPlagiarism(tx, textlibrary, minQuoteLength=10):\n",
    "    textlibrary.source_highlight(tx, minQuoteLength)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Tql-eLGvEARO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " of her he batk on a make her beand aspansion_ vigyed out to her forded here poor spald mode that clmations stirf.  But mo stlee\n"
     ]
    }
   ],
   "source": [
    "txt=textlib.decode(id)\n",
    "txti=txt.split('\\r\\n')\n",
    "for t in txti:\n",
    "    print(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZBil008pEARS"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span style=\"background-color:#ebdef0;\"> of her he</span><sup>[2]</sup> batk on a<span style=\"background-color:#d8daef;\"> make her </span><sup>[1]</sup>beand aspansion_ vigy<span style=\"background-color:#e2d7d5;\">ed out to her </span><sup>[4]</sup><span style=\"background-color:#e2d7d5;\">forded her</span><sup>[4]</sup>e poor spald mode that clmations stirf.  But mo stlee"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<small><p style=\"text-align:right;\">Sources: <span style=\"background-color:#ebdef0;\">Emily Brontë: Wuthering Heights</span><sup>[2]</sup>, <span style=\"background-color:#d8daef;\">Jane Austen: Pride and Prejudice</span><sup>[1]</sup>, <span style=\"background-color:#e2d7d5;\">Jane Austen: Emma</span><sup>[4]</sup></p></small>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "detectPlagiarism(tx, textlib)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lXYmlO_IEARU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yWE_ZZMKEARV"
   },
   "source": [
    "## References:\n",
    "* <https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/text_generation.ipynb>\n",
    "* <https://colab.research.google.com/github/tensorflow/tpu/blob/master/tools/colab/shakespeare_with_tpu_and_keras.ipynb>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WW6LPdlhQtgF"
   },
   "source": [
    "## 6. A dialog with the trained model [not ported yet]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxDNYZiEQtgF"
   },
   "outputs": [],
   "source": [
    "# Do a dialog with the recursive neural net trained above:\n",
    "# def genDialogAnswer(prompt, g_state=None, endPrompt='.', maxEndPrompts=2,\n",
    "# maxAnswerSize=512, temperature=1.0):\n",
    "\n",
    "\n",
    "def doDialog():\n",
    "    # 0.1 (frozen character) - 1.3 (creative/chaotic character)\n",
    "    temperature = 0.6\n",
    "    endPrompt = '.'  # the endPrompt character is the end-mark in answers.\n",
    "    # look for number of maxEndPrompts until answer is finished.\n",
    "    maxEndPrompts = 4\n",
    "    maxAnswerSize = 2048  # Maximum length of the answer\n",
    "    minAnswerSize = 64  # Minimum length of the answer\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        print(\"Please enter some dialog.\")\n",
    "        print(\"The net will answer according to your input.\")\n",
    "        print(\"'bye' for end,\")\n",
    "        print(\"'reset' to reset the conversation context,\")\n",
    "        print(\"'temperature=<float>' [0.1(frozen)-1.0(creative)]\")\n",
    "        print(\"    to change character of the dialog.\")\n",
    "        print(\"    Current temperature={}.\".format(temperature))\n",
    "        print()\n",
    "        xso = None\n",
    "        bye = False\n",
    "        model.init.run()\n",
    "\n",
    "        tflogdir = os.path.realpath(model.logdir)\n",
    "        if not os.path.exists(tflogdir):\n",
    "            print(\"You haven't trained a model, no data found at: {}\".format(\n",
    "                trainParams[\"logdir\"]))\n",
    "            return\n",
    "\n",
    "        # Used for saving the training parameters periodically\n",
    "        saver = tf.train.Saver()\n",
    "        checkpoint_file = os.path.join(tflogdir, model.checkpoint)\n",
    "\n",
    "        lastSave = tf.train.latest_checkpoint(tflogdir, latest_filename=None)\n",
    "        if lastSave is not None:\n",
    "            pt = lastSave.rfind('-')\n",
    "            if pt != -1:\n",
    "                pt += 1\n",
    "                start_iter = int(lastSave[pt:])\n",
    "            # print(\"Restoring checkpoint at {}: {}\".format(start_iter, lastSave))\n",
    "            saver.restore(sess, lastSave)\n",
    "        else:\n",
    "            print(\"No checkpoints have been saved at:{}\".format(tflogdir))\n",
    "            return\n",
    "\n",
    "        # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "        doini = True\n",
    "\n",
    "        bye = False\n",
    "        while not bye:\n",
    "            print(\"> \", end=\"\")\n",
    "            prompt = input()\n",
    "            if prompt == 'bye':\n",
    "                bye = True\n",
    "                print(\"Good bye!\")\n",
    "                continue\n",
    "            if prompt == 'reset':\n",
    "                doini = True\n",
    "                # g_state = sess.run([model.init_state_0], feed_dict={model.batch_size: 1})\n",
    "                print(\"(conversation context marked for reset)\")\n",
    "                continue\n",
    "            if prompt[:len(\"temperature=\")] == \"temperature=\":\n",
    "                t = float(prompt[len(\"temperature=\"):])\n",
    "                if t > 0.05 and t < 1.4:\n",
    "                    temperature = t\n",
    "                    print(\"(generator temperature now {})\".format(t))\n",
    "                    print()\n",
    "                    continue\n",
    "                print(\"Invalid temperature-value ignored! [0.1-1.0]\")\n",
    "                continue\n",
    "            xs = ' ' * model.steps\n",
    "            xso = ''\n",
    "            for rep in range(1):\n",
    "                for i in range(len(prompt)):\n",
    "                    xs = xs[1:]+prompt[i]\n",
    "                    X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                    if doini:\n",
    "                        doini = False\n",
    "                        g_state = sess.run(\n",
    "                            [model.init_state_0], feed_dict={model.X: X_new})\n",
    "                    g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                               feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                          model.temperature: temperature})\n",
    "            ans = 0\n",
    "            numEndPrompts = 0\n",
    "            while (ans < maxAnswerSize and numEndPrompts < maxEndPrompts) or ans < minAnswerSize:\n",
    "\n",
    "                X_new = np.transpose([[textlib.c2i[sj]] for sj in xs])\n",
    "                g_state, y_pred = sess.run([model.final_state, model.output_softmax_temp],\n",
    "                                           feed_dict={model.X: X_new, model.init_state: g_state,\n",
    "                                                      model.temperature: temperature})\n",
    "                inds = list(range(model.vocab_size))\n",
    "                ind = np.random.choice(inds, p=y_pred[0, -1].ravel())\n",
    "                nc = textlib.i2c[ind]\n",
    "                if nc == endPrompt:\n",
    "                    numEndPrompts += 1\n",
    "                xso += nc\n",
    "                xs = xs[1:]+nc\n",
    "                ans += 1\n",
    "            print(xso.replace(\"\\\\n\", \"\\n\"))\n",
    "            textlib.source_highlight(xso, 13)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0JEPK2WIQtgI"
   },
   "outputs": [],
   "source": [
    "# Talk to the net!\n",
    "doDialog()"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": "ec3a4d2d-8063-4bfd-a4a2-ee070d3272f7",
   "lastKernelId": "1acc2b74-f51e-477b-910a-a5519dad53b9"
  },
  "accelerator": "TPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Copy of eager_poet.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
